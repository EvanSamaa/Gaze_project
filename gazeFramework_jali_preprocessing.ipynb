{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os, sys\n",
    "import librosa\n",
    "sys.path.insert(0, '/Users/evanpan/Documents/GitHub/EvansToolBox/Utils')\n",
    "sys.path.insert(0, '/Users/evanpan/Documents/GitHub/Gaze_project')\n",
    "sys.path.insert(0, \"C:/Users/evansamaa/Documents/GitHub/EvansToolBox/Utils\")\n",
    "from Signal_processing_utils import intensity_from_signal, pitch_from_signal, sparse_key_smoothing, laplacian_smoothing\n",
    "from Speech_Data_util import Sentence_word_phone_parser\n",
    "from prototypes.InputDataStructures import Dietic_Conversation_Gaze_Scene_Info\n",
    "from prototypes.MVP.MVP_static_saliency_list import ObjectBasedFixSaliency\n",
    "from prototypes.MVP.MVP_Aversion_saliency_list import AversionSignalDrivenSaliency, CTSAversionSignalDrivenSaliency\n",
    "from prototypes.MVP.MVP_look_at_point_planner import HabituationBasedPlanner, RandomPlanner, PartnerHabituationPlanner\n",
    "from prototypes.MVP.MVP_eye_head_driver import HeuristicGazeMotionGenerator\n",
    "\n",
    "from prototypes.EyeCatch.Saccade_model_with_internal_model import *\n",
    "from prototypes.Gaze_aversion_prior.Heuristic_model import *\n",
    "from prototypes.Boccignone2020.Gaze_target_planner import Scavenger_based_planner\n",
    "from prototypes.Boccignone2020.Improved_gaze_target_planner import Scavenger_planner_with_nest\n",
    "from prototypes.JaliNeck.JaliNeck import NeckCurve\n",
    "from prototypes.Gaze_aversion_prior.Ribhav_model import predict_aversion\n",
    "from prototypes.InputDataStructures import AgentInfo, TurnTakingData\n",
    "import pickle\n",
    "import math\n",
    "from datetime import datetime\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(0)\n",
    "# inputs\n",
    "input_folder = \"/Users/evanpan/Documents/Datasets/Ribhav_processed_dataset/\"\n",
    "input_folder = \"F:/MASC/Ribhav_processed_dataset\"\n",
    "input_file = \"Madelaine Petsch audition for The Prom\"\n",
    "scene_data_path = \"data/look_at_points/simplest_scene.json\"\n",
    "shot_id = 1\n",
    "speaker_id = 1\n",
    "turn_taking_threshold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 24"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather The transcript and Audio From Jali (because it's more reliable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file_no_space = \"_\".join(input_file.split(\" \"))\n",
    "audio_1_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.wav\".format(shot_id, 1))\n",
    "audio_2_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.wav\".format(shot_id, 2))\n",
    "script_1_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.txt\".format(shot_id, 1))\n",
    "script_2_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.txt\".format(shot_id, 2))\n",
    "praatoutput_1_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}_PraatOutput.txt\".format(shot_id, 1))\n",
    "praatoutput_2_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}_PraatOutput.txt\".format(shot_id, 2))\n",
    "sementic_script_1 = Sentence_word_phone_parser(praatoutput_1_path, script_1_path)\n",
    "sementic_script_1.get_turns(turn_taking_threshold)\n",
    "sementic_script_2 = Sentence_word_phone_parser(praatoutput_2_path, script_2_path)\n",
    "sementic_script_2.get_turns(turn_taking_threshold)\n",
    "audio_1, sr= librosa.load(audio_1_path, sr=44100)\n",
    "audio_2, sr= librosa.load(audio_2_path, sr=44100)\n",
    "agentScene1 = AgentInfo(scene_data_path)\n",
    "agentScene2 = AgentInfo(scene_data_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 3)\n"
     ]
    }
   ],
   "source": [
    "print(agentScene1.get_all_positions().shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Turn Taking Gaze Behaviour for character 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(lst, t):\n",
    "    left, right = 0, len(lst) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        if lst[mid][0] <= t and lst[mid][1] >= t:\n",
    "            return mid\n",
    "        elif lst[mid][1] < t:\n",
    "            left = mid + 1\n",
    "        elif lst[mid][0] > t:\n",
    "            right = mid - 1\n",
    "    return -1\n",
    "def find_closest_next(lst, t):\n",
    "    left, right = 0, len(lst) - 1\n",
    "    closest_index = None\n",
    "    \n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        if closest_index is None or abs(lst[mid][0] - t) < abs(lst[closest_index][0] - t):\n",
    "            closest_index = mid\n",
    "        \n",
    "        if lst[mid][0] <= t:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    \n",
    "    return closest_index\n",
    "def find_closest_prev(lst, t):\n",
    "    left, right = 0, len(lst) - 1\n",
    "    closest_index = None\n",
    "    \n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        if closest_index is None or abs(lst[mid][1] - t) <= abs(lst[closest_index][1] - t):\n",
    "            closest_index = mid\n",
    "        \n",
    "        if lst[mid][1] <= t:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    \n",
    "    return closest_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_direct_gaze = np.array([0.00078064, 0.00078064, 0.00078064, 0.        , 0.00078064,\n",
    "0.00078064, 0.00156128, 0.00156128, 0.00156128, 0.        ,\n",
    "       0.00078064, 0.00156128, 0.00078064, 0.00234192, 0.00468384,\n",
    "       0.0039032 , 0.00624512, 0.00858704, 0.0039032 , 0.01092896,\n",
    "       0.02185792, 0.04996097, 0.13739266, 0.45199063, 0.12880562,\n",
    "       0.07416081, 0.03278689, 0.01249024, 0.01092896, 0.00702576,\n",
    "       0.00546448, 0.00234192, 0.00156128, 0.0039032 , 0.00156128,\n",
    "       0.00078064, 0.00156128, 0.        , 0.00078064, 0.00234192])\n",
    "delay_of_direct_gaze = np.array([-48.2  , -46.178, -44.156, -42.134, -40.112, -38.09 , -36.068,\n",
    "       -34.046, -32.024, -30.002, -27.98 , -25.958, -23.936, -21.914,\n",
    "       -19.892, -17.87 , -15.848, -13.826, -11.804,  -9.782,  -7.76 ,\n",
    "        -5.738,  -3.716,  -1.694,   0.328,   2.35 ,   4.372,   6.394,\n",
    "         8.416,  10.438,  12.46 ,  14.482,  16.504,  18.526,  20.548,\n",
    "        22.57 ,  24.592,  26.614,  28.636,  30.658,  32.68 ])\n",
    "prov_direct_gaze = ss.rv_histogram([prob_direct_gaze, delay_of_direct_gaze])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if speaker_id == 1:\n",
    "    self_script = sementic_script_1\n",
    "    other_script = sementic_script_2\n",
    "    scene = agentScene1\n",
    "    audio = audio_1\n",
    "    audio_location = audio_1_path\n",
    "else:\n",
    "    other_script = sementic_script_1\n",
    "    self_script = sementic_script_2\n",
    "    audio = audio_2\n",
    "    scene = agentScene2\n",
    "    audio_location = audio_2_path\n",
    "turn_taking_salience_list = []\n",
    "turn_taking_inhibition_list = []\n",
    "# compute\n",
    "t_end = max(self_script.phone_intervals[-1][1], other_script.phone_intervals[-1][1]) + 5\n",
    "t_end_frame = math.floor(t_end * fps)\n",
    "ts = np.arange(0, t_end_frame)/fps\n",
    "out = []\n",
    "skip=False\n",
    "prev_salience = 0\n",
    "prev_inhibition = 0\n",
    "for t in ts:\n",
    "    # if current turn is skipped, simply repeat the previous salience\n",
    "    self_interval_index = find_index(self_script.turns_intervals, t)\n",
    "    other_interval_index = find_index(other_script.turns_intervals, t)\n",
    "    if skip:\n",
    "        turn_taking_salience_list += [prev_salience]\n",
    "        turn_taking_inhibition_list += [prev_inhibition] \n",
    "        if other_interval_index >= 0 and t > (other_script.turns_intervals[other_interval_index][1] + other_script.turns_intervals[other_interval_index][0])/2:\n",
    "            skip = False\n",
    "    elif other_interval_index >= 0:\n",
    "        # if the other person is talking, the influence from turn taking is to look at them\n",
    "        turn_taking_salience_list += [1]\n",
    "        turn_taking_inhibition_list += [1] # this one is multiplicative\n",
    "    elif self_interval_index >= 0:\n",
    "        interval = self_script.turns_intervals[self_interval_index]\n",
    "        # if the speech is an interjection:\n",
    "        prev_partner_speech_interval = other_script.turns_intervals[find_closest_prev(other_script.turns_intervals, t)]\n",
    "        next_partner_speech_interval = other_script.turns_intervals[find_closest_next(other_script.turns_intervals, t)]\n",
    "        if next_partner_speech_interval[0] - prev_partner_speech_interval[1] < 2: # i.e. very close to the current\n",
    "            turn_taking_salience_list += [1]\n",
    "            turn_taking_inhibition_list += [1] # this one is multiplicative\n",
    "        else:\n",
    "            # otherwise try and see \n",
    "            delay = prov_direct_gaze.rvs(1)\n",
    "            if interval[1] - t + delay < 0:\n",
    "                skip=True\n",
    "                turn_taking_salience_list += [1]\n",
    "                turn_taking_inhibition_list += [1] # this one is multiplicative\n",
    "            else:        \n",
    "                turn_taking_salience_list += [0]\n",
    "                turn_taking_inhibition_list += [1] # this one is multiplicative  \n",
    "    else:     \n",
    "        turn_taking_salience_list += [0]\n",
    "        turn_taking_inhibition_list += [1] # this one is multiplicative\n",
    "turn_taking_salience_list = np.array(turn_taking_salience_list)\n",
    "turn_taking_inhibition_list = np.array(turn_taking_inhibition_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Signal_processing_utils import interpolate1D\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from prototypes.MVP.MVP_Aversion_saliency_list import Base_Static_Saliency_List\n",
    "class TurnTakingDrivenSaliency(Base_Static_Saliency_List):\n",
    "    def __init__(self, scene_info: AgentInfo, audio: np.array, script: Sentence_word_phone_parser, sr=44100, dt=1/24):\n",
    "        self.scene_info: AgentInfo = scene_info\n",
    "        self._number_of_objects = scene_info.object_pos.shape[0] + scene_info.active_object_pos.shape[0] + scene_info.get_wondering_points().shape[0]\n",
    "        self._sr = sr\n",
    "        self._dt = dt # 100 hz\n",
    "        self._audio_start = 0\n",
    "        self._audio_end = float(audio.shape[0]) / float(self._sr)\n",
    "        self._numb_of_frames = int(np.ceil((self._audio_end) / self._dt)) # total number of frames\n",
    "        # self._audio = audio\n",
    "        # self._script = script\n",
    "        self.evaluated = False\n",
    "        self.map = np.zeros((int(self._numb_of_frames), self._number_of_objects))\n",
    "        self.map_interp = None\n",
    "    def get_object_positions(self):\n",
    "        objects = self.scene_info.object_pos\n",
    "        active_objects = self.scene_info.active_object_pos\n",
    "        wondering = self.scene_info.get_wondering_points()\n",
    "        return np.concatenate([objects, active_objects, wondering], axis=0)\n",
    "    \n",
    "    def evaluate_all(self):\n",
    "        if self.evaluated:\n",
    "            return self.map\n",
    "        else:\n",
    "            self.compute_salience()\n",
    "            x = np.arange(0, self._numb_of_frames) * self._dt\n",
    "            self.map_interp = interp1d(x, self.map, axis=0, fill_value=\"extrapolate\")\n",
    "            self.evaluated = True\n",
    "            return self.map\n",
    "    def evaluate(self, t):\n",
    "        if self.evaluated:\n",
    "            return self.map_interp(t)\n",
    "        else:\n",
    "            self.compute_salience()\n",
    "            x = np.arange(0, self._numb_of_frames) * self._dt\n",
    "            self.map_interp = interp1d(x, self.map, axis=0, fill_value=\"extrapolate\")\n",
    "            self.evaluated = True\n",
    "            return self.map_interp(t)\n",
    "    def compute_salience(self, aversion_prob_time, aversion_prob_val, interval=True):\n",
    "        # continue setting salience for all objects\n",
    "        for j in range(0, self._numb_of_frames):\n",
    "            for i in range(0, self._number_of_objects):\n",
    "                if i < self.scene_info.get_object_positions(coordinate_space=\"global\").shape[0]:\n",
    "                    self.map[j, i] = self.scene_info.object_interest[i]\n",
    "                elif i == self.scene_info.get_object_positions(coordinate_space=\"global\").shape[0]:\n",
    "                    self.map[j, i] = 1 - interpolate1D(aversion_prob_time, aversion_prob_val, float(j) * self._dt)                \n",
    "                else:\n",
    "                    if interpolate1D(aversion_prob_time, aversion_prob_val, float(j) * self._dt) < 0.3:\n",
    "                        self.map[j, i] = 0\n",
    "                    else:\n",
    "                        self.map[j, i] = 0.5\n",
    "\n",
    "    def plot(self):\n",
    "        resolution = 5\n",
    "        out_img = np.zeros((180 * resolution, 360 * resolution))\n",
    "        out_img[45*resolution:135*resolution, 90*resolution:270*resolution] = 0.1\n",
    "        rot = rotation_angles_frome_positions(self.scene_info.object_pos)\n",
    "        fig, ax = plt.subplots(1)\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "        # Show the image\n",
    "        ax.imshow(out_img)\n",
    "\n",
    "        # Now, loop through coord arrays, and create a circle at each x,y pair\n",
    "        # Show the image\n",
    "        for object_i in range(len(self.scene_info.scene_object_id)):\n",
    "            pos_i_global = self.scene_info.object_pos[object_i]\n",
    "            pos_i_local = self.scene_info.transform_world_to_local((pos_i_global))\n",
    "            pos_i_local = np.expand_dims(pos_i_local, axis=0)\n",
    "            rot_i = rotation_angles_frome_positions(pos_i_local)[0]\n",
    "            circ = Circle(((int(rot_i[0]) + 180) * resolution, int(rot_i[1] + 90) * resolution), int(40 * self.map[0, object_i]))\n",
    "            ax.add_patch(circ)\n",
    "        plt.show()\n",
    "aversion_saliency = TurnTakingDrivenSaliency(agentScene1, audio, None, dt=0.02)\n",
    "aversion_saliency.compute_salience(ts, 1-turn_taking_salience_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prototypes.MVP.MVP_static_saliency_list import ObjectBasedFixSaliency\n",
    "from prototypes.MVP.MVP_Aversion_saliency_list import AversionSignalDrivenSaliency\n",
    "from Geometry_Util import rotation_angles_frome_positions\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "class Scavenger_planner_simple:\n",
    "    def __init__(self, saliency_maps, scene_info, self_id=-1):\n",
    "        # hyper-parameters\n",
    "        self.smoothing_constant = 0.2\n",
    "        self.kappa = 1.3333333 # this is the distance factor (i.e. cost of migration), this is from the paper\n",
    "        self.kappa = 2.2 # this is the distance factor (i.e. cost of migration)\n",
    "        self.momentum_weight = 2\n",
    "        self.phi = .5 # this is the consumption efficiency i.e. how long it takes to consume all resources within a patch\n",
    "        self.beta = 20 # this is use to generate the probability of the bernoulli variable that determines staying vs\n",
    "        self.min_saccade_time = 0.2 # this specified how closely two nearby saccade can be with one another.\n",
    "\n",
    "        self.nest_consumption_rate = 0.2 # the amount of time to consume food at the nest\n",
    "        # TODO: this tao should also be time varying (Using this for the basis to implement reactive gaze (to the\n",
    "        # listener's gaze))\n",
    "        self.predation_risk_tao = 0.5 # the constant for exponential distribution for predation\n",
    "        # get the dt\n",
    "        self.dt = saliency_maps[0]._dt\n",
    "        # ====================================== Storing saliency maps ======================================\n",
    "        # store information about the scene\n",
    "        self.scene_info = scene_info\n",
    "        # store the saliency maps as a list of array, make sure they all have the same length\n",
    "        self.saliency_maps_arrs = []\n",
    "        # similarly store all the positions\n",
    "        self.object_positions = []\n",
    "        # get the maximum size\n",
    "        self.self_id = self_id\n",
    "        object_count = 0\n",
    "        time_count = 0\n",
    "        max_id = 0\n",
    "        for i in range(0, len(saliency_maps)):\n",
    "            arr = saliency_maps[i].map\n",
    "            if arr.shape[1] > object_count:\n",
    "                object_count = arr.shape[1]\n",
    "                max_id = i\n",
    "            if arr.shape[0] > time_count:\n",
    "                time_count = arr.shape[0]\n",
    "        for i in range(0, len(saliency_maps)):\n",
    "            arr = saliency_maps[i].map\n",
    "            if arr.shape[1] < object_count:\n",
    "                extension = np.zeros((arr.shape[0], object_count-arr.shape[1]))\n",
    "                arr = np.concatenate([arr, extension], axis=1)\n",
    "            if arr.shape[0] < time_count:\n",
    "                extension = np.zeros((time_count - arr.shape[0], object_count))\n",
    "                arr = np.concatenate([arr, extension], axis=0)\n",
    "            self.saliency_maps_arrs.append(arr)\n",
    "        # get the position of objects\n",
    "        self.object_positions = saliency_maps[max_id].get_object_positions()\n",
    "        # turn them into rotation angles\n",
    "        self.object_positions = rotation_angles_frome_positions(self.object_positions) / 180 * np.pi\n",
    "        # get the conversation partner's id\n",
    "        if self.self_id == -1:\n",
    "            self.conversation_partner_id = agentScene1.object_pos.shape[0]\n",
    "            # ========================================= state variables ==========================================\n",
    "            self.current_look_at = self.conversation_partner_id\n",
    "            # index for the nest, of which the gaze will gravitate towards\n",
    "            self.nest_index = self.conversation_partner_id\n",
    "        else:\n",
    "            self.conversation_partner_id = agentScene1.object_pos.shape[0]\n",
    "            # ========================================= state variables ==========================================\n",
    "            self.current_look_at = self.conversation_partner_id\n",
    "            # index for the nest, of which the gaze will gravitate towards\n",
    "            self.nest_index = self.conversation_partner_id\n",
    "        # momentum stores the current gaze_direction from the conversation-partner\n",
    "        self.momentum = np.array([0, 0])\n",
    "    def compute(self, initial_target):\n",
    "        # variable to store the output values\n",
    "        output_t = [0]\n",
    "        output_target = [self.nest_index]\n",
    "        # step 1: obtain smoothed saliency map\n",
    "        for i in range(0, len(self.saliency_maps_arrs)):\n",
    "            map_i = self.saliency_maps_arrs[i]\n",
    "            smoothed_map = np.zeros(self.saliency_maps_arrs[i].shape)\n",
    "            for t in range(0, map_i.shape[0]):\n",
    "                smoothed_map[t] = map_i[max(0, t-1)] * self.smoothing_constant + (1 - self.smoothing_constant) * map_i[t]\n",
    "            self.saliency_maps_arrs[i] = smoothed_map\n",
    "        # step 2: obtain patches through thresholding, we skip this step as we don't work with images, but a list of objects\n",
    "        # step 3: perform the planning\n",
    "        # perform a summation for all saliency maps to obtain the overall resources:\n",
    "        values = np.zeros(self.saliency_maps_arrs[0].shape)\n",
    "        for map_arr in self.saliency_maps_arrs:\n",
    "            values += map_arr\n",
    "        if self.self_id >= 0:\n",
    "            values[:, self.self_id] = 0\n",
    "        # initialize the first look at point with the user speficied initial target\n",
    "        self.current_look_at = self.nest_index\n",
    "        time_within_patch = 0\n",
    "        time_away_from_nest = 0\n",
    "        # add the first target to the output list\n",
    "        output_target.append(self.current_look_at)\n",
    "        output_t.append(0)\n",
    "        for i in range(0, self.saliency_maps_arrs[0].shape[0]):\n",
    "            # update new nest\n",
    "            if self.self_id >= 0:\n",
    "                self.nest_index = agentScene1.object_pos.shape[0]\n",
    "            ##############################################################################################\n",
    "            ############################### decide whether to switch patch ###############################\n",
    "            ##############################################################################################\n",
    "            # compute rho (value)\n",
    "            look_at_mask = np.zeros((self.saliency_maps_arrs[0].shape[1], ))\n",
    "            look_at_mask[self.current_look_at] = 1\n",
    "            not_looked_at_mask = np.ones(look_at_mask.shape) - look_at_mask\n",
    "            # compute the distance to the patch (first use this variable to store the position of look_at_point)\n",
    "            distance_to_patch = np.tile(self.object_positions[self.current_look_at:self.current_look_at+1], [self.object_positions.shape[0], 1])\n",
    "            distance_to_patch = (distance_to_patch - self.object_positions)\n",
    "            distance_to_patch = np.sqrt(np.square(distance_to_patch).sum(axis=1))\n",
    "\n",
    "            # compute the distance to the nest\n",
    "            distance_to_nest = np.tile(self.object_positions[self.nest_index:self.nest_index + 1], [self.object_positions.shape[0], 1])\n",
    "            distance_to_nest = (distance_to_nest - self.object_positions)\n",
    "            distance_to_nest = np.sqrt(np.square(distance_to_nest).sum(axis=1))\n",
    "            # updates 5 times a second\n",
    "            if (float(i) * self.dt / 0.1).is_integer() and i > 0:\n",
    "                # if we are currently in the nest\n",
    "                if self.current_look_at == self.nest_index :\n",
    "                    # compute the average value of going out\n",
    "                    M = self.nest_consumption_rate\n",
    "                    rho = values[i] * np.exp(-self.kappa * distance_to_patch)\n",
    "                    rho[self.current_look_at] = rho[self.current_look_at] * np.exp(-self.nest_consumption_rate * time_within_patch)\n",
    "                    rho_mean = 1 / (rho.shape[0] - 1) * np.sum(rho * not_looked_at_mask)\n",
    "                    rho_max = np.max(rho * not_looked_at_mask)\n",
    "                    p_leave = 1 / (1 + np.exp(self.beta * (rho[self.current_look_at] - rho_mean)))\n",
    "                    rv = np.random.binomial(1, p_leave)\n",
    "                    if rv == 1:\n",
    "                        prob = values[i] * np.exp(-self.kappa * distance_to_patch) * not_looked_at_mask\n",
    "                        heat = self.beta/2\n",
    "                        probability = np.exp(heat * prob)/np.sum(np.exp(heat * prob))\n",
    "                        probability[self.self_id] = 0\n",
    "                        probability = probability / probability.sum()\n",
    "                        # find the item with maximum probability\n",
    "                        deterministic_new_patch = np.argmax(prob)\n",
    "                        # sample the items for a more randomized new look-at-point\n",
    "                        sampled_new_patch = np.random.choice(np.arange(0, prob.shape[0]), 1, p=probability)[0]\n",
    "                        # use the sampled patch id for better looking result in a static scene\n",
    "                        new_patch = sampled_new_patch\n",
    "                        time_within_patch = 0\n",
    "                        time_away_from_nest = 0\n",
    "                        self.current_look_at = new_patch\n",
    "                        output_target.append(self.current_look_at)\n",
    "                        output_t.append(self.dt * i)\n",
    "                        self.momentum = self.object_positions[self.current_look_at] - self.object_positions[self.nest_index]\n",
    "                else:\n",
    "\n",
    "                    # compute distance-weighted patch value rho\n",
    "                    momemtum_distance = np.expand_dims(self.momentum, axis=0)\n",
    "                    momemtum_distance = momemtum_distance * (self.object_positions - np.expand_dims(self.object_positions[self.nest_index], axis=0))\n",
    "                    momemtum_distance = 1 / (1 + np.exp(- 10 * momemtum_distance.sum(axis=1)))\n",
    "                    rho = values[i] * np.exp(-self.kappa * distance_to_patch)\n",
    "                    risk = np.exp(-self.kappa * distance_to_nest) * self.predation_risk_tao * np.exp(self.predation_risk_tao * time_away_from_nest)\n",
    "                    # if it is still worth it to not return to nest\n",
    "                    if (rho - risk).max() > 0:\n",
    "                        # compute Q, the expected return of leaving the current patch and move to another patch\n",
    "                        Q = 1 / (self.object_positions.shape[0] - 1) * np.sum(rho * not_looked_at_mask * np.exp(-momemtum_distance))\n",
    "                        # compute g_patch, the instetaneous gain by staying at the current patch\n",
    "                        if rho[self.current_look_at] > 0:\n",
    "                            g_patch = rho[self.current_look_at]\n",
    "                            g_patch = rho[self.current_look_at] * np.exp(-self.phi / values[i, self.current_look_at] * time_within_patch)\n",
    "                        else:\n",
    "                            g_patch = 0\n",
    "                        # compute the probability of migration (logistic function as per the paper, howeverm it is very noisy )\n",
    "                        p_stay = 1 / (1 + np.exp(-self.beta*(g_patch - Q)))\n",
    "                        ########################### sample from bernoulli distribution to determine wheter to switch patch #####################\n",
    "                        # if the sampling determine that there is a patch switch (the issue is that given the sampling frequency,\n",
    "                        # it is actually highly likely for the gaze target to switch\n",
    "                        if p_stay <= 1.0:\n",
    "                            rv = np.random.binomial(1, p_stay)\n",
    "                        else:\n",
    "                            rv = 0\n",
    "                        if rv == 0 and time_within_patch >= self.min_saccade_time:\n",
    "                            # get probability of the gaze aversion\n",
    "                            prob = values[i] * np.exp(-self.kappa * distance_to_patch) * not_looked_at_mask * np.exp(-self.momentum_weight*momemtum_distance)\n",
    "                            heat = self.beta / 2\n",
    "                            probability = np.exp(heat * prob) / np.sum(np.exp(heat * prob))\n",
    "                            probability[self.self_id] = 0\n",
    "                            probability = probability / probability.sum()\n",
    "                            # find the item with maximum probability\n",
    "                            deterministic_new_patch = np.argmax(prob)\n",
    "                            # sample the items for a more randomized new look-at-point\n",
    "                            sampled_new_patch = np.random.choice(np.arange(0, prob.shape[0]), 1, p=probability)[0]\n",
    "                            # use the sampled patch id for better looking result in a static scene\n",
    "                            new_patch = sampled_new_patch\n",
    "\n",
    "                            time_within_patch = 0\n",
    "                            self.current_look_at = new_patch\n",
    "                            output_target.append(self.current_look_at)\n",
    "                            output_t.append(self.dt * i)\n",
    "                            self.momentum = self.object_positions[self.current_look_at] - self.object_positions[self.nest_index]\n",
    "                    else:\n",
    "                        time_within_patch = 0\n",
    "                        time_away_from_nest = 0\n",
    "                        self.current_look_at = self.nest_index\n",
    "                        output_target.append(self.current_look_at)\n",
    "                        output_t.append(self.dt * i)\n",
    "                        self.momentum = self.momentum * 0\n",
    "\n",
    "            # accumulate time away and within patch\n",
    "            time_within_patch += self.dt\n",
    "            time_away_from_nest += self.dt\n",
    "        return output_t, output_target\n",
    "    def model_fitting(self, average_aversion_duration, average_gaze_duration):\n",
    "        return\n",
    "planner = Scavenger_planner_simple([aversion_saliency], scene)\n",
    "# planner = Scavenger_planner_with_nest([base_saliency, aversion_saliency], scene)\n",
    "output_times, output_targets = planner.compute(scene.object_pos.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 7, 2, 4, 7, 2, 7, 2, 6, 6, 7, 4, 6, 7, 1, 6, 5, 1, 7, 1, 8, 6, 7, 1, 0, 7, 2, 4, 7, 4, 0, 7, 4, 2, 7, 6, 3, 4, 7, 7, 3, 0, 5, 2, 7, 2, 1, 3, 8, 7, 2, 0, 7, 2, 4, 3, 7, 2, 7, 1, 6, 7, 0, 0, 6, 7, 4, 5, 0, 7, 6, 4, 7, 5, 0, 2, 7, 0, 4, 1, 7, 3, 2, 7, 8, 5, 7, 6, 8, 0, 0, 7, 4]\n"
     ]
    }
   ],
   "source": [
    "print(output_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for axis 0 with size 16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\EVANSA~1\\AppData\\Local\\Temp/ipykernel_14472/2908012019.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0moutput_target_positions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscene\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all_positions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoordinate_space\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0minternal_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInternalModelCenterBias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscene\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSacccadeGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_times\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_target_positions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minternal_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m# print(output_times, output_target_positions, output_targets)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mek\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmicro_saccade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\evansamaa\\Documents\\GitHub\\Gaze_project\\prototypes\\EyeCatch\\Saccade_model_with_internal_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, target_times, target_positions, target_index, internal_model, dt)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_positions_head\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_gaze_intervals_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_gaze_intervals_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gaze_intervals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_times\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_positions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\evansamaa\\Documents\\GitHub\\Gaze_project\\prototypes\\EyeCatch\\Saccade_model_with_internal_model.py\u001b[0m in \u001b[0;36mget_gaze_intervals\u001b[1;34m(self, time_arr, pos_arr)\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mgaze_intervals_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0mgaze_intervals_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[1;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mpos_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.00001\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                     \u001b[0mgaze_intervals_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 16 is out of bounds for axis 0 with size 16"
     ]
    }
   ],
   "source": [
    "# =============================================================================================\n",
    "# ========================== plan scan path based on the saliency maps ========================\n",
    "# =============================================================================================\n",
    "# get view_target planner\n",
    "# planner = PartnerHabituationPlanner(base_saliency, audio, sementic_script, scene, 0.8)\n",
    "# planner = HabituationBasedPlanner(base_saliency, audio, sementic_script, scene, 0.7)\n",
    "# compute the gaze targets and times\n",
    "# output_times, output_targets = planner.compute()\n",
    "#get the output_targets_positions from the scene\n",
    "output_target_positions = []\n",
    "wondering_positions = scene.get_wondering_points()\n",
    "for i in output_targets:\n",
    "    output_target_positions.append(scene.get_all_positions(coordinate_space=\"local\", ))\n",
    "internal_model = InternalModelCenterBias(scene)\n",
    "generator = SacccadeGenerator(output_times, output_target_positions, output_targets, internal_model)\n",
    "# print(output_times, output_target_positions, output_targets)\n",
    "ek, hk, micro_saccade = generator.compute()\n",
    "conversational_neck = NeckCurve(audio_location)\n",
    "jali_neck_output = conversational_neck.compute_curve()\n",
    "\n",
    "# arr = np.array(ek)\n",
    "# arr = arr[0, :, 1:]\n",
    "# partner = scene.transform_world_to_local(scene.object_pos[scene.get_conversation_partner_id()[0]])\n",
    "# partner = np.expand_dims(partner, axis=0)\n",
    "# distance_with_goal = arr - partner\n",
    "# plt.plot(distance_with_goal)\n",
    "# plt.show()\n",
    "blend_weight = []\n",
    "for i in range(1, len(hk[0])-1):\n",
    "    velocity = math.sqrt((hk[0][i][1]-hk[0][i-1][1])**2 + (hk[0][i-1][2]-hk[0][i][2])**2)\n",
    "    blend_weight.append([hk[0][i][0], 1 - min(1, velocity/0.75)])\n",
    "# motion_generator = HeuristicGazeMotionGenerator(scene, sementic_script)\n",
    "# ek, hk, micro_saccade = motion_generator.generate_neck_eye_curve(output_times, output_target_positions)\n",
    "# out_location = \"C:/Users/evan1/Documents/Gaze_project/data/look_at_points/prototype2p2.pkl\"\n",
    "# out_location = \"C:/Users/evansamaa/Desktop/Gaze_project/data/look_at_points/prototype2p2.pkl\"\n",
    "out_location = \"data/prototype2p2.pkl\"\n",
    "out = [ek, hk, micro_saccade, jali_neck_output, []]\n",
    "pickle.dump(out, open(out_location, 'wb'), protocol=2)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Visemenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d04a865f428ff383ca73f042ddebc2f550e42d245de5d03aeab7445d0864e42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
