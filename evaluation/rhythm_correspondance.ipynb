{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2 as cv\n",
    "import pickle as pkl\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os, sys\n",
    "import librosa\n",
    "\n",
    "from scipy import stats, spatial, ndimage\n",
    "sys.path.insert(0, '/Users/evanpan/Documents/GitHub/EvansToolBox/Utils')\n",
    "sys.path.insert(0, '/Users/evanpan/Documents/GitHub/Gaze_project')\n",
    "sys.path.insert(0, \"C:/Users/evansamaa/Documents/GitHub/EvansToolBox/Utils\")\n",
    "from Signal_processing_utils import intensity_from_signal, pitch_from_signal, sparse_key_smoothing, laplacian_smoothing\n",
    "from Speech_Data_util import Sentence_word_phone_parser\n",
    "from prototypes.InputDataStructures import Dietic_Conversation_Gaze_Scene_Info\n",
    "from prototypes.MVP.MVP_static_saliency_list import ObjectBasedFixSaliency\n",
    "from prototypes.MVP.MVP_Aversion_saliency_list import AversionSignalDrivenSaliency, CTSAversionSignalDrivenSaliency\n",
    "from prototypes.MVP.MVP_look_at_point_planner import HabituationBasedPlanner, RandomPlanner, PartnerHabituationPlanner\n",
    "from prototypes.MVP.MVP_eye_head_driver import HeuristicGazeMotionGenerator\n",
    "from prototypes.MVP.MVP_Aversion_saliency_list import Base_Static_Saliency_List\n",
    "from prototypes.EyeCatch.Saccade_model_with_internal_model import *\n",
    "from prototypes.Gaze_aversion_prior.Heuristic_model import *\n",
    "from prototypes.Boccignone2020.Gaze_target_planner import Scavenger_based_planner\n",
    "from prototypes.Boccignone2020.Improved_gaze_target_planner import Scavenger_planner_with_nest, Scavenger_planner_simple \n",
    "from prototypes.JaliNeck.JaliNeck import NeckCurve\n",
    "from prototypes.Gaze_aversion_prior.Ribhav_model import predict_aversion\n",
    "from prototypes.InputDataStructures import AgentInfo, TurnTakingData\n",
    "from prototypes.MVP.MVP_gaze_path_planner import Responsive_planner_simple\n",
    "import pickle\n",
    "import math\n",
    "from datetime import datetime, timezone\n",
    "import scipy.stats as ss\n",
    "from Signal_processing_utils import interpolate1D, runEuro, intensity_from_signal\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport prototypes.InputDataStructures\n",
    "%aimport Signal_processing_utils\n",
    "%aimport Speech_Data_util\n",
    "%aimport prototypes.MVP.MVP_static_saliency_list\n",
    "%aimport prototypes.EyeCatch.Saccade_model_with_internal_model\n",
    "%aimport prototypes.InputDataStructures\n",
    "%aimport prototypes.Jin2019.EyeHeadDecomposition\n",
    "%aimport prototypes.Optimization_based_head_eye_seperator.Baseline_optimization\n",
    "%aimport prototypes.Boccignone2020.Improved_gaze_target_planner\n",
    "%aimport prototypes.MVP.MVP_gaze_path_planner\n",
    "%aimport prototypes.JaliNeck.JaliNeck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "input_folder = \"/Volumes/EVAN_DISK/MASC/Ribhav_processed_dataset/\"\n",
    "# input_folder = \"F:/MASC/Ribhav_processed_dataset/\"\n",
    "# input_file = \"Stranger Things’ Dacre Montgomery’s Insane _Billy_ Audition Tape ｜ GQ\"\n",
    "input_file = \"Madelaine Petsch audition for The Prom\"\n",
    "\n",
    "input_scene_data_path = \"/Users/evanpan/Documents/GitHub/Gaze_project/data/look_at_points/simplest_scene.json\"\n",
    "speaker_id = 1\n",
    "shot_id = 1\n",
    "turn_taking_threshold = 2\n",
    "fps = 24\n",
    "np.random.seed(speaker_id + shot_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file_no_space = \"_\".join(input_file.split(\" \"))\n",
    "# input_file_no_space = \"'\" + input_file_no_space + \"'\"\n",
    "shot_file_name = os.path.join(os.path.join(os.path.join(input_folder, \"shots\"), input_file), \"shot_cuts.json\")\n",
    "video_path = os.path.join(*[input_folder, \"video\", input_file+\".mp4\"])\n",
    "audio_1_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.wav\".format(shot_id, 1))\n",
    "audio_2_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.wav\".format(shot_id, 2))\n",
    "script_1_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.txt\".format(shot_id, 1))\n",
    "script_2_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.txt\".format(shot_id, 2))\n",
    "praatoutput_1_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}_PraatOutput.txt\".format(shot_id, 1))\n",
    "praatoutput_2_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}_PraatOutput.txt\".format(shot_id, 2))\n",
    "head_direction_json_path = os.path.join(*[input_folder, \"pose\", input_file+\".pkl\"])\n",
    "gaze_direction_json_path = os.path.join(*[input_folder, \"L2CSNet\", input_file+\".json\"])\n",
    "landmarks_mediapipe_json_path = os.path.join(*[input_folder, \"faces\", input_file+\" faces.pkl\"])\n",
    "python_out_location = os.path.join(*[input_folder, \"outputs\", input_file_no_space+\".pkl\"])\n",
    "scene_file_out_location = os.path.join(*[input_folder, \"annotated_scene\", input_file+\".json\"])\n",
    "animation_file_path = os.path.join(*[input_folder, \"outputs\", input_file+\".json\"])\n",
    "gd_file_path = os.path.join(*[input_folder, \"outputs\", input_file+\"_gaze_deploy.json\"])\n",
    "st_file_path = os.path.join(*[input_folder, \"outputs\", input_file+\"_stare.json\"])\n",
    "ss_file_path = os.path.join(*[input_folder, \"outputs\", input_file+\"_static_salience.json\"])\n",
    "\n",
    "scene_data_path = os.path.join(*[input_folder, \"annotated_scene\", input_file+\".json\"])\n",
    "\n",
    "# out_location = \"/Users/evanpan/Documents/GitHub/Gaze_project/data/look_at_points/prototype2p2.pkl\"\n",
    "# out_location = \"C:/Users/evansamaa/Documents/GitHub/Gaze_project/data/look_at_points/prototype2p2.pkl\"\n",
    "\n",
    "sementic_script_1 = Sentence_word_phone_parser(praatoutput_1_path, script_1_path)\n",
    "# sementic_script_1.get_turns(turn_taking_threshold)\n",
    "sementic_script_2 = Sentence_word_phone_parser(praatoutput_2_path, script_2_path)\n",
    "# sementic_script_2.get_turns(turn_taking_threshold)\n",
    "audio_1, sr= librosa.load(audio_1_path, sr=44100)\n",
    "audio_2, sr= librosa.load(audio_2_path, sr=44100)\n",
    "agentScene1 = AgentInfo(scene_data_path)\n",
    "agentScene2 = AgentInfo(scene_data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the beat from the audio as per Rhythmic Gesticulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the energy and derivative\n",
    "audio = audio_1\n",
    "audio_energy = intensity_from_signal(audio, int(sr/50))\n",
    "ts = np.arange(0, audio_energy.shape[0]) / 50\n",
    "daudio_dt = dx_dt(audio_energy)\n",
    "Dm = 0.2\n",
    "DM = 0.6\n",
    "DM_frame = math.floor(DM / (ts[1] - ts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative find audio onset between 0.2 and 0.6 seconds to identify beats\n",
    "beats = [[0, False]] # start with a pseudo beat\n",
    "for i in range(0, audio_energy.shape[0]):\n",
    "    if daudio_dt[i] > 5:\n",
    "        current_beat_t = ts[i]\n",
    "        if current_beat_t - ts[beats[-1][0]] <= Dm:\n",
    "            continue\n",
    "        if current_beat_t - tspbeats[-1][0]] >= DM:\n",
    "            # these are stored as integer indexes\n",
    "            start = beats[-1][0]\n",
    "            end = current_beat_t\n",
    "            counter = start + DM_frame\n",
    "            while counter < end:\n",
    "                beats.append([counter, False])\n",
    "                counter = counter + DM_frame\n",
    "        beats.append([i, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
