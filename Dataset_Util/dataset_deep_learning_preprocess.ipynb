{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import json\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import soundfile as sf\n",
    "import whisper_timestamped\n",
    "# import utility functions\n",
    "sys.path.insert(0, '/Users/evanpan/Documents/GitHub/EvansToolBox/Utils')\n",
    "# sys.path.insert(0, \"C:/Users/evansamaa/Documents/GitHub/EvansToolBox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_angles_frome_positions(arr):\n",
    "    \"\"\"\n",
    "    converts an array of positions to an array of rotation angles (azimuth, elevation)\n",
    "    centered at the origin, where:\n",
    "        azimuth: +right,-left\n",
    "        elevation: +up,-down\n",
    "    here we assume that the input vectors are in world coordinates\n",
    "    :param arr: array with shape (N, 3)\n",
    "    :return: array with shape (N, 2)\n",
    "    \"\"\"\n",
    "    # F: arr (N, 3) -> arr (N, 2) or arr (3, ) -> (2, )\n",
    "    # in the output is in the convention of (azimuth, elevation)\n",
    "    if len(arr.shape) == 2:\n",
    "        mag = np.sqrt(np.sum(arr * arr, axis=1, keepdims=True))\n",
    "        out = arr / mag\n",
    "        out[:, 0] = np.arcsin(out[:, 0])\n",
    "        out[:, 1] = np.arcsin(out[:, 1])\n",
    "        return out[:, 0:2] * 180 / np.pi\n",
    "    else:\n",
    "        mag = np.sqrt(np.sum(arr * arr))\n",
    "        out = arr / mag\n",
    "        out[0] = np.arcsin(out[0])\n",
    "        out[1] = np.arcsin(out[1])\n",
    "        return out[0:2] * 180 / np.pi\n",
    "def get_valid_shots(shots, fps, shot_length_mininmum=5):\n",
    "    t0 = datetime.strptime(\"00:00:00.0\", '%H:%M:%S.%f').timestamp()\n",
    "    for shot in shots:\n",
    "        start = shot[0]\n",
    "        end = shot[1]\n",
    "    # load the input shots range\n",
    "    valid_shots_time, valid_shots_frames = [], []\n",
    "    t0 = datetime.strptime(\"00:00:00.0\", '%H:%M:%S.%f').timestamp()\n",
    "    for i in range(len(shots)):\n",
    "        start = datetime.strptime(shots[i][0], '%H:%M:%S.%f').timestamp()\n",
    "        end = datetime.strptime(shots[i][1], '%H:%M:%S.%f').timestamp()\n",
    "        if (end-start) >= shot_length_mininmum:\n",
    "            start_t = start-t0\n",
    "            end_t = end - t0\n",
    "            valid_shots_time.append([start-t0, end-t0])\n",
    "            valid_shots_frames.append([int(np.round(start_t*fps)), int(np.round(end_t*fps))])\n",
    "\n",
    "    return valid_shots_time, valid_shots_frames\n",
    "def load_head_and_gaze_angles(all_gaze_data, all_head_data):\n",
    "\n",
    "    # head data\n",
    "    head_angle_data = all_head_data[\"HEAD\"]\n",
    "    head_rotmat_per_frame = head_angle_data[\"ROTMAT\"]\n",
    "    head_bbox_per_frame = all_head_data[\"BBOX\"] # we are not using but having it here is nice\n",
    "    head_angle_per_frame = []\n",
    "    neutral_position = np.array([0, 0, 100])\n",
    "    for i in range(0, head_rotmat_per_frame.shape[0]):\n",
    "        pos = head_rotmat_per_frame[i] @ neutral_position\n",
    "        head_angle_per_frame.append(rotation_angles_frome_positions(pos[:]))\n",
    "    head_angle_per_frame = np.array(head_angle_per_frame)\n",
    "    # getting rotation angle in z direction\n",
    "    neutral_position2 = np.array([0, 100, 0])\n",
    "    head_angle_z_per_frame = []\n",
    "    for i in range(0, head_rotmat_per_frame.shape[0]):\n",
    "        pos = head_rotmat_per_frame[i] @ neutral_position2\n",
    "        pos = np.array([pos[1], pos[2], pos[0]])\n",
    "        head_angle_z_per_frame.append(rotation_angles_frome_positions(pos)[1])\n",
    "    head_angle_xy_per_frame = np.array(head_angle_per_frame)\n",
    "    head_angle_z_per_frame = np.expand_dims(np.array(head_angle_z_per_frame), axis=1)\n",
    "    head_angle_per_frame = np.concatenate([head_angle_xy_per_frame, head_angle_z_per_frame], axis=1)\n",
    "\n",
    "    # getting gaze data\n",
    "    gaze_angle_data = all_gaze_data[\"RAW_GAZE\"]\n",
    "    gaze_angle_per_frame = gaze_angle_data[\"EULER\"]\n",
    "    gaze_rotmat_per_frame = gaze_angle_data[\"ROTMAT\"]\n",
    "    blinks = all_head_data[\"BLINKS\"]\n",
    "    gaze_vec = np.array([0, 0, 100])\n",
    "    eye_angle_per_frame = []\n",
    "    for i in range(0, gaze_rotmat_per_frame.shape[0]):\n",
    "        eye_line = gaze_rotmat_per_frame[i] @ gaze_vec\n",
    "        eye_line = eye_line / eye_line[2] * 100\n",
    "        eye_angle_per_frame.append(eye_line)\n",
    "    eye_angle_per_frame = np.array(eye_angle_per_frame)\n",
    "    eye_angle_per_frame = rotation_angles_frome_positions(eye_angle_per_frame[:])\n",
    "    return eye_angle_per_frame, head_angle_per_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"/Volumes/EVAN_DISK/MASC/Ribhav_processed_dataset/\"\n",
    "output_folder = \"/Volumes/EVAN_DISK/MASC/deep_learning_processed_dataset/\"\n",
    "redo = False\n",
    "\n",
    "target_fps = 24\n",
    "window_length = 20\n",
    "stride_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_path = os.path.join(output_folder, \"metadata.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Structure Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Deal with the output folder structures:\n",
    "# remove everything in the output folder\n",
    "if redo:\n",
    "    try:\n",
    "        os.mkdir(output_folder)\n",
    "    except:\n",
    "        shutil.rmtree(output_folder)\n",
    "        os.mkdir(output_folder)\n",
    "        \n",
    "    # this set is temporary\n",
    "    os.mkdir(os.path.join(output_folder, \"taudio\")) # this one will have MFCC, intensity, \n",
    "    os.mkdir(os.path.join(output_folder, \"ttext\")) # this one give the text per \n",
    "    os.mkdir(os.path.join(output_folder, \"tgaze\")) # this will store the per time-stamp.\n",
    "    os.mkdir(os.path.join(output_folder, \"thead\")) # this one one is also per time-stamp\n",
    "    os.mkdir(os.path.join(output_folder, \"tfixation\")) # this will have the gaze fixation. \n",
    "    os.mkdir(os.path.join(output_folder, \"tblinks\")) # this one is \n",
    "    os.mkdir(os.path.join(output_folder, \"taversion_label\")) # this one is also one per time frame\n",
    "\n",
    "    # this set is permanant\n",
    "    os.mkdir(os.path.join(output_folder, \"audio\")) # this one will have MFCC, intensity, \n",
    "    os.mkdir(os.path.join(output_folder, \"text\")) # this one give the text per \n",
    "    os.mkdir(os.path.join(output_folder, \"gaze\")) # this will store the per time-stamp.\n",
    "    os.mkdir(os.path.join(output_folder, \"head\")) # this one one is also per time-stamp\n",
    "    os.mkdir(os.path.join(output_folder, \"fixation\")) # this will have the gaze fixation. \n",
    "    os.mkdir(os.path.join(output_folder, \"blinks\")) # this one is \n",
    "    os.mkdir(os.path.join(output_folder, \"aversion_label\")) # this one is also one per time frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate metadata file\n",
    "video_list_path = os.path.join(*[input_folder, \"video\"])\n",
    "video_list = os.listdir(video_list_path)\n",
    "all_metadata = {}\n",
    "for video in video_list:\n",
    "    if video[0:2] != \"._\" and video != \".\" and video != \"..\":\n",
    "        cap = cv2.VideoCapture(os.path.join(*[video_list_path, video]))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "        height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        metadata = {\"fps\": fps,\n",
    "                    \"width\": width, \n",
    "                    \"height\": height,\n",
    "                    \"frame_count\": frame_count}\n",
    "        all_metadata[video] = metadata\n",
    "video_metadata_path = os.path.join(*[input_folder, \"local_metadata.json\"])\n",
    "json.dump(all_metadata, open(video_metadata_path, \"w\"))\n",
    "\n",
    "# obtain all the file_paths\n",
    "video_metadata_path = os.path.join(*[input_folder, \"local_metadata.json\"])\n",
    "video_metadatas = json.load(open(video_metadata_path))\n",
    "video_names = list(video_metadatas.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently on video 0, Ronen Rubinstein Self Tape.mp4\n",
      "currently on video 1, ‘SWEATER’ DANIEL SELF-TAPE - ZACK FERNANDEZ.mp4\n",
      "currently on video 2, dacre montgomery audition tape.mp4\n",
      "currently on video 3, A self tape I_m very very proud of.mp4\n",
      "currently on video 4, Natalia Dyer - Stranger Things ＂Nancy Wheeler＂  Audition Tape.mp4\n",
      "currently on video 5, Harrison Green self tape reel 2021.mp4\n",
      "currently on video 6, Dramatic Audition Self-Tape “Shameless”.mp4\n",
      "currently on video 7, SELF-TAPE THAT GOT ME BOOKED ｜ Indie Short Film Audition.mp4\n",
      "currently on video 8, Dramatic Self Tape Reel.mp4\n",
      "currently on video 9, Self Tape Audition.mp4\n",
      "currently on video 10, The Audition That Got Me ACCEPTED Into Drama School!.mp4\n",
      "currently on video 11, Therapist - Acting - Audition - Self-tape - by Thain Wesley.mp4\n",
      "currently on video 12, Fabricio Suarez self-tape for Comedic Series.mp4\n",
      "currently on video 13, Self-Tape Demo Reel.mp4\n",
      "currently on video 14, Kelsey Boze Self Tape Reel.mp4\n",
      "currently on video 15, Stefania self tape： Danielle Reading lines with her.mp4\n",
      "currently on video 16, Stephanie Hsu ‘Everything Everywhere All At Once’ Audition.mp4\n",
      "currently on video 17, Monologue Self Tape- Savannah, Dear John.mp4\n",
      "currently on video 18, Euphoria AMI self-tape audition.mp4\n",
      "currently on video 19, Self Tape Audition ＂Amos＂ in ＂My Brothers Keeper＂.mp4\n",
      "currently on video 20, My Pretty Little Liars Audition Tape! ｜ Shay Mitchell.mp4\n",
      "currently on video 21, ＂Transfers＂ Audition Self-tape.mp4\n",
      "currently on video 22, Nicholas Sparrow - Self Tape -The Rock.mp4\n",
      "currently on video 23, Self Taping (Audition) Example of 4 candidates.mp4\n",
      "currently on video 24, Allison Ungar Self Tape.mp4\n",
      "currently on video 25, Skai Jackson’s audition tape for the role of Zoya Lott in the #GossipGirl revival (Viral).mp4\n",
      "currently on video 26, Chernobyl - Actor - Self-tape - Thain Wesley.mp4\n",
      "currently on video 27, Mikey Hurn self-tape for the role of Walter Hopkins.mp4\n",
      "currently on video 28, Child Actor Farts During Self tape.mp4\n",
      "currently on video 29, Rachel McAdams Audition Tape.mp4\n",
      "currently on video 30, Dacre Montgomery - Audition Tape.mp4\n",
      "currently on video 31, Self Tape Audition Example (Drama) #teenactor.mp4\n",
      "currently on video 32, Degrassi High, Alex (self tape acting).mp4\n",
      "currently on video 33, Dramatic Monologue - Fleabag (self-tape style).mp4\n",
      "currently on video 34, Dramatic Self Tape.mp4\n",
      "currently on video 35, Self Tapes I BOOKED!- All American on THE CW- season 3 episode 7- scene 2.mp4\n",
      "currently on video 36, Self Tape Showreel 2022.mp4\n",
      "currently on video 37, DEMO REEL EXAMPLES 2020.mp4\n",
      "currently on video 38, Stranger Things’ Dacre Montgomery’s Insane _Billy_ Audition Tape ｜ GQ.mp4\n",
      "currently on video 39, Amber Pursey, Self- tape.mp4\n",
      "currently on video 40, Neda Ghassemi- Self Tape Demo Reel.mp4\n",
      "currently on video 41, Self-tape.mp4\n",
      "currently on video 42, Libby Lee, Self Tape Reel (2021).mp4\n",
      "currently on video 43, Self-Tape Audition Sample, ＂Hallmark＂ Christmas Film Scene： Veterinarian.mp4\n",
      "currently on video 44, Self-Tape Example 48 Hour Film Festival ｜ How to do a self tape audition How to book your next movie.mp4\n",
      "currently on video 45, 3 Minute Audition Monologue From Search Party ｜ Day 24 of 31 Self Tape Challenge.mp4\n",
      "currently on video 46, SHERRY HSU - Self Tape.mp4\n",
      "currently on video 47, Self Tape ｜ Viola_s ＂I left no ring with her＂ monologue ｜ Twelfth Night.mp4\n",
      "currently on video 48, Self-Tape Audition Reel 2022.mp4\n",
      "currently on video 49, TV Audition Scene Day 30 of 31 Self-Tape Challenge.mp4\n",
      "currently on video 50, Self Tape - Elaine Morillo - Undisclosed Role (RAYN).mp4\n",
      "currently on video 51, Nykki McGee - Self Tape Reel 2020.mp4\n",
      "currently on video 52, Self-tape showreel.mp4\n",
      "currently on video 53, Self-Tape example.mp4\n",
      "currently on video 54, National Youth Theatre Self-Tape Audition.mp4\n",
      "currently on video 55, SELF TAPE Role： Malcolm Wyatt by Anthony Mark Mancini.mp4\n",
      "currently on video 56, Pull My Strings (Self Tape).mp4\n",
      "currently on video 57, CMTC 2019 - Child 4-11 Self-Tape Competition Winner.mp4\n",
      "currently on video 58, NicholasZoto  REEL.mp4\n",
      "currently on video 59, Commercial Self Tape - People Store.mp4\n",
      "currently on video 60, Self-Tape Audition Example Tasmania ｜ How to Get an Agent in Hollywood and Los Angeles ｜ Slate.mp4\n",
      "currently on video 61, Emily Owens - Acting Selftape Reel.mp4\n",
      "currently on video 62, Self Tape Reel.mp4\n",
      "currently on video 63, Self tape 4⧸16.mp4\n",
      "currently on video 64, Acting Self Tape Audition Example - Resentful Daughter.mp4\n",
      "currently on video 65, Self-Tape Audition.mp4\n",
      "currently on video 66, Lilly Niehaus Self Tape Reel.mp4\n",
      "currently on video 67, My Audition Tape for The National Youth Theatre 2020 - Great News Today!!.mp4\n",
      "currently on video 68, SELF TAPE DEMO REEL- Lillian Johnson.mp4\n",
      "currently on video 69, IP-1110 WHEELER Self-Tape Cleo.mp4\n",
      "currently on video 70, Acting self tape 1.mp4\n",
      "currently on video 71, Madelaine Petsch audition for The Prom.mp4\n",
      "currently on video 72, CLYDE SELF-TAPE - ZACK FERNANDEZ.mp4\n",
      "currently on video 73, Self Tape example.mp4\n",
      "currently on video 74, George Diss -Self tape montage.mp4\n",
      "currently on video 75, Freya Callaghan - Self Tapes, March 2022.mp4\n",
      "currently on video 76, Self tape For All Mankind_Iryna.mp4\n",
      "currently on video 77, Self tape.mp4\n",
      "currently on video 78, Hugh Laurie - House MD, Audition Tape.mp4\n",
      "currently on video 79, Oxford School of Drama Audition Tape (I GOT IN!!).mp4\n",
      "currently on video 80, Jake Williams： Self-Tape Reel.mp4\n",
      "currently on video 81, Barb the Awkward Disaster - Self-tape Practice.mp4\n",
      "currently on video 82, Self-Tape： Grosse Pointe Blank.mp4\n",
      "currently on video 83, CMTC 2019 - 12+ Winner of Self-Tape Competition.mp4\n",
      "currently on video 84, Mihail Krastev self-tape.mp4\n",
      "currently on video 85, SELF TAPE SHOWREEL.mp4\n",
      "currently on video 86, Self tape ＂London＂ from Suite life of Zack and Cody.mp4\n",
      "currently on video 87, _Emma_ Self Tape (2).mp4\n",
      "currently on video 88, My audition tape break up scene show reel.mp4\n",
      "currently on video 89, Self-Tape Audition by Louise Chiasson for Wallflower Drama Series.mp4\n",
      "currently on video 90, Self-Tape Challenge - Deborah Lettner  - Final Destination.mp4\n",
      "currently on video 91, Self Tape Demo Reel.mp4\n",
      "currently on video 92, Victoria Summer - Self Tape Audition - Mary Poppins.mp4\n",
      "currently on video 93, ACTING SELF TAPE ｜｜ January 2023 ｜｜ Dream Girl Georgina Allerton Monologue.mp4\n",
      "currently on video 94, Sheriff Jessica Cooper Self-Tape Audition.mp4\n",
      "currently on video 95, Self-tape Dramatic Acting Reel - Rose Eshay.mp4\n",
      "currently on video 96, IP-1110 ⧸ WHEELER ⧸ Self-tape Audition ⧸ Captain Townes Tk1.mp4\n",
      "currently on video 97, CMTC 2019- 1st Runner Up for Child 4-11 Self- Tape Competition.mp4\n",
      "currently on video 98, Rebecca Schmautz Self Tape.mp4\n",
      "currently on video 99, Memory ｜ Theatrical Audition ｜ Acting in Hollywood ｜ Crying on Camera Example Self Tape.mp4\n",
      "currently on video 100, Julia Senise Self Tape.mp4\n",
      "currently on video 101, Aussie self tape reel.mp4\n",
      "currently on video 102, Scottish Detective monologue ｜ Self tape ｜ _Solving murder mysteries aye_.mp4\n",
      "currently on video 103, Henry Thomas audition för ET ＂Ok kid, you got the job＂.mp4\n",
      "currently on video 104, Delilah Self Tape Audition ＂The After Party＂.mp4\n",
      "currently on video 105, Audition to Booking   OZARK.mp4\n",
      "currently on video 106, Anthony J Tremé - ＂Sands of Shrive＂ - Role： Ray  (self-tape audition).mp4\n",
      "currently on video 107, Phillip Lanos ｜ Self tape ｜ Audition.mp4\n",
      "currently on video 108, Anthony J Tremé - ＂ Driver Ed＂ - Role： Clay   (self-tape audition).mp4\n",
      "currently on video 109, Hannah Crowe Self tape-short horror film.mp4\n",
      "currently on video 110, Ryan Johns - Self Tape Reel.mp4\n"
     ]
    }
   ],
   "source": [
    "metadata = {}\n",
    "output_file_names = []\n",
    "output_file_fps = []\n",
    "output_file_sr = []\n",
    "output_file_audio_length = []\n",
    "output_file_annotation_length = []\n",
    "output_file_video_interval = []\n",
    "output_file_audio_interval = []\n",
    "# for i in range(0, 3):\n",
    "for i in range(0, len(video_names)):\n",
    "    print(\"currently on video {}, {}\".format(i, video_names[i]))\n",
    "    TESTING = True\n",
    "    # load the data for one video\n",
    "    file_name_video = video_names[i]\n",
    "    file_name = file_name_video.split(\".\")[0]\n",
    "    metadata = video_metadatas[file_name_video]\n",
    "    fps = metadata[\"fps\"]\n",
    "\n",
    "    # get file_paths\n",
    "    audio_path = os.path.join(*[input_folder, \"audio\", file_name + \".wav\"])\n",
    "    # annotation\n",
    "    gaze_direction_path = os.path.join(*[input_folder, \"ETHGaze-Mod\", file_name+\".pkl\"])\n",
    "    head_direction_path = os.path.join(*[input_folder, \"pose\", file_name+\".pkl\"])\n",
    "    diarization_path = os.path.join(*[input_folder, \"tracklets\", file_name+\"_Speakers.json\"])\n",
    "    # shots\n",
    "    shot_path = os.path.join(*[input_folder, \"shots\", file_name, \"shot_cuts.json\"])\n",
    "    try:\n",
    "        shots = json.load(open(shot_path))[\"shots\"]\n",
    "    except:\n",
    "        shot_path = os.path.join(*[input_folder, \"shots\", file_name, file_name, \"shot_cuts.json\"])\n",
    "        shots = json.load(open(shot_path))[\"shots\"]\n",
    "    # video\n",
    "    all_gaze_data = pkl.load(open(gaze_direction_path, \"rb\"))\n",
    "    all_head_data = pkl.load(open(head_direction_path, \"rb\"))\n",
    "    gaze, neck = load_head_and_gaze_angles(all_gaze_data, all_head_data) # each one is of shape [N, 2] (the two angles are asimuth)\n",
    "    blinks = all_head_data[\"BLINKS\"] # of shapa [N, ], 1 = eye close, 0 = eye open\n",
    "\n",
    "    # audio\n",
    "    try:\n",
    "        audio, sr = librosa.load(str(audio_path))\n",
    "    except:\n",
    "        print(\"failed for video: {}\".format(i))\n",
    "        continue\n",
    "    speaker = json.load(open(diarization_path))[\"aligned\"] # {speaker_id: [{\"start\": t, \"end\": t, \"start_frame\":frame, \"end_frame\":frame}]}\n",
    "\n",
    "    # obtain the valid shots for this \n",
    "    valid_shots_time, valid_shots_frame = get_valid_shots(shots, fps, 5)\n",
    "\n",
    "    # all the annotation data\n",
    "    gazes_per_shot = []\n",
    "    head_per_shot = []\n",
    "    blink_per_shot = []\n",
    "    for j in range(0, len(valid_shots_time)):\n",
    "        time_range = valid_shots_time[j]\n",
    "        frame_range = valid_shots_frame[j]\n",
    "        gaze_in_shot = gaze[frame_range[0]:frame_range[1]]\n",
    "        gazes_per_shot.append(gaze_in_shot)\n",
    "        head_in_shot = neck[frame_range[0]:frame_range[1]]\n",
    "        head_per_shot.append(head_in_shot)\n",
    "        blink_in_shot = blinks[frame_range[0]:frame_range[1]]\n",
    "        blink_per_shot.append(blink_in_shot)\n",
    "        output_file_video_interval.append(frame_range)\n",
    "        # do stuff to them here\n",
    "\n",
    "    # identify the speaker in each shot\n",
    "    speaker_id_per_shot = []\n",
    "    speaker_ids_with_off_screen = list(speaker.keys())\n",
    "    # here we wish to ignore Off-Screen\n",
    "    speaker_ids = []\n",
    "    for i in range(0, len(speaker_ids_with_off_screen)):\n",
    "        if speaker_ids_with_off_screen[i] != \"OFF-SCREEN\":\n",
    "            speaker_ids.append(speaker_ids_with_off_screen[i])\n",
    "    auds = []\n",
    "    if len(speaker_ids) == 0:\n",
    "        for j in range(0, len(valid_shots_time)):\n",
    "            # get the duration of the shot\n",
    "            shot_range_time = valid_shots_time[j]\n",
    "            shot_range_frames = valid_shots_frame[j]\n",
    "            # get the audio to only include the shot range (two tracks. one for speaker one for listener) \n",
    "            audio_start = int(shot_range_time[0] * sr)\n",
    "            audio_end = np.minimum(int(shot_range_time[1] * sr), audio.shape[0])\n",
    "            audio_of_shot = audio[audio_start:audio_end]\n",
    "            on_screen_bitmap = np.ones(audio_of_shot.shape)\n",
    "            off_screen_bitmap = np.zeros(audio_of_shot.shape)\n",
    "            audio_on_screen = audio_of_shot * on_screen_bitmap\n",
    "            audio_off_screen = audio_of_shot * off_screen_bitmap\n",
    "            # store this for later\n",
    "            output_file_audio_interval.append([int(audio_start), int(audio_end)])\n",
    "            auds.append([audio_on_screen, audio_off_screen])\n",
    "    elif len(speaker_ids) > 0:\n",
    "        for j in range(0, len(valid_shots_time)):\n",
    "            valid_shot_turn = valid_shots_frame[j]\n",
    "            # create list to store the percentage overlap between the speaker's turn and the current shot\n",
    "            speaker_overlaps = []\n",
    "            for id in range(0, len(speaker_ids)):\n",
    "                speaker_overlaps.append(0)\n",
    "            # iterate through each speaker to find their overlap\n",
    "            for id in range(0, len(speaker_ids)):\n",
    "                speaker_activities = speaker[speaker_ids[id]]\n",
    "                # iterate through each speech to sum up the overlapp\n",
    "                for turn in range(0, len(speaker_activities)):\n",
    "                    speech_interval = [speaker_activities[turn][\"start_frame\"], speaker_activities[turn][\"end_frame\"]]\n",
    "                    # find overlapp\n",
    "                    if np.maximum(speech_interval[0], valid_shot_turn[0]) <= np.minimum(speech_interval[1], valid_shot_turn[1]):\n",
    "                        speaker_overlaps[id] = speaker_overlaps[id] + 1\n",
    "            speaker_id_per_shot.append(speaker_ids[np.argmax(speaker_overlaps)])\n",
    "\n",
    "        # parse audio for each shot (2 audio per shot)\n",
    "        t0 = datetime.strptime(\"00:00:00.0\", '%H:%M:%S.%f').timestamp()\n",
    "        # start = datetime.strptime(shots[i][0], '%H:%M:%S.%f').timestamp()\n",
    "        for j in range(0, len(valid_shots_time)):\n",
    "            # get the duration of the shot\n",
    "            shot_range_time = valid_shots_time[j]\n",
    "            shot_range_frames = valid_shots_frame[j]\n",
    "            # get the speaker activity of the speaker \n",
    "            speaker_activity = speaker[speaker_id_per_shot[j]]\n",
    "            # get the audio to only include the shot range (two tracks. one for speaker one for listener) \n",
    "            audio_start = int(shot_range_time[0] * sr)\n",
    "            audio_end = np.minimum(int(shot_range_time[1] * sr), audio.shape[0])\n",
    "            audio_of_shot = audio[audio_start:audio_end]\n",
    "            on_screen_bitmap = np.zeros(audio_of_shot.shape)\n",
    "            off_screen_bitmap = np.ones(audio_of_shot.shape)\n",
    "            output_file_audio_interval.append([int(audio_start), int(audio_end)])\n",
    "            # parse the audio to get a bitmap of speech turn \n",
    "            for interval_i in range(0,len(speaker_activity)):\n",
    "                # get the start and end of the current speaker turn\n",
    "                turn_start = speaker_activity[interval_i][\"start\"]\n",
    "                turn_end = speaker_activity[interval_i][\"end\"]\n",
    "                # turn it into numbers, and make sure that 0 is the start of the shot not the video\n",
    "                turn_start = datetime.strptime(turn_start, '%H:%M:%S.%f').timestamp() - t0\n",
    "                turn_end = datetime.strptime(turn_end, '%H:%M:%S.%f').timestamp() - t0\n",
    "                # get the same thing in frames\n",
    "                turn_start_frame = int(turn_start * sr) - audio_start\n",
    "                turn_end_frame = int(turn_end * sr) - audio_start\n",
    "                on_screen_bitmap[turn_start_frame:turn_end_frame] = on_screen_bitmap[turn_start_frame:turn_end_frame] + 1\n",
    "            off_screen_bitmap = off_screen_bitmap - on_screen_bitmap\n",
    "            audio_on_screen = audio_of_shot * on_screen_bitmap\n",
    "            audio_off_screen = audio_of_shot * off_screen_bitmap\n",
    "            auds.append([audio_on_screen, audio_off_screen])\n",
    "\n",
    "    for j in range(0, len(auds)):\n",
    "        output_audio_onscreen_path = os.path.join(*[output_folder, \"taudio\", file_name+\"_{}_{}.wav\".format(j, 0)]) \n",
    "        output_audio_offscreen_path = os.path.join(*[output_folder, \"taudio\", file_name+\"_{}_{}.wav\".format(j, 1)]) \n",
    "        output_gaze_path = os.path.join(*[output_folder, \"tgaze\", file_name+\"_{}.pkl\".format(j)]) \n",
    "        output_head_path = os.path.join(*[output_folder, \"thead\", file_name+\"_{}.pkl\".format(j)]) \n",
    "        output_blinks_path = os.path.join(*[output_folder, \"tblinks\", file_name+\"_{}.pkl\".format(j)]) \n",
    "        # annotation files\n",
    "        pkl.dump(gazes_per_shot[j], open(output_gaze_path,  \"wb\"))\n",
    "        pkl.dump(head_per_shot[j], open(output_head_path,  \"wb\"))\n",
    "        pkl.dump(blink_per_shot[j], open(output_blinks_path,  \"wb\"))\n",
    "        sf.write(output_audio_onscreen_path, auds[j][0], sr)\n",
    "        sf.write(output_audio_offscreen_path, auds[j][1], sr)\n",
    "        output_file_names.append(file_name+\"_{}\".format(j))\n",
    "        output_file_fps.append(fps)\n",
    "        output_file_sr.append(sr)\n",
    "        output_file_audio_length.append(int(auds[j][0].shape[0]))\n",
    "        output_file_annotation_length.append(int(gazes_per_shot[j].shape[0]))\n",
    "output_json = {\"data\":[]}\n",
    "for i in range(0, len(output_file_names)):\n",
    "    output_json[\"data\"].append({\"name\":output_file_names[i],\n",
    "                               \"fps\":output_file_fps[i],\n",
    "                               \"sr\":output_file_sr[i],\n",
    "                               \"audio_length\":output_file_audio_length[i],\n",
    "                               \"annotation_length\":output_file_annotation_length[i], \n",
    "                               \"audio_range\": output_file_audio_interval[i],\n",
    "                               \"video_range\": output_file_video_interval[i]})\n",
    "json.dump(output_json, open(output_json_path, \"w\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = whisper_timestamped.load_model(\"tiny.en\")\n",
    "output_json = json.load(open(output_json_path, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4708/4708 [00:10<00:00, 435.37frames/s]\n",
      "100%|██████████| 3210/3210 [00:06<00:00, 461.14frames/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     trascript_json \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m:word_alignment}\n\u001b[1;32m     13\u001b[0m     json\u001b[39m.\u001b[39mdump(trascript_json, \u001b[39mopen\u001b[39m(output_text_file_path, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m A[\u001b[39m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "# [sr, audio_oEnscreen, audio_offscreen], [fps, gaze, head, blinks, aversion], [file_name, shot_range] = dataset.get_video(29)\n",
    "for i in range(0, len(output_json[\"data\"])):\n",
    "    file_name = output_json[\"data\"][i][\"name\"]\n",
    "    for speaker in range(0, 2):\n",
    "        file_path = os.path.join(*[output_folder, \"taudio\", file_name+\"_{}.wav\".format(speaker)])\n",
    "        output_text_file_path = os.path.join(*[output_folder, \"ttext\", file_name+\"_{}.json\".format(speaker)])\n",
    "        # get word alignment result\n",
    "        result_word = whisper_timestamped.transcribe(model_word, file_path, beam_size=5, best_of=5, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0), vad=True)\n",
    "        word_alignment = []\n",
    "        for s in range(0,len(result_word[\"segments\"])):\n",
    "            word_alignment = word_alignment + result_word[\"segments\"][s][\"words\"]\n",
    "        trascript_json = {\"text\":word_alignment}\n",
    "        json.dump(trascript_json, open(output_text_file_path, \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/EVAN_DISK/MASC/deep_learning_processed_dataset/ttext/Ronen Rubinstein Self Tape_0_0.json'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text_file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
