{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import json\n",
    "import torch\n",
    "import librosa\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import soundfile as sf\n",
    "import whisper_timestamped\n",
    "# import utility functions\n",
    "sys.path.insert(0, '/Users/evanpan/Documents/GitHub/EvansToolBox/Utils')\n",
    "# sys.path.insert(0, \"C:/Users/evansamaa/Documents/GitHub/EvansToolBox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_angles_frome_positions(arr):\n",
    "    \"\"\"\n",
    "    converts an array of positions to an array of rotation angles (azimuth, elevation)\n",
    "    centered at the origin, where:\n",
    "        azimuth: +right,-left\n",
    "        elevation: +up,-down\n",
    "    here we assume that the input vectors are in world coordinates\n",
    "    :param arr: array with shape (N, 3)\n",
    "    :return: array with shape (N, 2)\n",
    "    \"\"\"\n",
    "    # F: arr (N, 3) -> arr (N, 2) or arr (3, ) -> (2, )\n",
    "    # in the output is in the convention of (azimuth, elevation)\n",
    "    if len(arr.shape) == 2:\n",
    "        mag = np.sqrt(np.sum(arr * arr, axis=1, keepdims=True))\n",
    "        out = arr / mag\n",
    "        out[:, 0] = np.arcsin(out[:, 0])\n",
    "        out[:, 1] = np.arcsin(out[:, 1])\n",
    "        return out[:, 0:2] * 180 / np.pi\n",
    "    else:\n",
    "        mag = np.sqrt(np.sum(arr * arr))\n",
    "        out = arr / mag\n",
    "        out[0] = np.arcsin(out[0])\n",
    "        out[1] = np.arcsin(out[1])\n",
    "        return out[0:2] * 180 / np.pi\n",
    "def get_valid_shots(shots, fps, shot_length_mininmum=5):\n",
    "    t0 = datetime.strptime(\"00:00:00.0\", '%H:%M:%S.%f').timestamp()\n",
    "    for shot in shots:\n",
    "        start = shot[0]\n",
    "        end = shot[1]\n",
    "    # load the input shots range\n",
    "    valid_shots_time, valid_shots_frames = [], []\n",
    "    t0 = datetime.strptime(\"00:00:00.0\", '%H:%M:%S.%f').timestamp()\n",
    "    for i in range(len(shots)):\n",
    "        start = datetime.strptime(shots[i][0], '%H:%M:%S.%f').timestamp()\n",
    "        end = datetime.strptime(shots[i][1], '%H:%M:%S.%f').timestamp()\n",
    "        if (end-start) >= shot_length_mininmum:\n",
    "            start_t = start-t0\n",
    "            end_t = end - t0\n",
    "            valid_shots_time.append([start-t0, end-t0])\n",
    "            valid_shots_frames.append([int(np.round(start_t*fps)), int(np.round(end_t*fps))])\n",
    "\n",
    "    return valid_shots_time, valid_shots_frames\n",
    "def load_head_and_gaze_angles(all_gaze_data, all_head_data):\n",
    "\n",
    "    # head data\n",
    "    head_angle_data = all_head_data[\"HEAD\"]\n",
    "    head_rotmat_per_frame = head_angle_data[\"ROTMAT\"]\n",
    "    head_bbox_per_frame = all_head_data[\"BBOX\"] # we are not using but having it here is nice\n",
    "    head_angle_per_frame = []\n",
    "    neutral_position = np.array([0, 0, 100])\n",
    "    for i in range(0, head_rotmat_per_frame.shape[0]):\n",
    "        pos = head_rotmat_per_frame[i] @ neutral_position\n",
    "        head_angle_per_frame.append(rotation_angles_frome_positions(pos[:]))\n",
    "    head_angle_per_frame = np.array(head_angle_per_frame)\n",
    "    # getting rotation angle in z direction\n",
    "    neutral_position2 = np.array([0, 100, 0])\n",
    "    head_angle_z_per_frame = []\n",
    "    for i in range(0, head_rotmat_per_frame.shape[0]):\n",
    "        pos = head_rotmat_per_frame[i] @ neutral_position2\n",
    "        pos = np.array([pos[1], pos[2], pos[0]])\n",
    "        head_angle_z_per_frame.append(rotation_angles_frome_positions(pos)[1])\n",
    "    head_angle_xy_per_frame = np.array(head_angle_per_frame)\n",
    "    head_angle_z_per_frame = np.expand_dims(np.array(head_angle_z_per_frame), axis=1)\n",
    "    head_angle_per_frame = np.concatenate([head_angle_xy_per_frame, head_angle_z_per_frame], axis=1)\n",
    "\n",
    "    # getting gaze data\n",
    "    gaze_angle_data = all_gaze_data[\"RAW_GAZE\"]\n",
    "    gaze_angle_per_frame = gaze_angle_data[\"EULER\"]\n",
    "    gaze_rotmat_per_frame = gaze_angle_data[\"ROTMAT\"]\n",
    "    blinks = all_head_data[\"BLINKS\"]\n",
    "    gaze_vec = np.array([0, 0, 100])\n",
    "    eye_angle_per_frame = []\n",
    "    for i in range(0, gaze_rotmat_per_frame.shape[0]):\n",
    "        eye_line = gaze_rotmat_per_frame[i] @ gaze_vec\n",
    "        eye_line = eye_line / eye_line[2] * 100\n",
    "        eye_angle_per_frame.append(eye_line)\n",
    "    eye_angle_per_frame = np.array(eye_angle_per_frame)\n",
    "    eye_angle_per_frame = rotation_angles_frome_positions(eye_angle_per_frame[:])\n",
    "    return eye_angle_per_frame, head_angle_per_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"/Volumes/EVAN_DISK/MASC/Ribhav_processed_dataset/\"\n",
    "output_folder = \"/Volumes/EVAN_DISK/MASC/deep_learning_processed_dataset/\"\n",
    "input_folder = \"/scratch/ondemand27/evanpan/data/Ribshabh_processed_dataset/\"\n",
    "output_folder = \"/scratch/ondemand27/evanpan/data/deep_learning_processed_dataset/\"\n",
    "\n",
    "\n",
    "redo = False\n",
    "\n",
    "target_fps = 24\n",
    "window_length = 20\n",
    "stride_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json_path = os.path.join(output_folder, \"metadata.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Folder Structure Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Deal with the output folder structures:\n",
    "# remove everything in the output folder\n",
    "if redo:\n",
    "    try:\n",
    "        os.mkdir(output_folder)\n",
    "    except:\n",
    "        shutil.rmtree(output_folder)\n",
    "        os.mkdir(output_folder)\n",
    "        \n",
    "    # this set is temporary\n",
    "    os.mkdir(os.path.join(output_folder, \"taudio\")) # this one will have MFCC, intensity, \n",
    "    os.mkdir(os.path.join(output_folder, \"ttext\")) # this one give the text per \n",
    "    os.mkdir(os.path.join(output_folder, \"tgaze\")) # this will store the per time-stamp.\n",
    "    os.mkdir(os.path.join(output_folder, \"thead\")) # this one one is also per time-stamp\n",
    "    os.mkdir(os.path.join(output_folder, \"tfixation\")) # this will have the gaze fixation. \n",
    "    os.mkdir(os.path.join(output_folder, \"tblinks\")) # this one is \n",
    "    os.mkdir(os.path.join(output_folder, \"taversion_label\")) # this one is also one per time frame\n",
    "\n",
    "    # this set is permanant\n",
    "    os.mkdir(os.path.join(output_folder, \"audio\")) # this one will have MFCC, intensity, \n",
    "    os.mkdir(os.path.join(output_folder, \"text\")) # this one give the text per \n",
    "    os.mkdir(os.path.join(output_folder, \"gaze\")) # this will store the per time-stamp.\n",
    "    os.mkdir(os.path.join(output_folder, \"head\")) # this one one is also per time-stamp\n",
    "    os.mkdir(os.path.join(output_folder, \"fixation\")) # this will have the gaze fixation. \n",
    "    os.mkdir(os.path.join(output_folder, \"blinks\")) # this one is \n",
    "    os.mkdir(os.path.join(output_folder, \"aversion_label\")) # this one is also one per time frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/ondemand27/evanpan/data/Ribshabh_processed_dataset/video'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# generate metadata file\u001b[39;00m\n\u001b[1;32m      2\u001b[0m video_list_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m*\u001b[39m[input_folder, \u001b[39m\"\u001b[39m\u001b[39mvideo\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m video_list \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(video_list_path)\n\u001b[1;32m      4\u001b[0m all_metadata \u001b[39m=\u001b[39m {}\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m video \u001b[39min\u001b[39;00m video_list:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/ondemand27/evanpan/data/Ribshabh_processed_dataset/video'"
     ]
    }
   ],
   "source": [
    "# generate metadata file\n",
    "video_list_path = os.path.join(*[input_folder, \"video\"])\n",
    "video_list = os.listdir(video_list_path)\n",
    "all_metadata = {}\n",
    "for video in video_list:\n",
    "    if video[0:2] != \"._\" and video != \".\" and video != \"..\":\n",
    "        cap = cv2.VideoCapture(os.path.join(*[video_list_path, video]))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "        height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        metadata = {\"fps\": fps,\n",
    "                    \"width\": width, \n",
    "                    \"height\": height,\n",
    "                    \"frame_count\": frame_count}\n",
    "        all_metadata[video] = metadata\n",
    "video_metadata_path = os.path.join(*[input_folder, \"local_metadata.json\"])\n",
    "json.dump(all_metadata, open(video_metadata_path, \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# obtain all the file_paths\n",
    "video_metadata_path = os.path.join(*[input_folder, \"local_metadata.json\"])\n",
    "video_metadatas = json.load(open(video_metadata_path))\n",
    "video_names = list(video_metadatas.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently on video 103, Henry Thomas audition för ET ＂Ok kid, you got the job＂.mp4\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/ondemand27/evanpan/data/Ribshabh_processed_dataset/ETHGaze-Mod/Henry Thomas audition för ET ＂Ok kid, you got the job＂.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m diarization_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m*\u001b[39m[input_folder, \u001b[39m\"\u001b[39m\u001b[39mtracklets\u001b[39m\u001b[39m\"\u001b[39m, file_name\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_Speakers.json\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     25\u001b[0m \u001b[39m# video\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m all_gaze_data \u001b[39m=\u001b[39m pkl\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39;49m(gaze_direction_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     27\u001b[0m all_head_data \u001b[39m=\u001b[39m pkl\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(head_direction_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     28\u001b[0m gaze, neck \u001b[39m=\u001b[39m load_head_and_gaze_angles(all_gaze_data, all_head_data) \u001b[39m# each one is of shape [N, 2] (the two angles are asimuth)\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/ondemand27/evanpan/data/Ribshabh_processed_dataset/ETHGaze-Mod/Henry Thomas audition för ET ＂Ok kid, you got the job＂.pkl'"
     ]
    }
   ],
   "source": [
    "metadata = {}\n",
    "output_file_names = []\n",
    "output_file_fps = []\n",
    "output_file_sr = []\n",
    "output_file_audio_length = []\n",
    "output_file_annotation_length = []\n",
    "output_file_video_interval = []\n",
    "output_file_audio_interval = []\n",
    "# for i in range(0, 3):\n",
    "for i in range(103, len(video_names)):\n",
    "    print(\"currently on video {}, {}\".format(i, video_names[i]))\n",
    "    TESTING = True\n",
    "    # load the data for one video\n",
    "    file_name_video = video_names[i]\n",
    "    file_name = file_name_video.split(\".\")[0]\n",
    "    metadata = video_metadatas[file_name_video]\n",
    "    fps = metadata[\"fps\"]\n",
    "\n",
    "    # get file_paths\n",
    "    audio_path = os.path.join(*[input_folder, \"audio\", file_name + \".wav\"])\n",
    "    # annotation\n",
    "    gaze_direction_path = os.path.join(*[input_folder, \"ETHGaze-Mod\", file_name+\".pkl\"])\n",
    "    head_direction_path = os.path.join(*[input_folder, \"pose\", file_name+\".pkl\"])\n",
    "    diarization_path = os.path.join(*[input_folder, \"tracklets\", file_name+\"_Speakers.json\"])\n",
    "    \n",
    "    # video\n",
    "    try:\n",
    "        all_gaze_data = pkl.load(open(gaze_direction_path, \"rb\"))\n",
    "        all_head_data = pkl.load(open(head_direction_path, \"rb\"))\n",
    "        gaze, neck = load_head_and_gaze_angles(all_gaze_data, all_head_data) # each one is of shape [N, 2] (the two angles are asimuth)\n",
    "        blinks = all_head_data[\"BLINKS\"] # of shapa [N, ], 1 = eye close, 0 = eye open\n",
    "    except:\n",
    "        print(\"failed for video: {}, file not found\".format(i))\n",
    "        continue\n",
    "    # shots\n",
    "    shot_path = os.path.join(*[input_folder, \"shots\", file_name, \"shot_cuts.json\"])\n",
    "    try:\n",
    "        shots = json.load(open(shot_path))[\"shots\"]\n",
    "    except:\n",
    "        shot_path = os.path.join(*[input_folder, \"shots\", file_name, file_name, \"shot_cuts.json\"])\n",
    "        shots = json.load(open(shot_path))[\"shots\"]\n",
    "\n",
    "    # audio\n",
    "    try:\n",
    "        audio, sr = librosa.load(str(audio_path))\n",
    "    except:\n",
    "        print(\"failed for video: {}\".format(i))\n",
    "        continue\n",
    "    speaker = json.load(open(diarization_path))[\"aligned\"] # {speaker_id: [{\"start\": t, \"end\": t, \"start_frame\":frame, \"end_frame\":frame}]}\n",
    "\n",
    "    # obtain the valid shots for this \n",
    "    valid_shots_time, valid_shots_frame = get_valid_shots(shots, fps, 5)\n",
    "\n",
    "    # all the annotation data\n",
    "    gazes_per_shot = []\n",
    "    head_per_shot = []\n",
    "    blink_per_shot = []\n",
    "    for j in range(0, len(valid_shots_time)):\n",
    "        time_range = valid_shots_time[j]\n",
    "        frame_range = valid_shots_frame[j]\n",
    "        gaze_in_shot = gaze[frame_range[0]:frame_range[1]]\n",
    "        gazes_per_shot.append(gaze_in_shot)\n",
    "        head_in_shot = neck[frame_range[0]:frame_range[1]]\n",
    "        head_per_shot.append(head_in_shot)\n",
    "        blink_in_shot = blinks[frame_range[0]:frame_range[1]]\n",
    "        blink_per_shot.append(blink_in_shot)\n",
    "        output_file_video_interval.append(frame_range)\n",
    "        # do stuff to them here\n",
    "\n",
    "    # identify the speaker in each shot\n",
    "    speaker_id_per_shot = []\n",
    "    speaker_ids_with_off_screen = list(speaker.keys())\n",
    "    # here we wish to ignore Off-Screen\n",
    "    speaker_ids = []\n",
    "    for i in range(0, len(speaker_ids_with_off_screen)):\n",
    "        if speaker_ids_with_off_screen[i] != \"OFF-SCREEN\":\n",
    "            speaker_ids.append(speaker_ids_with_off_screen[i])\n",
    "    auds = []\n",
    "    if len(speaker_ids) == 0:\n",
    "        for j in range(0, len(valid_shots_time)):\n",
    "            # get the duration of the shot\n",
    "            shot_range_time = valid_shots_time[j]\n",
    "            shot_range_frames = valid_shots_frame[j]\n",
    "            # get the audio to only include the shot range (two tracks. one for speaker one for listener) \n",
    "            audio_start = int(shot_range_time[0] * sr)\n",
    "            audio_end = np.minimum(int(shot_range_time[1] * sr), audio.shape[0])\n",
    "            audio_of_shot = audio[audio_start:audio_end]\n",
    "            on_screen_bitmap = np.ones(audio_of_shot.shape)\n",
    "            off_screen_bitmap = np.zeros(audio_of_shot.shape)\n",
    "            audio_on_screen = audio_of_shot * on_screen_bitmap\n",
    "            audio_off_screen = audio_of_shot * off_screen_bitmap\n",
    "            # store this for later\n",
    "            output_file_audio_interval.append([int(audio_start), int(audio_end)])\n",
    "            auds.append([audio_on_screen, audio_off_screen])\n",
    "    elif len(speaker_ids) > 0:\n",
    "        for j in range(0, len(valid_shots_time)):\n",
    "            valid_shot_turn = valid_shots_frame[j]\n",
    "            # create list to store the percentage overlap between the speaker's turn and the current shot\n",
    "            speaker_overlaps = []\n",
    "            for id in range(0, len(speaker_ids)):\n",
    "                speaker_overlaps.append(0)\n",
    "            # iterate through each speaker to find their overlap\n",
    "            for id in range(0, len(speaker_ids)):\n",
    "                speaker_activities = speaker[speaker_ids[id]]\n",
    "                # iterate through each speech to sum up the overlapp\n",
    "                for turn in range(0, len(speaker_activities)):\n",
    "                    speech_interval = [speaker_activities[turn][\"start_frame\"], speaker_activities[turn][\"end_frame\"]]\n",
    "                    # find overlapp\n",
    "                    if np.maximum(speech_interval[0], valid_shot_turn[0]) <= np.minimum(speech_interval[1], valid_shot_turn[1]):\n",
    "                        speaker_overlaps[id] = speaker_overlaps[id] + 1\n",
    "            speaker_id_per_shot.append(speaker_ids[np.argmax(speaker_overlaps)])\n",
    "\n",
    "        # parse audio for each shot (2 audio per shot)\n",
    "        t0 = datetime.strptime(\"00:00:00.0\", '%H:%M:%S.%f').timestamp()\n",
    "        # start = datetime.strptime(shots[i][0], '%H:%M:%S.%f').timestamp()\n",
    "        for j in range(0, len(valid_shots_time)):\n",
    "            # get the duration of the shot\n",
    "            shot_range_time = valid_shots_time[j]\n",
    "            shot_range_frames = valid_shots_frame[j]\n",
    "            # get the speaker activity of the speaker \n",
    "            speaker_activity = speaker[speaker_id_per_shot[j]]\n",
    "            # get the audio to only include the shot range (two tracks. one for speaker one for listener) \n",
    "            audio_start = int(shot_range_time[0] * sr)\n",
    "            audio_end = np.minimum(int(shot_range_time[1] * sr), audio.shape[0])\n",
    "            audio_of_shot = audio[audio_start:audio_end]\n",
    "            on_screen_bitmap = np.zeros(audio_of_shot.shape)\n",
    "            off_screen_bitmap = np.ones(audio_of_shot.shape)\n",
    "            output_file_audio_interval.append([int(audio_start), int(audio_end)])\n",
    "            # parse the audio to get a bitmap of speech turn \n",
    "            for interval_i in range(0,len(speaker_activity)):\n",
    "                # get the start and end of the current speaker turn\n",
    "                turn_start = speaker_activity[interval_i][\"start\"]\n",
    "                turn_end = speaker_activity[interval_i][\"end\"]\n",
    "                # turn it into numbers, and make sure that 0 is the start of the shot not the video\n",
    "                turn_start = datetime.strptime(turn_start, '%H:%M:%S.%f').timestamp() - t0\n",
    "                turn_end = datetime.strptime(turn_end, '%H:%M:%S.%f').timestamp() - t0\n",
    "                # get the same thing in frames\n",
    "                turn_start_frame = int(turn_start * sr) - audio_start\n",
    "                turn_end_frame = int(turn_end * sr) - audio_start\n",
    "                on_screen_bitmap[turn_start_frame:turn_end_frame] = on_screen_bitmap[turn_start_frame:turn_end_frame] + 1\n",
    "            off_screen_bitmap = off_screen_bitmap - on_screen_bitmap\n",
    "            audio_on_screen = audio_of_shot * on_screen_bitmap\n",
    "            audio_off_screen = audio_of_shot * off_screen_bitmap\n",
    "            auds.append([audio_on_screen, audio_off_screen])\n",
    "\n",
    "    for j in range(0, len(auds)):\n",
    "        output_audio_onscreen_path = os.path.join(*[output_folder, \"taudio\", file_name+\"_{}_{}.wav\".format(j, 0)]) \n",
    "        output_audio_offscreen_path = os.path.join(*[output_folder, \"taudio\", file_name+\"_{}_{}.wav\".format(j, 1)]) \n",
    "        output_gaze_path = os.path.join(*[output_folder, \"tgaze\", file_name+\"_{}.pkl\".format(j)]) \n",
    "        output_head_path = os.path.join(*[output_folder, \"thead\", file_name+\"_{}.pkl\".format(j)]) \n",
    "        output_blinks_path = os.path.join(*[output_folder, \"tblinks\", file_name+\"_{}.pkl\".format(j)]) \n",
    "        # annotation files\n",
    "        pkl.dump(gazes_per_shot[j], open(output_gaze_path,  \"wb\"))\n",
    "        pkl.dump(head_per_shot[j], open(output_head_path,  \"wb\"))\n",
    "        pkl.dump(blink_per_shot[j], open(output_blinks_path,  \"wb\"))\n",
    "        sf.write(output_audio_onscreen_path, auds[j][0], sr)\n",
    "        sf.write(output_audio_offscreen_path, auds[j][1], sr)\n",
    "        output_file_names.append(file_name+\"_{}\".format(j))\n",
    "        output_file_fps.append(fps)\n",
    "        output_file_sr.append(sr)\n",
    "        output_file_audio_length.append(int(auds[j][0].shape[0]))\n",
    "        output_file_annotation_length.append(int(gazes_per_shot[j].shape[0]))\n",
    "output_json = {\"data\":[]}\n",
    "for i in range(0, len(output_file_names)):\n",
    "    output_json[\"data\"].append({\"name\":output_file_names[i],\n",
    "                               \"fps\":output_file_fps[i],\n",
    "                               \"sr\":output_file_sr[i],\n",
    "                               \"audio_length\":output_file_audio_length[i],\n",
    "                               \"annotation_length\":output_file_annotation_length[i], \n",
    "                               \"audio_range\": output_file_audio_interval[i],\n",
    "                               \"video_range\": output_file_video_interval[i]})\n",
    "json.dump(output_json, open(output_json_path, \"w\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word = whisper_timestamped.load_model(\"tiny.en\")\n",
    "output_json = json.load(open(output_json_path, \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4708/4708 [00:10<00:00, 435.37frames/s]\n",
      "100%|██████████| 3210/3210 [00:06<00:00, 461.14frames/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     trascript_json \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m:word_alignment}\n\u001b[1;32m     13\u001b[0m     json\u001b[39m.\u001b[39mdump(trascript_json, \u001b[39mopen\u001b[39m(output_text_file_path, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m A[\u001b[39m2\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "# [sr, audio_oEnscreen, audio_offscreen], [fps, gaze, head, blinks, aversion], [file_name, shot_range] = dataset.get_video(29)\n",
    "for i in range(0, len(output_json[\"data\"])):\n",
    "    file_name = output_json[\"data\"][i][\"name\"]\n",
    "    for speaker in range(0, 2):\n",
    "        file_path = os.path.join(*[output_folder, \"taudio\", file_name+\"_{}.wav\".format(speaker)])\n",
    "        output_text_file_path = os.path.join(*[output_folder, \"ttext\", file_name+\"_{}.json\".format(speaker)])\n",
    "        # get word alignment result\n",
    "        result_word = whisper_timestamped.transcribe(model_word, file_path, beam_size=5, best_of=5, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0), vad=True)\n",
    "        word_alignment = []\n",
    "        for s in range(0,len(result_word[\"segments\"])):\n",
    "            word_alignment = word_alignment + result_word[\"segments\"][s][\"words\"]\n",
    "        trascript_json = {\"text\":word_alignment}\n",
    "        json.dump(trascript_json, open(output_text_file_path, \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/EVAN_DISK/MASC/deep_learning_processed_dataset/ttext/Ronen Rubinstein Self Tape_0_0.json'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text_file_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
