{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\EVANSA~1\\AppData\\Local\\Temp/ipykernel_22060/1860163269.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBinaryF1Score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mF1Score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchmetrics'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from torchmetrics.classification import BinaryF1Score, F1Score\n",
    "import wandb\n",
    "from tqdm import trange\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '/Users/evanpan/Documents/GitHub/EvansToolBox/Utils')\n",
    "sys.path.insert(0, '/Users/evanpan/Desktop/openpose/python/')\n",
    "sys.path.insert(0, '/scratch/ondemand27/evanpan/EvansToolBox/Utils/')\n",
    "sys.path.insert(0, '/scratch/ondemand27/evanpan/Gaze_project/')\n",
    "\n",
    "# from training.model import *\n",
    "from Dataset_Util.dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%aimport training.model\n",
    "%aimport Dataset_Util.dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaded Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = \"/scratch/ondemand27/evanpan/data/deep_learning_processed_dataset\"\n",
    "model_save_location = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models\"\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/baseline_config.json\", \"r\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the training test split here:\n",
    "dataset_metadata = \"/scratch/ondemand27/evanpan/data/deep_learning_processed_dataset/video_to_window_metadata.json\"\n",
    "dataset_metadata = json.load(open(dataset_metadata, \"r\"))\n",
    "all_videos = list(dataset_metadata.keys())\n",
    "training_set = []\n",
    "testing_set = []\n",
    "# get the name of the videos (this ensures no contamination because the same shot is split)\n",
    "for i in range(0, len(all_videos)):\n",
    "    if i / len(all_videos) < 0.9:\n",
    "        training_set.append(all_videos[i])\n",
    "    else:\n",
    "        testing_set.append(all_videos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio_only_GazePredictionModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(Audio_only_GazePredictionModel, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * (config[\"input_layer_out\"]) * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)        \n",
    "        # output layers\n",
    "        self.output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3)\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        \n",
    "        text_feature_self = mod_audio_self[:, :, :6]\n",
    "        mod_audio_self = mod_audio_self[:, :, 6:]\n",
    "        text_feature_other = mod_audio_self[:, :, :6]\n",
    "        mod_audio_other = mod_audio_other[:, :, 6:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, x2_windowed], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "        return x\n",
    "    def load_weights(self, pretrained_dict):\n",
    "    #   not_copy = set(['fc.weight', 'fc.bias'])\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBaseline_GazePredictionModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(SentenceBaseline_GazePredictionModel, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * (config[\"input_layer_out\"] + 6) * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)        \n",
    "        # output layers\n",
    "        self.output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3)\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        \n",
    "        text_feature_self = mod_audio_self[:, :, :6]\n",
    "        mod_audio_self = mod_audio_self[:, :, 6:]\n",
    "        text_feature_other = mod_audio_self[:, :, :6]\n",
    "        mod_audio_other = mod_audio_other[:, :, 6:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, text_feature_self, x2_windowed, text_feature_other], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "        return x\n",
    "    def load_weights(self, pretrained_dict):\n",
    "    #   not_copy = set(['fc.weight', 'fc.bias'])\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBaseline_GazePredictionModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(SimpleBaseline_GazePredictionModel, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * config[\"input_layer_out\"] * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)\n",
    "        self.lstm = nn.LSTM(2 * config[\"input_layer_out\"] * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        # output layers\n",
    "        self.output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3, nn.Sigmoid())\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, x2_windowed], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBaselineTransformer_GazePredictionModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(SimpleBaselineTransformer_GazePredictionModel, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * config[\"input_layer_out\"] * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)\n",
    "        self.trasnformer = nn.Transformer(2 * config[\"input_layer_out\"] * (self.frames_ahead + self.frames_behind + 1), batch_first=True,)\n",
    "        \n",
    "        # output layers\n",
    "        self.output_layer_1 = nn.Linear(2 * config[\"input_layer_out\"] * (self.frames_ahead + self.frames_behind + 1), config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3, nn.Sigmoid())\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, x2_windowed], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBaseline_Gaze_and_Direction_PredictionModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(SentenceBaseline_Gaze_and_Direction_PredictionModel, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * (config[\"input_layer_out\"] + 6) * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)        \n",
    "        # aversion output layers\n",
    "        self.output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3)\n",
    "\n",
    "        # directional output layers\n",
    "        self.directional_output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.directional_output_layer_1 = nn.Sequential(self.directional_output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.directional_output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.directional_output_layer_2 = nn.Sequential(self.directional_output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.directional_output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], 4)\n",
    "        self.directional_output_layer_3 = nn.Sequential(self.directional_output_layer_3)\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        \n",
    "        text_feature_self = mod_audio_self[:, :, :6]\n",
    "        mod_audio_self = mod_audio_self[:, :, 6:]\n",
    "        text_feature_other = mod_audio_self[:, :, :6]\n",
    "        mod_audio_other = mod_audio_other[:, :, 6:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, text_feature_self, x2_windowed, text_feature_other], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "\n",
    "        x_dir = self.activation(out)\n",
    "        x_dir = self.directional_output_layer_1(x_dir)\n",
    "        x_dir = self.directional_output_layer_2(x_dir)\n",
    "        x_dir = self.directional_output_layer_3(x_dir)\n",
    "        return x, x_dir\n",
    "    def load_weights(self, pretrained_dict):\n",
    "    #   not_copy = set(['fc.weight', 'fc.bias'])\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBaseline_Gaze_and_Direction_PredictionModel_only_updown(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(SentenceBaseline_Gaze_and_Direction_PredictionModel_only_updown, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * (config[\"input_layer_out\"] + 6) * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)        \n",
    "        # aversion output layers\n",
    "        self.output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3)\n",
    "\n",
    "        # directional output layers\n",
    "        self.directional_output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.directional_output_layer_1 = nn.Sequential(self.directional_output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.directional_output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.directional_output_layer_2 = nn.Sequential(self.directional_output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.directional_output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], 3)\n",
    "        self.directional_output_layer_3 = nn.Sequential(self.directional_output_layer_3)\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        \n",
    "        text_feature_self = mod_audio_self[:, :, :6]\n",
    "        mod_audio_self = mod_audio_self[:, :, 6:]\n",
    "        text_feature_other = mod_audio_self[:, :, :6]\n",
    "        mod_audio_other = mod_audio_other[:, :, 6:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, text_feature_self, x2_windowed, text_feature_other], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "\n",
    "        x_dir = self.activation(out)\n",
    "        x_dir = self.directional_output_layer_1(x_dir)\n",
    "        x_dir = self.directional_output_layer_2(x_dir)\n",
    "        x_dir = self.directional_output_layer_3(x_dir)\n",
    "        return x, x_dir\n",
    "    def load_weights(self, pretrained_dict):\n",
    "    #   not_copy = set(['fc.weight', 'fc.bias'])\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - For baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, config, train_data, valid_data, wandb, model_name):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train() \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    training_loss = []\n",
    "    valid_loss = []\n",
    "    training_f1 = []\n",
    "    valid_f1 = []\n",
    "    aversion_vs_start = []\n",
    "    count = 0\n",
    "    # f1_score = BinaryF1Score(num_classes=2).to(device)\n",
    "    f1_score = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\").to(device)\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "        total_aversion_predicted = 0\n",
    "        total_train_f1 = 0\n",
    "        total_valid_f1 = 0\n",
    "        train_batch_counter = 0\n",
    "        valid_batch_counter = 0\n",
    "        total_prediction_counter = 0\n",
    "        prediction_mean = 0\n",
    "        prediction_std = 0\n",
    "        model.zero_grad()\n",
    "        for _, (X, Y) in enumerate(train_data):\n",
    "            train_batch_counter += 1\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            if \"Transformer\" in config[\"model_type\"]:\n",
    "                all_zero = torch.zeros(Y.shape).to(device)\n",
    "                pred = model(X, all_zero)\n",
    "            else:\n",
    "                pred = model(X)\n",
    "            loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_train_loss += loss.item()\n",
    "            # binary_pred = torch.round(pred)\n",
    "            binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "            prediction_mean = torch.mean(binary_pred.float()).item()\n",
    "            prediction_std = torch.std(binary_pred.float()).item()            \n",
    "            f1_train = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "            total_aversion_predicted += torch.sum(binary_pred).item()\n",
    "            total_prediction_counter += binary_pred.size()[0] * binary_pred.size()[1] \n",
    "            total_train_f1 += f1_train\n",
    "            del X, Y, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        total_train_f1 /= train_batch_counter\n",
    "        total_train_loss /= len(train_data)\n",
    "        total_aversion_predicted /= total_prediction_counter\n",
    "\n",
    "        for _, (X, Y) in enumerate(valid_data):\n",
    "            with torch.no_grad():\n",
    "                valid_batch_counter += 1\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                if \"Transformer\" in config[\"model_type\"]:\n",
    "                    all_zero = torch.zeros(Y.shape).to(device)\n",
    "                    pred = model(X, all_zero)\n",
    "                else:\n",
    "                    pred = model(X)\n",
    "                loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "                # binary_pred = torch.round(pred)\n",
    "                binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "                f1_valid = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "                total_valid_f1 += f1_valid\n",
    "                del X, Y, pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        total_valid_f1 /= valid_batch_counter\n",
    "        total_valid_loss /= len(valid_data)\n",
    "\n",
    "        if config['wandb']:\n",
    "            wandb.log({'training loss': total_train_loss,\n",
    "                        'validation_loss': total_valid_loss,\n",
    "                        'training_f1': total_train_f1,\n",
    "                        'validation_f1': total_valid_f1, \n",
    "                        \"percentage_predicted_aversion\": total_aversion_predicted})\n",
    "        training_loss.append(total_train_loss)\n",
    "        valid_loss.append(total_valid_loss)\n",
    "        training_f1.append(total_train_f1)\n",
    "        valid_f1.append(total_valid_f1)\n",
    "        aversion_vs_start.append(total_aversion_predicted)\n",
    "        if total_valid_f1 == max(valid_f1):\n",
    "            try:\n",
    "                os.mkdir(os.path.join(*[model_save_location, model_name]))\n",
    "            except:\n",
    "                pass\n",
    "            config_save_path = os.path.join(*[model_save_location, model_name, \"config.json\"])\n",
    "            json.dump(config, open(config_save_path, \"w\"))\n",
    "            file_name = f'time={datetime.now()}_epoch={epoch}.pt'\n",
    "            save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        if config['early_stopping']>0:\n",
    "            if epoch > 1:\n",
    "                if total_valid_f1 < np.mean(valid_f1[epoch - 7:epoch - 2]):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "            if count >= config['early_stopping']:\n",
    "                print('\\n\\nStopping early due to decrease in performance on validation set\\n\\n')\n",
    "                break \n",
    "        if count == 0:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1))\n",
    "        else:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}, model have not improved for {} iterations\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1, count))\n",
    "    if config['wandb']:\n",
    "        save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "        wandb.save(save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight and Biases Stuff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03449014124949654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:02<07:47,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5974612089529724, std: 0.03449014124949654\n",
      "training L: 0.7498041014467824\n",
      "validation L:0.7801158889905458\n",
      "0.0387789817215484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  1%|          | 2/200 [00:04<06:59,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.648003122124935, std: 0.0387789817215484\n",
      "training L: 0.750510350090623\n",
      "validation L:0.7802088892277198\n",
      "0.029644162000120988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 3/200 [00:06<06:41,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6950838980002896, std: 0.029644162000120988\n",
      "training L: 0.7506199214130393\n",
      "validation L:0.7802088892277198\n",
      "0.02540355121551124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 4/200 [00:08<06:31,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6974240211738563, std: 0.02540355121551124\n",
      "training L: 0.7506199214130393\n",
      "validation L:0.7802088892277198\n",
      "0.014475230401421608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 5/200 [00:10<06:36,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6587029522108311, std: 0.014475230401421608\n",
      "training L: 0.7506199214130393\n",
      "validation L:0.7802088892277198\n",
      "0.008366695740317093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 6/200 [00:12<06:28,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6412970440137612, std: 0.008366695740317093\n",
      "training L: 0.7506199214130393\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 6/200 [00:12<06:37,  2.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m valid_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(validation_dataset, config[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m model \u001b[39m=\u001b[39m SimpleBaseline_GazePredictionModel(config)\n\u001b[0;32m----> 9\u001b[0m train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)\n\u001b[1;32m     10\u001b[0m run_obj\u001b[39m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[55], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, config, train_data, valid_data, wandb, model_save_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m prediction_std \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     26\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfor\u001b[39;00m _, (X, Y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_data):\n\u001b[1;32m     28\u001b[0m     train_batch_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     29\u001b[0m     X, Y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), Y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/Gaze_project/Dataset_Util/dataloader.py:184\u001b[0m, in \u001b[0;36mAversion_SelfTap111.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    182\u001b[0m output_target \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(aversion_label_path)\n\u001b[1;32m    183\u001b[0m \u001b[39m# see if we need to concat any thing\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m input_audio_on_screen \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(onscreen_audio_feature_path)\n\u001b[1;32m    185\u001b[0m input_audio_off_screen \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(offscreen_audio_feature_path)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_only:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mopen_memmap(file, mode\u001b[39m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(fid, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[1;32m    433\u001b[0m                                  pickle_kwargs\u001b[39m=\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    434\u001b[0m                                  max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/format.py:770\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    768\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    769\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 770\u001b[0m     count \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49mmultiply\u001b[39m.\u001b[39;49mreduce(shape, dtype\u001b[39m=\u001b[39;49mnumpy\u001b[39m.\u001b[39;49mint64)\n\u001b[1;32m    772\u001b[0m \u001b[39m# Now read the actual data.\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[39mif\u001b[39;00m dtype\u001b[39m.\u001b[39mhasobject:\n\u001b[1;32m    774\u001b[0m     \u001b[39m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/baseline_config.json\", \"r\"))\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set[:10])\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set[:2])\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SimpleBaseline_GazePredictionModel(config)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)\n",
    "run_obj.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - For Audio Only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:01<05:35,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5375652030339769, std: 0.038422848050369815\n",
      "training L: 0.7323386721605636\n",
      "validation L:0.7280215162212137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:03<05:08,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5396894879007264, std: 0.03841229046891618\n",
      "training L: 0.7376692695463082\n",
      "validation L:0.7348674337168585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 3/200 [00:04<04:59,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5418414783457995, std: 0.0382727172988321\n",
      "training L: 0.7430556688251317\n",
      "validation L:0.7396762471093492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 4/200 [00:05<04:43,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5443468853526254, std: 0.03826918974963933\n",
      "training L: 0.7486199438292928\n",
      "validation L:0.7408316961362148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 5/200 [00:07<04:44,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5468758275736018, std: 0.03847715916641862\n",
      "training L: 0.751742524009893\n",
      "validation L:0.7457544486877387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 6/200 [00:08<04:34,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5487699903925971, std: 0.03846862209941646\n",
      "training L: 0.7545170146204431\n",
      "validation L:0.7470093760103459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 7/200 [00:10<04:38,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5512063178953902, std: 0.03835452302643252\n",
      "training L: 0.7570841856556142\n",
      "validation L:0.7537631811961684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 8/200 [00:11<04:39,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5532094666883071, std: 0.038496512912125684\n",
      "training L: 0.7617156097791649\n",
      "validation L:0.7549647661755285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 9/200 [00:13<04:39,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5554321769720268, std: 0.03805370844533758\n",
      "training L: 0.7656895903795565\n",
      "validation L:0.7606082318286761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 10/200 [00:14<04:38,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5574337472479528, std: 0.03852652032566119\n",
      "training L: 0.7645938095386826\n",
      "validation L:0.7613699280234122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 11/200 [00:16<04:29,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.559981626833095, std: 0.038347647409568156\n",
      "training L: 0.7698066319794983\n",
      "validation L:0.7640094711917916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 12/200 [00:17<04:30,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5622004870390143, std: 0.038257843264804414\n",
      "training L: 0.7722952821004156\n",
      "validation L:0.7646226415094339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 13/200 [00:18<04:22,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5640483085938395, std: 0.038382897538683555\n",
      "training L: 0.7718415896573535\n",
      "validation L:0.7659007136695161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 14/200 [00:20<04:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5665515094659646, std: 0.03814504706038931\n",
      "training L: 0.7751711855559309\n",
      "validation L:0.7667606516290727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 15/200 [00:21<04:12,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5685410694377985, std: 0.03836502495453606\n",
      "training L: 0.7761418131118908\n",
      "validation L:0.767887763055339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 16/200 [00:22<04:17,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5710678310108791, std: 0.038261168945831855\n",
      "training L: 0.7776878065176539\n",
      "validation L:0.7699121920895174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 17/200 [00:24<04:20,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5729987377715186, std: 0.038263397836928795\n",
      "training L: 0.7794270039622067\n",
      "validation L:0.7713554193498332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 18/200 [00:25<04:22,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.575360365072677, std: 0.03808680008983807\n",
      "training L: 0.7809720723049091\n",
      "validation L:0.7731926776295377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 19/200 [00:27<04:15,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5771138850246609, std: 0.038405858208403906\n",
      "training L: 0.7801478737455783\n",
      "validation L:0.7712205700123915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 20/200 [00:28<04:18,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5790664975047349, std: 0.03818548580533589\n",
      "training L: 0.7813007020576506\n",
      "validation L:0.7748400524165575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 21/200 [00:30<04:19,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5807451319438747, std: 0.038410857081182706\n",
      "training L: 0.7829657011271361\n",
      "validation L:0.775189014041043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 22/200 [00:31<04:11,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.582903799551212, std: 0.03838164564712754\n",
      "training L: 0.782283917014456\n",
      "validation L:0.7741985203452528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 23/200 [00:32<04:05,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5850501231411687, std: 0.03800898681254578\n",
      "training L: 0.7848788953444503\n",
      "validation L:0.7764959237040455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 24/200 [00:34<04:08,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5871901499704449, std: 0.03825333668878838\n",
      "training L: 0.7846295374685186\n",
      "validation L:0.7776071264014744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 25/200 [00:35<04:02,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5890066738191944, std: 0.03832396503706255\n",
      "training L: 0.7850146176798577\n",
      "validation L:0.7762248502534173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 26/200 [00:37<03:57,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5907227826742486, std: 0.03842968105761962\n",
      "training L: 0.7855368728455089\n",
      "validation L:0.7766096232062006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 27/200 [00:38<03:54,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5928269195039755, std: 0.038216674116663824\n",
      "training L: 0.7860214730384049\n",
      "validation L:0.778169014084507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 28/200 [00:39<03:50,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5951770582878034, std: 0.0382402461495092\n",
      "training L: 0.786470234515935\n",
      "validation L:0.7770622508432996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 29/200 [00:40<03:46,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5965046600255317, std: 0.038272955835638604\n",
      "training L: 0.7863828252933311\n",
      "validation L:0.7768582375478927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 30/200 [00:42<03:45,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5982373753965138, std: 0.038192511542916426\n",
      "training L: 0.7866228168971783\n",
      "validation L:0.7777267156862745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 31/200 [00:43<03:52,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5998975201858427, std: 0.03825841699566252\n",
      "training L: 0.7868739309185847\n",
      "validation L:0.7783386874713171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 32/200 [00:45<03:48,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6019111391163703, std: 0.038308990823368284\n",
      "training L: 0.7869959193470956\n",
      "validation L:0.7779735008041664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 33/200 [00:46<03:52,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6036713508981063, std: 0.03826371443377106\n",
      "training L: 0.7872043671920694\n",
      "validation L:0.7781857121003518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 34/200 [00:47<03:48,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6051162892788426, std: 0.03837444523661545\n",
      "training L: 0.7873779564167555\n",
      "validation L:0.7789152024446142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 35/200 [00:49<03:44,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6067495495697391, std: 0.03793514279397345\n",
      "training L: 0.7877943138048349\n",
      "validation L:0.7784742394129338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 36/200 [00:50<03:42,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6085021555963259, std: 0.03827496985733568\n",
      "training L: 0.7881376251273752\n",
      "validation L:0.7788652699189479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 37/200 [00:52<03:48,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6098727395617883, std: 0.038379580377216525\n",
      "training L: 0.7877607288420043\n",
      "validation L:0.7788138184041578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 38/200 [00:53<03:43,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6117281921448884, std: 0.03800839217986361\n",
      "training L: 0.7881538738090959\n",
      "validation L:0.7788219115287646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 39/200 [00:54<03:39,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6132555046716315, std: 0.03813427039511276\n",
      "training L: 0.7882763475161373\n",
      "validation L:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 40/200 [00:56<03:36,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6146646101171072, std: 0.03815154347277368\n",
      "training L: 0.7881655387195304\n",
      "validation L:0.7792108677402122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 41/200 [00:57<03:42,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6157974883200633, std: 0.03834291259328531\n",
      "training L: 0.7884848575577479\n",
      "validation L:0.7794488128864799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 42/200 [00:58<03:37,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6175480401993695, std: 0.0380655845445438\n",
      "training L: 0.78839590443686\n",
      "validation L:0.7795678399633504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 43/200 [01:00<03:33,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6189577184437718, std: 0.03818574687252487\n",
      "training L: 0.7886725239855712\n",
      "validation L:0.7797955135052648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 44/200 [01:01<03:38,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6200836475331568, std: 0.03797256523459504\n",
      "training L: 0.7887201735357917\n",
      "validation L:0.7800747577999847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 45/200 [01:03<03:33,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6214130102648945, std: 0.03796793105375696\n",
      "training L: 0.7886042614316495\n",
      "validation L:0.7794566544566545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 46/200 [01:04<03:30,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.62247247622866, std: 0.03803493969337708\n",
      "training L: 0.7886333368249361\n",
      "validation L:0.7800411993591211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 47/200 [01:05<03:27,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6238084493179571, std: 0.037875842012672904\n",
      "training L: 0.7886422117168332\n",
      "validation L:0.7798809342085178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 48/200 [01:07<03:26,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6248945816924095, std: 0.03827418552441704\n",
      "training L: 0.7889847875188847\n",
      "validation L:0.7793971766501335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 49/200 [01:08<03:23,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6261484635792381, std: 0.03790382461395846\n",
      "training L: 0.7887989708456119\n",
      "validation L:0.7801007172287502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 50/200 [01:09<03:21,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6274820380729639, std: 0.03795289443161719\n",
      "training L: 0.7888533221647172\n",
      "validation L:0.7798368031724243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 51/200 [01:11<03:19,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6287918735569356, std: 0.038010754836282835\n",
      "training L: 0.7889241263762565\n",
      "validation L:0.779769623922496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 52/200 [01:12<03:18,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6298811093429626, std: 0.037993665419687114\n",
      "training L: 0.7888297235850186\n",
      "validation L:0.7800152555301296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 53/200 [01:13<03:16,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6308418167201261, std: 0.03790918813462353\n",
      "training L: 0.7888596386263013\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 54/200 [01:15<03:15,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6317639670979591, std: 0.03772842298266372\n",
      "training L: 0.7888322292174486\n",
      "validation L:0.7796170569837516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 55/200 [01:16<03:14,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6328735757656325, std: 0.03798885606805677\n",
      "training L: 0.7888549892318737\n",
      "validation L:0.7797437461866992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 56/200 [01:17<03:12,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6336673558270467, std: 0.03800663855394427\n",
      "training L: 0.7889274391338159\n",
      "validation L:0.779769623922496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 57/200 [01:19<03:10,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6345197747454334, std: 0.037944321430231465\n",
      "training L: 0.7889455502549761\n",
      "validation L:0.7800823547353973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 58/200 [01:20<03:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6357832789447202, std: 0.03807899012996211\n",
      "training L: 0.7889392375910389\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 59/200 [01:21<03:13,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6364019636121019, std: 0.03815005899083669\n",
      "training L: 0.7889935696126813\n",
      "validation L:0.7804133933338419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 60/200 [01:23<03:17,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6374840290487794, std: 0.03800236525374677\n",
      "training L: 0.7889219542103453\n",
      "validation L:0.7803538743136058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 61/200 [01:24<03:12,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.637778766308804, std: 0.0378343596086585\n",
      "training L: 0.7888865627897239\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 62/200 [01:26<03:08,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6388077489404568, std: 0.03789524239582371\n",
      "training L: 0.7890487155716379\n",
      "validation L:0.7797437461866992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 63/200 [01:27<03:06,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6392297369299728, std: 0.037880109618109344\n",
      "training L: 0.7889707971350389\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 64/200 [01:28<03:04,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6401475170824421, std: 0.03775352116254244\n",
      "training L: 0.7890558421170666\n",
      "validation L:0.7798032186713447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 65/200 [01:30<03:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6408517402368207, std: 0.03767392863007117\n",
      "training L: 0.7889589999700948\n",
      "validation L:0.7802943643712347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 66/200 [01:31<03:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6415438765502811, std: 0.0378771443774228\n",
      "training L: 0.7889526885579281\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 67/200 [01:32<02:58,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6418545848710732, std: 0.03785251692680205\n",
      "training L: 0.7888078004426632\n",
      "validation L:0.7796506750057204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 68/200 [01:34<02:56,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6430145007766597, std: 0.03793944414056601\n",
      "training L: 0.7890007027827205\n",
      "validation L:0.779955762336969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 69/200 [01:35<02:54,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6431724290954164, std: 0.03794557136403669\n",
      "training L: 0.7889707971350389\n",
      "validation L:0.7800488102501525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 70/200 [01:36<02:53,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6438113379585376, std: 0.037867618304883076\n",
      "training L: 0.7890377369774535\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 71/200 [01:38<02:52,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6443665282661462, std: 0.03755890204180918\n",
      "training L: 0.7890558421170666\n",
      "validation L:0.7800488102501525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 72/200 [01:39<02:51,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6449561462094501, std: 0.037872248734854085\n",
      "training L: 0.789103033746505\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 73/200 [01:40<02:49,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6453280841816352, std: 0.03770579676780109\n",
      "training L: 0.789079437226201\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 74/200 [01:42<02:47,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6457358736092284, std: 0.03751679354920816\n",
      "training L: 0.7890188103711235\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 75/200 [01:43<02:46,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.646295564651153, std: 0.037746101125036374\n",
      "training L: 0.789050352828609\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 76/200 [01:44<02:45,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6462463095208183, std: 0.03750683690394778\n",
      "training L: 0.7890676394952455\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 77/200 [01:46<02:43,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6468440748456522, std: 0.0377434471441708\n",
      "training L: 0.789055023923445\n",
      "validation L:0.7800823547353973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 78/200 [01:47<02:40,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6470649900498601, std: 0.03791573178697559\n",
      "training L: 0.7891101542877647\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 79/200 [01:48<02:40,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.647273252066047, std: 0.03790480812766759\n",
      "training L: 0.7889290947695804\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 80/200 [01:50<02:38,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6475985561213322, std: 0.03768843110730487\n",
      "training L: 0.788983418310133\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 81/200 [01:51<02:37,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6478119237514677, std: 0.03795452502134771\n",
      "training L: 0.789133749962623\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 82/200 [01:52<02:36,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6477440890117648, std: 0.03784521988385853\n",
      "training L: 0.788947203157942\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 83/200 [01:54<02:35,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6483128562010473, std: 0.037709612182815766\n",
      "training L: 0.7890078344596615\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 84/200 [01:55<02:35,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6486097237679442, std: 0.03754975698081344\n",
      "training L: 0.7890920507721979\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 85/200 [01:56<02:33,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6489740447107436, std: 0.03753976711385344\n",
      "training L: 0.7890676394952455\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 86/200 [01:58<02:32,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6488574469954199, std: 0.03766541645707203\n",
      "training L: 0.7890613318979696\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 87/200 [01:59<02:30,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6489096932871466, std: 0.037728392982625425\n",
      "training L: 0.789103033746505\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 88/200 [02:00<02:29,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.649378472614487, std: 0.037616527532277295\n",
      "training L: 0.7890621495955866\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 89/200 [02:02<02:28,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6495942640500949, std: 0.03741697010708341\n",
      "training L: 0.7890983569794735\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 90/200 [02:03<02:26,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6495154665553903, std: 0.03751850003918505\n",
      "training L: 0.789085744187785\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 91/200 [02:04<02:25,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498210439550074, std: 0.03762015187425466\n",
      "training L: 0.789049534261322\n",
      "validation L:0.7800823547353973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 92/200 [02:06<02:23,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500737481281029, std: 0.03747188069033617\n",
      "training L: 0.7890204520990313\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 93/200 [02:07<02:22,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6497623566221192, std: 0.037333214636729145\n",
      "training L: 0.7889960379756298\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 94/200 [02:08<02:21,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6497072156138498, std: 0.03765919770957377\n",
      "training L: 0.7891211387219281\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 95/200 [02:10<02:20,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498812003882101, std: 0.03746079489707398\n",
      "training L: 0.7889661359049114\n",
      "validation L:0.7799893235720278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 96/200 [02:11<02:19,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499510457127998, std: 0.03757139404685042\n",
      "training L: 0.7890259400463482\n",
      "validation L:0.7801418439716312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 97/200 [02:12<02:17,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6504438735835141, std: 0.03743523975332393\n",
      "training L: 0.789085744187785\n",
      "validation L:0.7804468847708381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 98/200 [02:14<02:16,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.649769088231089, std: 0.037543300462062286\n",
      "training L: 0.7890440450916485\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 99/200 [02:15<02:14,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500536357439018, std: 0.03735566876637967\n",
      "training L: 0.789133749962623\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 100/200 [02:16<02:13,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501871478072954, std: 0.03762670929697387\n",
      "training L: 0.7890314284859903\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 101/200 [02:18<02:12,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501271829508782, std: 0.03714695929733477\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7801418439716312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 102/200 [02:19<02:10,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500918652320735, std: 0.03742554390392043\n",
      "training L: 0.7890322484189752\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 103/200 [02:20<02:09,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501412497418192, std: 0.03739204724230821\n",
      "training L: 0.789122765054117\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 104/200 [02:22<02:08,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6497717523605562, std: 0.03760921137923994\n",
      "training L: 0.7890920507721979\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 105/200 [02:23<02:07,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501580367727385, std: 0.037468959594168946\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 106/200 [02:24<02:05,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498953478366677, std: 0.037546487407533365\n",
      "training L: 0.7890314284859903\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 107/200 [02:26<02:04,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498872552868251, std: 0.037587378927069\n",
      "training L: 0.7890314284859903\n",
      "validation L:0.7801418439716312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 108/200 [02:27<02:03,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500029294532585, std: 0.037289182435219455\n",
      "training L: 0.7891400550173424\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 109/200 [02:28<02:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6502461237072283, std: 0.03755941936064975\n",
      "training L: 0.7891282572620312\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 110/200 [02:30<02:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501953252623862, std: 0.037394467274640464\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 111/200 [02:31<01:59,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498664065940516, std: 0.03725298950261908\n",
      "training L: 0.7891463596950217\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 112/200 [02:32<01:57,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499946135774257, std: 0.037446181779797444\n",
      "training L: 0.7890660047253043\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 113/200 [02:34<01:56,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500599020048369, std: 0.037402034475766774\n",
      "training L: 0.7890204520990313\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 114/200 [02:35<01:54,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6504418660385398, std: 0.03726827419105667\n",
      "training L: 0.7890739467153066\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 115/200 [02:36<01:53,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6503700653919925, std: 0.03724308902750212\n",
      "training L: 0.7891880578271465\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 116/200 [02:38<01:52,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501551283243268, std: 0.03723439041383075\n",
      "training L: 0.7890684566969158\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 117/200 [02:39<01:51,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6497153058598419, std: 0.037271555619692566\n",
      "training L: 0.7889960379756298\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 118/200 [02:40<01:49,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501052327897997, std: 0.03726821926702292\n",
      "training L: 0.789134562198203\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 119/200 [02:42<01:48,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6503252694044814, std: 0.03731639904103009\n",
      "training L: 0.7890613318979696\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 120/200 [02:43<01:46,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500738723572284, std: 0.037279360225059475\n",
      "training L: 0.7890621495955866\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 121/200 [02:44<01:45,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498417167752963, std: 0.037079801938013406\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 122/200 [02:46<01:44,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.650265070274585, std: 0.037034828333994324\n",
      "training L: 0.7890802535581868\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 123/200 [02:47<01:42,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6502467146360357, std: 0.037436068434452505\n",
      "training L: 0.7891164598594708\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 124/200 [02:48<01:41,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6496430102933335, std: 0.03721716628517319\n",
      "training L: 0.7890676394952455\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 125/200 [02:50<01:40,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499345323683191, std: 0.0374692464488565\n",
      "training L: 0.789050352828609\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 126/200 [02:51<01:38,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6495119998868814, std: 0.037422044042390024\n",
      "training L: 0.7891573470486215\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 127/200 [02:52<01:37,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6494766795521983, std: 0.037273700075502667\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 128/200 [02:54<01:39,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498283857833278, std: 0.037200995213504846\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 129/200 [02:55<01:37,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6494224017071586, std: 0.037259201298920665\n",
      "training L: 0.789139243155959\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 130/200 [02:57<01:35,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6491423779067859, std: 0.03721442709299281\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 131/200 [02:58<01:33,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499734358614128, std: 0.036943805382440485\n",
      "training L: 0.7891526639956945\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 132/200 [02:59<01:35,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500218696465765, std: 0.037401049570965614\n",
      "training L: 0.789133749962623\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 133/200 [03:01<01:32,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6494513582998024, std: 0.037263276859249905\n",
      "training L: 0.7890739467153066\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 134/200 [03:02<01:30,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6494726131571438, std: 0.03723299795554167\n",
      "training L: 0.789002347242364\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 135/200 [03:03<01:28,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6495496750291943, std: 0.03708250920514626\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 136/200 [03:05<01:26,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6494956205671558, std: 0.0371957410045398\n",
      "training L: 0.789085744187785\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 137/200 [03:06<01:24,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.650535865233708, std: 0.037154148136127794\n",
      "training L: 0.7890802535581868\n",
      "validation L:0.7799893235720278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 138/200 [03:07<01:22,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6495115698645602, std: 0.037151209592330675\n",
      "training L: 0.7890322484189752\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 139/200 [03:09<01:21,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6493358151127034, std: 0.03723563590252677\n",
      "training L: 0.7890377369774535\n",
      "validation L:0.7802088892277198\n",
      "mean: 0.6496482683585386, std: 0.03737806580611282\n",
      "training L: 0.7889417182500523\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 141/200 [03:12<01:21,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6503415530453325, std: 0.03719735407667015\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 142/200 [03:13<01:22,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6505178426903855, std: 0.037125989000501965\n",
      "training L: 0.789133749962623\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 143/200 [03:14<01:19,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6491846292292962, std: 0.03756563606990346\n",
      "training L: 0.7890377369774535\n",
      "validation L:0.7800823547353973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 144/200 [03:16<01:17,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6489230551477677, std: 0.03713717678777013\n",
      "training L: 0.7891463596950217\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 145/200 [03:17<01:14,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500091637951761, std: 0.03723965317236613\n",
      "training L: 0.7890920507721979\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 146/200 [03:18<01:13,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6505028659865627, std: 0.03726119110477237\n",
      "training L: 0.7891282572620312\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 147/200 [03:20<01:11,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6508349413277549, std: 0.03710587107158111\n",
      "training L: 0.7890322484189752\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 148/200 [03:21<01:09,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6497215027685552, std: 0.03727335028403669\n",
      "training L: 0.789050352828609\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 149/200 [03:23<01:11,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6484142505097515, std: 0.03713637206976326\n",
      "training L: 0.7890975420130375\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 150/200 [03:24<01:09,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6485851457429599, std: 0.03712819891392657\n",
      "training L: 0.7891101542877647\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 151/200 [03:25<01:07,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499649553057815, std: 0.037177752815117135\n",
      "training L: 0.7891518531254205\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 152/200 [03:27<01:05,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6509560817307467, std: 0.03726366366839285\n",
      "training L: 0.7891164598594708\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 153/200 [03:28<01:03,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6511630880661888, std: 0.03718854248276555\n",
      "training L: 0.7890739467153066\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 154/200 [03:29<01:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499539006502881, std: 0.03738341670309168\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 155/200 [03:31<01:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6503003855274889, std: 0.03720285269586973\n",
      "training L: 0.789050352828609\n",
      "validation L:0.7801418439716312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 156/200 [03:32<00:59,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498567126833039, std: 0.037093566403896376\n",
      "training L: 0.789079437226201\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 157/200 [03:34<01:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6503346970339198, std: 0.037376419242272814\n",
      "training L: 0.7890802535581868\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 158/200 [03:35<00:57,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.651107239786559, std: 0.037211889725529586\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 159/200 [03:36<00:58,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.650475750492941, std: 0.03715005283271281\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 160/200 [03:38<00:55,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498578179262586, std: 0.037078632658453556\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 161/200 [03:39<00:53,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6516970899085567, std: 0.03730244981036209\n",
      "training L: 0.7890259400463482\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 162/200 [03:40<00:51,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6520823474398048, std: 0.037284116516145244\n",
      "training L: 0.7890747634210881\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 163/200 [03:42<00:50,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6507470263261494, std: 0.03741363351940397\n",
      "training L: 0.789056660188369\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 164/200 [03:43<00:48,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6504946189314396, std: 0.03720300507104151\n",
      "training L: 0.7890621495955866\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 165/200 [03:45<00:48,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6502906632818455, std: 0.03728390134381776\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 166/200 [03:46<00:46,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499858704657006, std: 0.03723080418488043\n",
      "training L: 0.7890440450916485\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 167/200 [03:47<00:45,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6504895816566616, std: 0.03741216036682273\n",
      "training L: 0.789133749962623\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 168/200 [03:49<00:43,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6512761960166494, std: 0.03729500754168066\n",
      "training L: 0.789134562198203\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 169/200 [03:50<00:41,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6518689578150552, std: 0.037488372087739356\n",
      "training L: 0.7890802535581868\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 170/200 [03:51<00:40,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6519314242298094, std: 0.037489673191777144\n",
      "training L: 0.7891101542877647\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 171/200 [03:53<00:39,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.650693017283545, std: 0.03731097606676058\n",
      "training L: 0.7890920507721979\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 172/200 [03:54<00:37,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6510142397210906, std: 0.03743140358968756\n",
      "training L: 0.789050352828609\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 173/200 [03:55<00:36,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6514361465637869, std: 0.03725046086016204\n",
      "training L: 0.7890684566969158\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 174/200 [03:57<00:34,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6513559322656731, std: 0.03723085487433993\n",
      "training L: 0.7891581575445888\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 175/200 [03:58<00:33,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6509152845170848, std: 0.03743742924000529\n",
      "training L: 0.7891101542877647\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 176/200 [03:59<00:32,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.651684699399912, std: 0.03757594819922277\n",
      "training L: 0.7890802535581868\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 177/200 [04:01<00:30,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6507555980330351, std: 0.03752838806628672\n",
      "training L: 0.7891400550173424\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 178/200 [04:02<00:29,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6511253328075242, std: 0.03743421966639613\n",
      "training L: 0.7891164598594708\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 179/200 [04:03<00:28,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6521646833636132, std: 0.037542516726807904\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 180/200 [04:05<00:27,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6517703879675681, std: 0.03756854569544114\n",
      "training L: 0.7891699557469202\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 181/200 [04:06<00:26,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6502751314481255, std: 0.037648384066966877\n",
      "training L: 0.789110968262748\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 182/200 [04:08<00:24,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6514952344314373, std: 0.03747867401004681\n",
      "training L: 0.7891164598594708\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 183/200 [04:09<00:23,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6526221874426001, std: 0.037801697534765845\n",
      "training L: 0.7891164598594708\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 184/200 [04:10<00:21,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6525767401123828, std: 0.0376094175963354\n",
      "training L: 0.7890920507721979\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 185/200 [04:12<00:21,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6525164012595259, std: 0.037638741217422444\n",
      "training L: 0.789134562198203\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 186/200 [04:13<00:19,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6523958507033527, std: 0.03754099197440361\n",
      "training L: 0.7890684566969158\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 187/200 [04:14<00:17,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6523987897332388, std: 0.037702384989835296\n",
      "training L: 0.7891282572620312\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 188/200 [04:16<00:16,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6524044044291957, std: 0.03780756563922846\n",
      "training L: 0.7891282572620312\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 189/200 [04:17<00:15,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.651952320904239, std: 0.037778805295195735\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 190/200 [04:18<00:13,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6520589789247596, std: 0.03766379883222292\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 191/200 [04:20<00:12,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6527499982519376, std: 0.03785650292542167\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 192/200 [04:21<00:10,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6517534958450468, std: 0.037894126614088454\n",
      "training L: 0.7891518531254205\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 193/200 [04:23<00:09,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6502472349816975, std: 0.03800010367484725\n",
      "training L: 0.7891518531254205\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 194/200 [04:24<00:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6513283861060108, std: 0.03792280645361822\n",
      "training L: 0.7891400550173424\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 195/200 [04:25<00:06,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6521683720373683, std: 0.037860077419014454\n",
      "training L: 0.789122765054117\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 196/200 [04:27<00:05,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.652412709201666, std: 0.03828000420982498\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 197/200 [04:28<00:04,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6528608022047981, std: 0.038130725836294684\n",
      "training L: 0.7890975420130375\n",
      "validation L:0.7799893235720278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 198/200 [04:30<00:02,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6532184313266275, std: 0.03817823601465069\n",
      "training L: 0.7891400550173424\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 199/200 [04:31<00:01,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.652308709116172, std: 0.038421576275752184\n",
      "training L: 0.7891518531254205\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [04:32<00:00,  1.36s/it]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6524606512561358, std: 0.038428353510256905\n",
      "training L: 0.7890322484189752\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'time=2023-04-04 20:54:46.715070_epoch=196.pt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/audio_only_config.json\", \"r\"))\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set[:12], audio_only=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set[:2], audio_only=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SimpleBaseline_GazePredictionModel(config)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - For sentence level feature Only model (This is actually for word level only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m         X, Y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), Y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m         pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m---> 17\u001b[0m A[\u001b[39m2\u001b[39m]\n\u001b[1;32m     18\u001b[0m train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/word_config.json\", \"r\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, word_timing=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, word_timing=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "model.to(device)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## further train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m SentenceBaseline_GazePredictionModel(config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/word_config.json\", \"r\"))\n",
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "config[\"learning_rate\"] = 0.00001\n",
    "config[\"load_model\"] = True\n",
    "if config[\"wandb\"]:\n",
    "    wandb.login()\n",
    "    if config[\"load_model\"]:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, save_code=True,\n",
    "            resume='allow', id='8w9fyxan')\n",
    "        # checkpoint_name = \"gaze_prediction_team/gaze_prediction/8w9fyxan\"\n",
    "        checkpoint_path = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/time=2023-04-05 02:37:34.205141_epoch=200.pt\"\n",
    "        wandb.restore(checkpoint_path)\n",
    "        pretrained_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_weights(pretrained_dict)\n",
    "        model.to(config['device'])\n",
    "    else:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "else:\n",
    "    run_obj = None\n",
    "\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, sentence_timing=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, sentence_timing=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - For sentence level feature Only model (It's real this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/artemis.dgpsrv/ondemand27/evanpan/Gaze_project/training/wandb/run-20230407_195055-2btkatcn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/2btkatcn' target=\"_blank\">pretty-donkey-78</a></strong> to <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/2btkatcn' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/2btkatcn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mean: 0.17789314687252045, std: 0.3824285864830017\n",
      "training L: 0.38361384790041525\n",
      "validation L:0.5202852326801098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, mean: 0.5589313507080078, std: 0.49652254581451416\n",
      "training L: 0.5135592779060695\n",
      "validation L:0.5422346910798534\n",
      "Epoch 3, mean: 0.8715725541114807, std: 0.3345702886581421\n",
      "training L: 0.5083247797239047\n",
      "validation L:0.5124457632512047, model have not improved for 1 iterations\n",
      "Epoch 4, mean: 0.9777710437774658, std: 0.14742977917194366\n",
      "training L: 0.4620488325757937\n",
      "validation L:0.5003760834120156, model have not improved for 2 iterations\n",
      "Epoch 5, mean: 0.9966717958450317, std: 0.05759573355317116\n",
      "training L: 0.45366407573439577\n",
      "validation L:0.49920991627236866\n",
      "Epoch 6, mean: 0.9993893504142761, std: 0.024704914540052414\n",
      "training L: 0.45135263240753554\n",
      "validation L:0.49904111077228025\n",
      "Epoch 7, mean: 0.999908447265625, std: 0.009570656344294548\n",
      "training L: 0.4510088849744354\n",
      "validation L:0.4989816254955779, model have not improved for 1 iterations\n",
      "Epoch 8, mean: 0.9999695420265198, std: 0.005525789689272642\n",
      "training L: 0.4510375310310284\n",
      "validation L:0.49904111077228025, model have not improved for 2 iterations\n",
      "Epoch 9, mean: 0.9999695420265198, std: 0.005525789689272642\n",
      "training L: 0.4510375310310284\n",
      "validation L:0.49904111077228025, model have not improved for 3 iterations\n",
      "Epoch 10, mean: 0.99993896484375, std: 0.007814527489244938\n",
      "training L: 0.45102320827550085\n",
      "validation L:0.49904111077228025, model have not improved for 4 iterations\n",
      "Epoch 11, mean: 0.99993896484375, std: 0.007814527489244938\n",
      "training L: 0.45093922834713546\n",
      "validation L:0.49904111077228025, model have not improved for 5 iterations\n",
      "Epoch 12, mean: 0.9999695420265198, std: 0.005525789689272642\n",
      "training L: 0.4510375310310284\n",
      "validation L:0.49904111077228025\n",
      "Epoch 13, mean: 0.999908447265625, std: 0.009570656344294548\n",
      "training L: 0.4510088849744354\n",
      "validation L:0.4998447436699742\n",
      "Epoch 14, mean: 0.9996947050094604, std: 0.0174716804176569\n",
      "training L: 0.4512443909607265\n",
      "validation L:0.4989221311475409, model have not improved for 1 iterations\n",
      "Epoch 15, mean: 0.9996641874313354, std: 0.018324172124266624\n",
      "training L: 0.45139793012303303\n",
      "validation L:0.49983279500184485\n",
      "Epoch 16, mean: 0.998870313167572, std: 0.03359358757734299\n",
      "training L: 0.45186288379531225\n",
      "validation L:0.5008425060411832\n",
      "Epoch 17, mean: 0.9972519874572754, std: 0.052350964397192\n",
      "training L: 0.452519805536843\n",
      "validation L:0.5012535185673744\n",
      "Epoch 18, mean: 0.9957863092422485, std: 0.06477741152048111\n",
      "training L: 0.4544919343355489\n",
      "validation L:0.5013703832400874\n",
      "Epoch 19, mean: 0.9925191402435303, std: 0.08616947382688522\n",
      "training L: 0.45632163234957185\n",
      "validation L:0.5010809447725445\n",
      "Epoch 20, mean: 0.9914199113845825, std: 0.09223227947950363\n",
      "training L: 0.45653083796501637\n",
      "validation L:0.5028141452381555\n",
      "Epoch 21, mean: 0.9878779053688049, std: 0.1094328835606575\n",
      "training L: 0.4594892573498276\n",
      "validation L:0.5048017116359494\n",
      "Epoch 22, mean: 0.989221453666687, std: 0.10326070338487625\n",
      "training L: 0.45710465582560234\n",
      "validation L:0.5064171298766909\n",
      "Epoch 23, mean: 0.9887023568153381, std: 0.10569017380475998\n",
      "training L: 0.4579207796515155\n",
      "validation L:0.504652077735601\n",
      "Epoch 24, mean: 0.9910229444503784, std: 0.09432275593280792\n",
      "training L: 0.4564193465842272\n",
      "validation L:0.5030513518203085, model have not improved for 1 iterations\n",
      "Epoch 25, mean: 0.9930382370948792, std: 0.08314792066812515\n",
      "training L: 0.45624247204732504\n",
      "validation L:0.5019633548883111, model have not improved for 2 iterations\n",
      "Epoch 26, mean: 0.9952672123908997, std: 0.06863358616828918\n",
      "training L: 0.45465923941069897\n",
      "validation L:0.5008425060411832, model have not improved for 3 iterations\n",
      "Epoch 27, mean: 0.9969771504402161, std: 0.054898589849472046\n",
      "training L: 0.453308506609264\n",
      "validation L:0.5015340993823107, model have not improved for 4 iterations\n",
      "Epoch 28, mean: 0.9982596039772034, std: 0.04168311133980751\n",
      "training L: 0.45207707721173696\n",
      "validation L:0.49937816606318464, model have not improved for 5 iterations\n",
      "Epoch 29, mean: 0.9988092184066772, std: 0.03448851779103279\n",
      "training L: 0.4518340638223524\n",
      "validation L:0.49966544120946266, model have not improved for 6 iterations\n",
      "Epoch 30, mean: 0.998961865901947, std: 0.03220437839627266\n",
      "training L: 0.4516545751435136\n",
      "validation L:0.4992587620971374, model have not improved for 7 iterations\n",
      "Epoch 31, mean: 0.9993283152580261, std: 0.025909939780831337\n",
      "training L: 0.4514078019772601\n",
      "validation L:0.49972521759451, model have not improved for 8 iterations\n",
      "Epoch 32, mean: 0.9995725750923157, std: 0.020671507343649864\n",
      "training L: 0.4514387969966117\n",
      "validation L:0.4999044933649173, model have not improved for 9 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stopping early due to decrease in performance on validation set\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa38fc150c6040bab0a9445ac42207c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='2.544 MB of 2.544 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>percentage_predicted_aversion</td><td></td></tr><tr><td>training loss</td><td></td></tr><tr><td>training_f1</td><td></td></tr><tr><td>validation_f1</td><td></td></tr><tr><td>validation_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>percentage_predicted_aversion</td><td>0.99969</td></tr><tr><td>training loss</td><td>0.67176</td></tr><tr><td>training_f1</td><td>0.45141</td></tr><tr><td>validation_f1</td><td>0.49892</td></tr><tr><td>validation_loss</td><td>0.65346</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretty-donkey-78</strong> at: <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/2btkatcn' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/2btkatcn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230407_195055-2btkatcn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\", \"r\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "model.to(device)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, \"sentence_and_words\")\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for Audio only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevanpan\u001b[0m (\u001b[33mgaze_prediction_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /tmp/ipykernel_2948178/1684038919.py 1 <module>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_obj \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39;49minit(project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgaze_prediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49mconfig, settings\u001b[39m=\u001b[39;49mwandb\u001b[39m.\u001b[39;49mSettings(start_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfork\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      2\u001b[0m config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1166\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[39massert\u001b[39;00m logger\n\u001b[1;32m   1165\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39minterrupted\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[0;32m-> 1166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1167\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1168\u001b[0m     error_seen \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1144\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1142\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1144\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m   1145\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1146\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:801\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    799\u001b[0m run_start_handle \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39minterface\u001b[39m.\u001b[39mdeliver_run_start(run_obj)\n\u001b[1;32m    800\u001b[0m \u001b[39m# TODO: add progress to let user know we are doing something\u001b[39;00m\n\u001b[0;32m--> 801\u001b[0m run_start_result \u001b[39m=\u001b[39m run_start_handle\u001b[39m.\u001b[39;49mwait(timeout\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n\u001b[1;32m    802\u001b[0m \u001b[39mif\u001b[39;00m run_start_result \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    803\u001b[0m     run_start_handle\u001b[39m.\u001b[39mabandon()\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interface\u001b[39m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m MailboxError(\u001b[39m\"\u001b[39m\u001b[39mtransport failed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slot\u001b[39m.\u001b[39;49m_get_and_clear(timeout\u001b[39m=\u001b[39;49mwait_timeout)\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m on_progress \u001b[39mand\u001b[39;00m progress_handle \u001b[39mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_and_clear\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Optional[pb\u001b[39m.\u001b[39mResult], \u001b[39mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout\u001b[39m=\u001b[39;49mtimeout):\n\u001b[1;32m    131\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wait\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\", \"r\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = Audio_only_GazePredictionModel(config)\n",
    "model.to(device)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, \"auduo_only\")\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for sentence + word + audio. but also uses velocity as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity(out):\n",
    "    vel = torch.diff(out, dim=1, prepend=out[:, 0:1, :])\n",
    "    return vel\n",
    "def train_model_with_vel(model, config, train_data, valid_data, wandb, model_name, start = 1):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train() \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_fn_vel = nn.MSELoss()\n",
    "    training_loss = []\n",
    "    valid_loss = []\n",
    "    training_f1 = []\n",
    "    valid_f1 = []\n",
    "    aversion_vs_start = []\n",
    "    count = 0\n",
    "    # f1_score = BinaryF1Score(num_classes=2).to(device)\n",
    "    f1_score = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\").to(device)\n",
    "    for epoch in range(start, config['epochs'] + 1):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "        total_aversion_predicted = 0\n",
    "        total_train_f1 = 0\n",
    "        total_valid_f1 = 0\n",
    "        train_batch_counter = 0\n",
    "        valid_batch_counter = 0\n",
    "        total_prediction_counter = 0\n",
    "        prediction_mean = 0\n",
    "        prediction_std = 0\n",
    "        model.zero_grad()\n",
    "        for _, (X, [Y, Y_vel]) in enumerate(train_data):\n",
    "            train_batch_counter += 1\n",
    "            X, Y, Y_vel = X.to(device), Y.to(device), Y_vel.to(device)            \n",
    "            optimiser.zero_grad()\n",
    "            if \"Transformer\" in config[\"model_type\"]:\n",
    "                all_zero = torch.zeros(Y.shape).to(device)\n",
    "                pred = model(X, all_zero)\n",
    "            else:\n",
    "                pred = model(X)\n",
    "            loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "            # get the softmax\n",
    "            pred_vel = get_velocity(torch.softmax(pred, axis=2))\n",
    "            # add the velocity along dimension 1 and 2 respectively, the velocity are opposite \n",
    "            loss = loss + 0.3 * loss_fn_vel(pred_vel[:, :, 1], Y_vel)\n",
    "            loss = loss + 0.3 * loss_fn_vel(pred_vel[:, :, 0], -Y_vel)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_train_loss += loss.item()\n",
    "            # binary_pred = torch.round(pred)\n",
    "            binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "            prediction_mean = torch.mean(binary_pred.float()).item()\n",
    "            prediction_std = torch.std(binary_pred.float()).item()            \n",
    "            f1_train = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "            total_aversion_predicted += torch.sum(binary_pred).item()\n",
    "            total_prediction_counter += binary_pred.size()[0] * binary_pred.size()[1] \n",
    "            total_train_f1 += f1_train\n",
    "            del X, Y, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        total_train_f1 /= train_batch_counter\n",
    "        total_train_loss /= len(train_data)\n",
    "        total_aversion_predicted /= total_prediction_counter\n",
    "\n",
    "        for _, (X, [Y, Y_vel]) in enumerate(valid_data):\n",
    "            with torch.no_grad():\n",
    "                valid_batch_counter += 1\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                if \"Transformer\" in config[\"model_type\"]:\n",
    "                    all_zero = torch.zeros(Y.shape).to(device)\n",
    "                    pred = model(X, all_zero)\n",
    "                else:\n",
    "                    pred = model(X)\n",
    "                loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "                # binary_pred = torch.round(pred)\n",
    "                binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "                f1_valid = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "                total_valid_f1 += f1_valid\n",
    "                del X, Y, pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        total_valid_f1 /= valid_batch_counter\n",
    "        total_valid_loss /= len(valid_data)\n",
    "\n",
    "        if config['wandb']:\n",
    "            wandb.log({'training loss': total_train_loss,\n",
    "                        'validation_loss': total_valid_loss,\n",
    "                        'training_f1': total_train_f1,\n",
    "                        'validation_f1': total_valid_f1, \n",
    "                        \"percentage_predicted_aversion\": total_aversion_predicted})\n",
    "        training_loss.append(total_train_loss)\n",
    "        valid_loss.append(total_valid_loss)\n",
    "        training_f1.append(total_train_f1)\n",
    "        valid_f1.append(total_valid_f1)\n",
    "        aversion_vs_start.append(total_aversion_predicted)\n",
    "        if total_valid_f1 == max(valid_f1):\n",
    "            try:\n",
    "                os.mkdir(os.path.join(*[model_save_location, model_name]))\n",
    "            except:\n",
    "                pass\n",
    "            config_save_path = os.path.join(*[model_save_location, model_name, \"config.json\"])\n",
    "            json.dump(config, open(config_save_path, \"w\"))\n",
    "            file_name = f'time={datetime.now()}_epoch={epoch}.pt'\n",
    "            save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        if config['early_stopping']>0:\n",
    "            if epoch > 1:\n",
    "                if total_valid_f1 < np.mean(valid_f1[epoch - 7:epoch - 2]):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "            if count >= config['early_stopping']:\n",
    "                print('\\n\\nStopping early due to decrease in performance on validation set\\n\\n')\n",
    "                break \n",
    "        if count == 0:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1))\n",
    "        else:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}, model have not improved for {} iterations\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1, count))\n",
    "    if config['wandb']:\n",
    "        save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "        wandb.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:0x1rrp9p) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:0x1rrp9p). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35693a570f44b9ba6137febc6c8f93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669488116167485, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/artemis.dgpsrv/ondemand27/evanpan/Gaze_project/training/wandb/run-20230410_204035-3jnw3zl1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/3jnw3zl1' target=\"_blank\">legendary-snow-85</a></strong> to <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/3jnw3zl1' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/3jnw3zl1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_model_with_vel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m training_dataset[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m train_model_with_vel(model, config, train_dataloader, valid_dataloader, run_obj, \u001b[39m\"\u001b[39m\u001b[39msentence_and_words_with_vel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m run_obj\u001b[39m.\u001b[39mfinish()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model_with_vel' is not defined"
     ]
    }
   ],
   "source": [
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\", \"r\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "model.to(device)\n",
    "training_dataset[0]\n",
    "train_model_with_vel(model, config, train_dataloader, valid_dataloader, run_obj, \"sentence_and_words_with_vel\")\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:0x1rrp9p) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>percentage_predicted_aversion</td><td>0.63243</td></tr><tr><td>training loss</td><td>0.16717</td></tr><tr><td>training_f1</td><td>0.92708</td></tr><tr><td>validation_f1</td><td>0.91199</td></tr><tr><td>validation_loss</td><td>0.18008</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Word+sentence+audio with velocity loss</strong> at: <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/0x1rrp9p' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/0x1rrp9p</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230410_205557-0x1rrp9p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:0x1rrp9p). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9249cd3a22425d880210783f6c6eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669333699004103, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/artemis.dgpsrv/ondemand27/evanpan/Gaze_project/training/wandb/run-20230410_205633-0x1rrp9p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/0x1rrp9p' target=\"_blank\">Word+sentence+audio with velocity loss</a></strong> to <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/0x1rrp9p' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/0x1rrp9p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289, mean: 0.727568507194519, std: 0.4452143907546997\n",
      "training L: 0.8862669987488125\n",
      "validation L:0.8699159448302187\n",
      "Epoch 290, mean: 0.6913692951202393, std: 0.46193215250968933\n",
      "training L: 0.9023071043049081\n",
      "validation L:0.9032333195554947\n",
      "Epoch 291, mean: 0.6280165910720825, std: 0.48333799839019775\n",
      "training L: 0.9212732141318789\n",
      "validation L:0.9204383766673967\n",
      "Epoch 292, mean: 0.6386224031448364, std: 0.4804036617279053\n",
      "training L: 0.9278142231326103\n",
      "validation L:0.9182339477268722\n",
      "Epoch 293, mean: 0.6577759385108948, std: 0.47445806860923767\n",
      "training L: 0.9279337160812661\n",
      "validation L:0.919067533646487\n",
      "Epoch 294, mean: 0.6244979500770569, std: 0.4842562973499298\n",
      "training L: 0.9294962620281899\n",
      "validation L:0.9166458217475921\n",
      "Epoch 295, mean: 0.636564314365387, std: 0.48099276423454285\n",
      "training L: 0.9304094280362858\n",
      "validation L:0.9090100685655462\n",
      "Epoch 296, mean: 0.6215767860412598, std: 0.4849979281425476\n",
      "training L: 0.9275395727425941\n",
      "validation L:0.8954359215854675\n",
      "Epoch 297, mean: 0.6678672432899475, std: 0.47098225355148315\n",
      "training L: 0.9284840907072885\n",
      "validation L:0.9088835050936316\n",
      "Epoch 298, mean: 0.599601686000824, std: 0.4899831712245941\n",
      "training L: 0.9307441635142087\n",
      "validation L:0.9146186530694254\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\", \"r\"))\n",
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "config[\"learning_rate\"] = 0.001\n",
    "config[\"load_model\"] = True\n",
    "if config[\"wandb\"]:\n",
    "    wandb.login()\n",
    "    if config[\"load_model\"]:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, save_code=True,\n",
    "            resume='allow', id='0x1rrp9p')\n",
    "        # checkpoint_name = \"gaze_prediction_team/gaze_prediction/8w9fyxan\"\n",
    "        checkpoint_path = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/sentence_and_words_with_vel/time=2023-04-10 17:21:49.696880_epoch=267.pt\"\n",
    "        wandb.restore(checkpoint_path)\n",
    "        pretrained_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_weights(pretrained_dict)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "else:\n",
    "    run_obj = None\n",
    "\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "\n",
    "train_model_with_vel(model, config, train_dataloader, valid_dataloader, run_obj, \"sentence_and_words_with_vel\", 289)\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for sentence + word + audio. but also uses velocity as a loss function, also predict gaze Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity(out):\n",
    "    vel = torch.diff(out, dim=1, prepend=out[:, 0:1, :])\n",
    "    return vel\n",
    "def train_model_with_for_direction(model, config, train_data, valid_data, wandb, model_name, start = 1, num_of_classes=4):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train() \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_fn2 = nn.CrossEntropyLoss()\n",
    "    loss_fn_vel = nn.MSELoss()\n",
    "    training_loss = []\n",
    "    valid_loss = []\n",
    "    training_f1 = []\n",
    "    valid_f1 = []\n",
    "    aversion_vs_start = []\n",
    "    training_direction_pred_f1 = []\n",
    "    valid_direction_pred_f1 = []\n",
    "    count = 0\n",
    "    # f1_score = BinaryF1Score(num_classes=2).to(device)\n",
    "    f1_score = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\").to(device)\n",
    "    f1_score_direction = F1Score(task=\"multiclass\", num_classes=num_of_classes, average=\"weighted\").to(device)\n",
    "    for epoch in range(start, start + config['epochs'] + 1):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "        total_aversion_predicted = 0\n",
    "        total_train_f1 = 0\n",
    "        total_valid_f1 = 0\n",
    "        total_train_direction_f1 = 0\n",
    "        total_valid_direction_f1 = 0\n",
    "        train_batch_counter = 0\n",
    "        valid_batch_counter = 0\n",
    "        total_prediction_counter = 0\n",
    "        prediction_mean = 0\n",
    "        prediction_std = 0\n",
    "        model.zero_grad()\n",
    "        for _, (X, [Y, Y_vel, Y_dir]) in enumerate(train_data):\n",
    "            train_batch_counter += 1\n",
    "            X, Y, Y_vel, Y_dir= X.to(device), Y.to(device), Y_vel.to(device), Y_dir.to(device)   \n",
    "            optimiser.zero_grad()\n",
    "            if \"Transformer\" in config[\"model_type\"]:\n",
    "                all_zero = torch.zeros(Y.shape).to(device)\n",
    "                pred, dire_pred = model(X, all_zero)\n",
    "            else:\n",
    "                pred, dire_pred = model(X)\n",
    "            loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "            # loss for the directional classification\n",
    "            if epoch <= 100:\n",
    "                factor = 0.1\n",
    "            else:\n",
    "                factor = 1\n",
    "            loss = loss + factor * loss_fn2(dire_pred.transpose(2, 1), torch.argmax(Y_dir, axis=2).long())\n",
    "            # get the softmax\n",
    "            pred_vel = get_velocity(torch.softmax(pred, axis=2))\n",
    "            # add the velocity along dimension 1 and 2 respectively, the velocity are opposite \n",
    "            loss = loss + 0.3 * loss_fn_vel(pred_vel[:, :, 1], Y_vel)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_train_loss += loss.item()\n",
    "            # binary_pred = torch.round(pred)\n",
    "            binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "            dire_pred = torch.argmax(dire_pred, axis=2, keepdim=True)\n",
    "            f1_train = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "            f1_dire_train = f1_score_direction(dire_pred, torch.argmax(Y_dir, axis=2, keepdim=True)).item()\n",
    "            \n",
    "            prediction_mean = torch.mean(binary_pred.float()).item()\n",
    "            prediction_std = torch.std(binary_pred.float()).item()            \n",
    "            \n",
    "            total_aversion_predicted += torch.sum(binary_pred).item()\n",
    "            total_prediction_counter += binary_pred.size()[0] * binary_pred.size()[1] \n",
    "            total_train_f1 += f1_train\n",
    "            total_train_direction_f1 += f1_dire_train\n",
    "            del X, Y, pred, Y_vel, Y_dir, dire_pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        total_train_f1 /= train_batch_counter\n",
    "        total_train_direction_f1 /= train_batch_counter\n",
    "        total_train_loss /= len(train_data)\n",
    "        total_aversion_predicted /= total_prediction_counter\n",
    "        \n",
    "        for _, (X, [Y, Y_vel, Y_dir]) in enumerate(valid_data):\n",
    "            with torch.no_grad():\n",
    "                valid_batch_counter += 1\n",
    "                X, Y, Y_vel, Y_dir= X.to(device), Y.to(device), Y_vel.to(device), Y_dir.to(device)   \n",
    "                if \"Transformer\" in config[\"model_type\"]:\n",
    "                    all_zero = torch.zeros(Y.shape).to(device)\n",
    "                    pred, dire_pred = model(X, all_zero)\n",
    "                else:\n",
    "                    pred, dire_pred = model(X)\n",
    "                loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "                loss = loss + loss_fn2(dire_pred.transpose(2, 1), torch.argmax(Y_dir, axis=2).long())\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "                # binary_pred = torch.round(pred)\n",
    "                binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "                f1_valid = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "            \n",
    "                dire_pred = torch.argmax(dire_pred, axis=2, keepdim=True)\n",
    "                f1_dire_valid = f1_score_direction(dire_pred, torch.argmax(Y_dir, axis=2, keepdim=True)).item()\n",
    "\n",
    "                total_valid_f1 += f1_valid\n",
    "                total_valid_direction_f1 += f1_dire_valid\n",
    "                del X, Y, pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        total_valid_f1 /= valid_batch_counter\n",
    "        total_valid_direction_f1 /= valid_batch_counter\n",
    "        total_valid_loss /= len(valid_data)\n",
    "\n",
    "        if config['wandb']:\n",
    "            wandb.log({'training loss': total_train_loss,\n",
    "                        'validation_loss': total_valid_loss,\n",
    "                        'training_f1': total_train_f1,\n",
    "                        'validation_f1': total_valid_f1, \n",
    "                        'training_f1_direction': total_train_direction_f1,\n",
    "                        'validation_f1_direction': total_valid_direction_f1,\n",
    "                        \"percentage_predicted_aversion\": total_aversion_predicted})\n",
    "        training_loss.append(total_train_loss)\n",
    "        valid_loss.append(total_valid_loss)\n",
    "        training_f1.append(total_train_f1)\n",
    "        valid_f1.append(total_valid_f1)\n",
    "        aversion_vs_start.append(total_aversion_predicted)\n",
    "        if total_valid_f1 == max(valid_f1):\n",
    "            try:\n",
    "                os.mkdir(os.path.join(*[model_save_location, model_name]))\n",
    "            except:\n",
    "                pass\n",
    "            config_save_path = os.path.join(*[model_save_location, model_name, \"config.json\"])\n",
    "            json.dump(config, open(config_save_path, \"w\"))\n",
    "            file_name = f'time={datetime.now()}_epoch={epoch}.pt'\n",
    "            save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        if config['early_stopping']>0:\n",
    "            if epoch > 1:\n",
    "                if total_valid_f1 < np.mean(valid_f1[epoch - 7:epoch - 2]):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "            if count >= config['early_stopping']:\n",
    "                print('\\n\\nStopping early due to decrease in performance on validation set\\n\\n')\n",
    "                break \n",
    "        if count == 0:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1))\n",
    "        else:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}, model have not improved for {} iterations\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1, count))\n",
    "    if config['wandb']:\n",
    "        save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "        wandb.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe4e5d2e89a44239eb0c1eaf92fe3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670152865117416, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /tmp/ipykernel_64833/1140664169.py 1 <module>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_obj \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39;49minit(project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgaze_prediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49mconfig, settings\u001b[39m=\u001b[39;49mwandb\u001b[39m.\u001b[39;49mSettings(start_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfork\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      2\u001b[0m config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config2.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1166\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[39massert\u001b[39;00m logger\n\u001b[1;32m   1165\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39minterrupted\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[0;32m-> 1166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1167\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1168\u001b[0m     error_seen \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1144\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1142\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1144\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m   1145\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1146\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:744\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    741\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcommunicating run to backend with \u001b[39m\u001b[39m{\u001b[39;00mtimeout\u001b[39m}\u001b[39;00m\u001b[39m second timeout\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    743\u001b[0m run_init_handle \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39minterface\u001b[39m.\u001b[39mdeliver_run(run_proto)\n\u001b[0;32m--> 744\u001b[0m result \u001b[39m=\u001b[39m run_init_handle\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    745\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    746\u001b[0m     on_progress\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_progress_init,\n\u001b[1;32m    747\u001b[0m     cancel\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    748\u001b[0m )\n\u001b[1;32m    749\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    750\u001b[0m     run_result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mrun_result\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interface\u001b[39m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m MailboxError(\u001b[39m\"\u001b[39m\u001b[39mtransport failed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slot\u001b[39m.\u001b[39;49m_get_and_clear(timeout\u001b[39m=\u001b[39;49mwait_timeout)\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m on_progress \u001b[39mand\u001b[39;00m progress_handle \u001b[39mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_and_clear\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Optional[pb\u001b[39m.\u001b[39mResult], \u001b[39mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout\u001b[39m=\u001b[39;49mtimeout):\n\u001b[1;32m    131\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wait\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config2.json\", \"r\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_and_Directions_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "validation_dataset = Aversion_and_Directions_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SentenceBaseline_Gaze_and_Direction_PredictionModel(config)\n",
    "model.to(device)\n",
    "# training_dataset[0]\n",
    "train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \"aversion_and_direction_larg_batch\")\n",
    "run_obj.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevanpan\u001b[0m (\u001b[33mgaze_prediction_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b165acd33048edac2bd129ed7ac68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670304100262, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/artemis.dgpsrv/ondemand27/evanpan/Gaze_project/training/wandb/run-20230418_100007-vgl6tnx8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/vgl6tnx8' target=\"_blank\">unique-bee-92</a></strong> to <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/vgl6tnx8' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/vgl6tnx8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000, mean: 0.5964974761009216, std: 0.49060043692588806\n",
      "training L: 0.7636961073059446\n",
      "validation L:0.7689137324044213\n",
      "Epoch 2001, mean: 0.7000855207443237, std: 0.4582207500934601\n",
      "training L: 0.76545473908605\n",
      "validation L:0.7659171881928002\n",
      "Epoch 2002, mean: 0.6969318985939026, std: 0.45958492159843445\n",
      "training L: 0.7615947322489782\n",
      "validation L:0.7623851474615961\n",
      "Epoch 2003, mean: 0.6235925555229187, std: 0.4844846725463867\n",
      "training L: 0.7680084032317864\n",
      "validation L:0.762308164834449\n",
      "Epoch 2004, mean: 0.6037479043006897, std: 0.4891184866428375\n",
      "training L: 0.7716686227492106\n",
      "validation L:0.7716569796785826\n",
      "Epoch 2005, mean: 0.6100979447364807, std: 0.48772838711738586\n",
      "training L: 0.7708461095801616\n",
      "validation L:0.7730724027016194\n",
      "Epoch 2006, mean: 0.6527360677719116, std: 0.4761010408401489\n",
      "training L: 0.7702094992859493\n",
      "validation L:0.7732597568345345\n",
      "Epoch 2007, mean: 0.6747980117797852, std: 0.4684508144855499\n",
      "training L: 0.7696129638568975\n",
      "validation L:0.7695217552467748\n",
      "Epoch 2008, mean: 0.6792886853218079, std: 0.46675053238868713\n",
      "training L: 0.7687486765577729\n",
      "validation L:0.7685967563306427\n",
      "Epoch 2009, mean: 0.690136194229126, std: 0.46243777871131897\n",
      "training L: 0.7689987021358811\n",
      "validation L:0.7698396419321869\n",
      "Epoch 2010, mean: 0.6628227233886719, std: 0.47274652123451233\n",
      "training L: 0.7710965457366501\n",
      "validation L:0.7712337903363602\n",
      "Epoch 2011, mean: 0.6548812985420227, std: 0.47540751099586487\n",
      "training L: 0.7733001405819733\n",
      "validation L:0.7726966462233229\n",
      "Epoch 2012, mean: 0.6477771401405334, std: 0.477663516998291\n",
      "training L: 0.7749214867606964\n",
      "validation L:0.7744532127754531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2013, mean: 0.6394800543785095, std: 0.480151891708374\n",
      "training L: 0.7756743898189092\n",
      "validation L:0.7756401977547731\n",
      "Epoch 2014, mean: 0.6344783306121826, std: 0.48157668113708496\n",
      "training L: 0.7753428178748879\n",
      "validation L:0.7770119938721615\n",
      "Epoch 2015, mean: 0.6487113237380981, std: 0.47737351059913635\n",
      "training L: 0.7757421687511182\n",
      "validation L:0.7768912798836576\n",
      "Epoch 2016, mean: 0.6484209299087524, std: 0.47746387124061584\n",
      "training L: 0.7760938260563196\n",
      "validation L:0.7752476759948539\n",
      "Epoch 2017, mean: 0.6456319689750671, std: 0.47832190990448\n",
      "training L: 0.7760918199074245\n",
      "validation L:0.775854221338377\n",
      "Epoch 2018, mean: 0.6496117115020752, std: 0.47709208726882935\n",
      "training L: 0.7763709893244655\n",
      "validation L:0.7750653899924546\n",
      "Epoch 2019, mean: 0.6668227314949036, std: 0.471349835395813\n",
      "training L: 0.7767098275815247\n",
      "validation L:0.776819389594618\n",
      "Epoch 2020, mean: 0.6682025790214539, std: 0.47085919976234436\n",
      "training L: 0.7758314223377502\n",
      "validation L:0.7777598101194084\n",
      "Epoch 2021, mean: 0.6588249802589417, std: 0.47410455346107483\n",
      "training L: 0.7767718158613881\n",
      "validation L:0.7777068849587121\n",
      "Epoch 2022, mean: 0.6374136209487915, std: 0.48074737191200256\n",
      "training L: 0.7783528990296348\n",
      "validation L:0.7791644884166055\n",
      "Epoch 2023, mean: 0.6343635320663452, std: 0.48160871863365173\n",
      "training L: 0.778638872459579\n",
      "validation L:0.7786785204634723\n",
      "Epoch 2024, mean: 0.6710343360900879, std: 0.4698379933834076\n",
      "training L: 0.7768668014472866\n",
      "validation L:0.7798167515152168\n",
      "Epoch 2025, mean: 0.6611727476119995, std: 0.47331157326698303\n",
      "training L: 0.7761622895971338\n",
      "validation L:0.7797231098097186\n",
      "Epoch 2026, mean: 0.6564614772796631, std: 0.4748898148536682\n",
      "training L: 0.7781231170128922\n",
      "validation L:0.7794362921209212\n",
      "Epoch 2027, mean: 0.6195813417434692, std: 0.4854903221130371\n",
      "training L: 0.7797468603286994\n",
      "validation L:0.7762613288125786\n",
      "Epoch 2028, mean: 0.6416837573051453, std: 0.4795062243938446\n",
      "training L: 0.7794087961986798\n",
      "validation L:0.7778498646249521\n",
      "Epoch 2029, mean: 0.6625481247901917, std: 0.4728409945964813\n",
      "training L: 0.7783026371896021\n",
      "validation L:0.7800237022150187\n",
      "Epoch 2030, mean: 0.6629285216331482, std: 0.4727100729942322\n",
      "training L: 0.7766106875997343\n",
      "validation L:0.7793090050925137\n",
      "Epoch 2031, mean: 0.6524209380149841, std: 0.4762020409107208\n",
      "training L: 0.7789343036660916\n",
      "validation L:0.7783036931504621\n",
      "Epoch 2032, mean: 0.6277073621749878, std: 0.4834163188934326\n",
      "training L: 0.780018092573225\n",
      "validation L:0.7786181816986468\n",
      "Epoch 2033, mean: 0.6515408158302307, std: 0.4764828085899353\n",
      "training L: 0.7801804673575304\n",
      "validation L:0.7791549284195871\n",
      "Epoch 2034, mean: 0.6663388013839722, std: 0.47152087092399597\n",
      "training L: 0.7792463213471649\n",
      "validation L:0.7799497283099224\n",
      "Epoch 2035, mean: 0.6399234533309937, std: 0.4800228476524353\n",
      "training L: 0.7795181155082834\n",
      "validation L:0.7807654881347725\n",
      "Epoch 2036, mean: 0.6459020972251892, std: 0.47823959589004517\n",
      "training L: 0.7799546602416523\n",
      "validation L:0.7810838216833084\n",
      "Epoch 2037, mean: 0.6417040228843689, std: 0.47950026392936707\n",
      "training L: 0.7804409366670664\n",
      "validation L:0.7795644030548801\n",
      "Epoch 2038, mean: 0.6364277005195618, std: 0.48102807998657227\n",
      "training L: 0.7804066702478382\n",
      "validation L:0.7804011238299416\n",
      "Epoch 2039, mean: 0.6495869755744934, std: 0.47709983587265015\n",
      "training L: 0.7805165999110277\n",
      "validation L:0.781269361371761\n",
      "Epoch 2040, mean: 0.6444907188415527, std: 0.47866788506507874\n",
      "training L: 0.7806394892288394\n",
      "validation L:0.781431560143052\n",
      "Epoch 2041, mean: 0.6488125920295715, std: 0.4773419499397278\n",
      "training L: 0.7811157340671474\n",
      "validation L:0.7827997456796063\n",
      "Epoch 2042, mean: 0.645926833152771, std: 0.4782320261001587\n",
      "training L: 0.7818790640465609\n",
      "validation L:0.7828322876180807\n",
      "Epoch 2043, mean: 0.6391491293907166, std: 0.4802478849887848\n",
      "training L: 0.7816370035783893\n",
      "validation L:0.7829044453599765\n",
      "Epoch 2044, mean: 0.6470523476600647, std: 0.4778871238231659\n",
      "training L: 0.7819700372226405\n",
      "validation L:0.7817544234607634\n",
      "Epoch 2045, mean: 0.6566212773323059, std: 0.47483712434768677\n",
      "training L: 0.7810511836956027\n",
      "validation L:0.7808197838863687\n",
      "Epoch 2046, mean: 0.6551874279975891, std: 0.4753077030181885\n",
      "training L: 0.7811188602892569\n",
      "validation L:0.782530907194935\n",
      "Epoch 2047, mean: 0.6430996060371399, std: 0.4790855944156647\n",
      "training L: 0.7820109103266242\n",
      "validation L:0.7825038056667892\n",
      "Epoch 2048, mean: 0.6305188536643982, std: 0.4826648533344269\n",
      "training L: 0.7827316375261795\n",
      "validation L:0.7831026725132512\n",
      "Epoch 2049, mean: 0.6455959677696228, std: 0.47833287715911865\n",
      "training L: 0.7827760332938192\n",
      "validation L:0.7839270049544618\n",
      "Epoch 2050, mean: 0.655743420124054, std: 0.475125789642334\n",
      "training L: 0.782031258688373\n",
      "validation L:0.7839778947631766\n",
      "Epoch 2051, mean: 0.6472819447517395, std: 0.4778164327144623\n",
      "training L: 0.781920183895849\n",
      "validation L:0.7856625335347469\n",
      "Epoch 2052, mean: 0.6445087194442749, std: 0.47866249084472656\n",
      "training L: 0.7832256943014\n",
      "validation L:0.7848637644575248\n",
      "Epoch 2053, mean: 0.6327495574951172, std: 0.48205605149269104\n",
      "training L: 0.7838557941793658\n",
      "validation L:0.7865880803610755\n",
      "Epoch 2054, mean: 0.6441598534584045, std: 0.47876763343811035\n",
      "training L: 0.7844509789409769\n",
      "validation L:0.7874360980643142\n",
      "Epoch 2055, mean: 0.6457805633544922, std: 0.47827666997909546\n",
      "training L: 0.7838359623798247\n",
      "validation L:0.7860552244622636\n",
      "Epoch 2056, mean: 0.6413753628730774, std: 0.479597270488739\n",
      "training L: 0.7840502562581011\n",
      "validation L:0.7860773541221124\n",
      "Epoch 2057, mean: 0.6372740864753723, std: 0.4807872176170349\n",
      "training L: 0.7845036387997767\n",
      "validation L:0.785968899862103\n",
      "Epoch 2058, mean: 0.6438356637954712, std: 0.4788651168346405\n",
      "training L: 0.7849007151405527\n",
      "validation L:0.7854511911565338\n",
      "Epoch 2059, mean: 0.6501497030258179, std: 0.47692301869392395\n",
      "training L: 0.7839515488305527\n",
      "validation L:0.7849017240409445\n",
      "Epoch 2060, mean: 0.6429398059844971, std: 0.4791333079338074\n",
      "training L: 0.7841867768995889\n",
      "validation L:0.7872132131038034\n",
      "Epoch 2061, mean: 0.6376139521598816, std: 0.48069003224372864\n",
      "training L: 0.7852454589353953\n",
      "validation L:0.788320125210688\n",
      "Epoch 2062, mean: 0.6461856961250305, std: 0.47815296053886414\n",
      "training L: 0.7855430009028337\n",
      "validation L:0.7886967305497696\n",
      "Epoch 2063, mean: 0.6444299221038818, std: 0.47868624329566956\n",
      "training L: 0.7839367443944558\n",
      "validation L:0.78865469759768\n",
      "Epoch 2064, mean: 0.6458390355110168, std: 0.47825878858566284\n",
      "training L: 0.7846600882466127\n",
      "validation L:0.7866774593499231\n",
      "Epoch 2065, mean: 0.6398199200630188, std: 0.48005303740501404\n",
      "training L: 0.7854265638388833\n",
      "validation L:0.7859165656567173\n",
      "Epoch 2066, mean: 0.6389105319976807, std: 0.4803169369697571\n",
      "training L: 0.7848914180905282\n",
      "validation L:0.7860139357096634\n",
      "Epoch 2067, mean: 0.6523624062538147, std: 0.4762207567691803\n",
      "training L: 0.7844065332786208\n",
      "validation L:0.7848874556263454\n",
      "Epoch 2068, mean: 0.6415982246398926, std: 0.47953152656555176\n",
      "training L: 0.7843841729590105\n",
      "validation L:0.785933396937402\n",
      "Epoch 2069, mean: 0.6515138149261475, std: 0.47649142146110535\n",
      "training L: 0.7859806116097505\n",
      "validation L:0.784659108678922\n",
      "Epoch 2070, mean: 0.6373550891876221, std: 0.48076409101486206\n",
      "training L: 0.7859658347470765\n",
      "validation L:0.7843206573458825\n",
      "Epoch 2071, mean: 0.6361733078956604, std: 0.4811001420021057\n",
      "training L: 0.7863912296551865\n",
      "validation L:0.7869277713716455\n",
      "Epoch 2072, mean: 0.6537242531776428, std: 0.4757828712463379\n",
      "training L: 0.7861708937848144\n",
      "validation L:0.7872657163202186\n",
      "Epoch 2073, mean: 0.6389060020446777, std: 0.4803183078765869\n",
      "training L: 0.7857741875821587\n",
      "validation L:0.7885817627441496\n",
      "Epoch 2074, mean: 0.6382577419281006, std: 0.4805052578449249\n",
      "training L: 0.7867663785000194\n",
      "validation L:0.7876498715448937\n",
      "Epoch 2075, mean: 0.6439121961593628, std: 0.4788421392440796\n",
      "training L: 0.7872435865197107\n",
      "validation L:0.7885953979675648\n",
      "Epoch 2076, mean: 0.6496477127075195, std: 0.477080762386322\n",
      "training L: 0.7869915137054184\n",
      "validation L:0.7868721352748735\n",
      "Epoch 2077, mean: 0.6291975378990173, std: 0.48302021622657776\n",
      "training L: 0.7875403292587566\n",
      "validation L:0.7855871263252038\n",
      "Epoch 2078, mean: 0.6538143157958984, std: 0.4757537841796875\n",
      "training L: 0.7867690877305429\n",
      "validation L:0.78669386877983\n",
      "Epoch 2079, mean: 0.6524366736412048, std: 0.4761969745159149\n",
      "training L: 0.7857085809620563\n",
      "validation L:0.7867269059282471\n",
      "Epoch 2080, mean: 0.6436331272125244, std: 0.4789259433746338\n",
      "training L: 0.7860463506207153\n",
      "validation L:0.7866953233321419\n",
      "Epoch 2081, mean: 0.6485154628753662, std: 0.4774344563484192\n",
      "training L: 0.787324196340579\n",
      "validation L:0.7879858638046251\n",
      "Epoch 2082, mean: 0.6551851630210876, std: 0.4753083884716034\n",
      "training L: 0.7864584145933277\n",
      "validation L:0.7865321885976686\n",
      "Epoch 2083, mean: 0.6417310237884521, std: 0.47949227690696716\n",
      "training L: 0.7860426553585278\n",
      "validation L:0.7884158002903242\n",
      "Epoch 2084, mean: 0.6430433392524719, std: 0.47910240292549133\n",
      "training L: 0.787500112925018\n",
      "validation L:0.7858049113085599\n",
      "Epoch 2085, mean: 0.6415148973464966, std: 0.4795560836791992\n",
      "training L: 0.7870712799861701\n",
      "validation L:0.7857908401812719\n",
      "Epoch 2086, mean: 0.6273742318153381, std: 0.48350420594215393\n",
      "training L: 0.7869380538192172\n",
      "validation L:0.7815448647960632\n",
      "Epoch 2087, mean: 0.6570737361907959, std: 0.4746876358985901\n",
      "training L: 0.7842580164830162\n",
      "validation L:0.7835665459856439\n",
      "Epoch 2088, mean: 0.6626583933830261, std: 0.4728030860424042\n",
      "training L: 0.7827882069330234\n",
      "validation L:0.7857466106202946\n",
      "Epoch 2089, mean: 0.622689962387085, std: 0.4847140312194824\n",
      "training L: 0.7858060436656797\n",
      "validation L:0.7786668196779165\n",
      "Epoch 2090, mean: 0.6231648921966553, std: 0.48459357023239136\n",
      "training L: 0.7854796522980569\n",
      "validation L:0.7843457174649682\n",
      "Epoch 2091, mean: 0.6520382761955261, std: 0.4763243496417999\n",
      "training L: 0.7852484769907533\n",
      "validation L:0.7830046711162381\n",
      "Epoch 2092, mean: 0.6644074320793152, std: 0.4721977412700653\n",
      "training L: 0.7826252103801326\n",
      "validation L:0.7766683368931815\n",
      "Epoch 2093, mean: 0.6650174260139465, std: 0.4719848930835724\n",
      "training L: 0.7788815033401878\n",
      "validation L:0.7747387361206279\n",
      "Epoch 2094, mean: 0.6105931401252747, std: 0.4876163601875305\n",
      "training L: 0.7797927702289948\n",
      "validation L:0.7698528477927827\n",
      "Epoch 2095, mean: 0.6301901936531067, std: 0.48275360465049744\n",
      "training L: 0.7775023822839096\n",
      "validation L:0.7716381321715675\n",
      "Epoch 2096, mean: 0.69545978307724, std: 0.46021294593811035\n",
      "training L: 0.7710068170934574\n",
      "validation L:0.7694653234754926\n",
      "Epoch 2097, mean: 0.679050087928772, std: 0.4668421149253845\n",
      "training L: 0.7706739853981639\n",
      "validation L:0.7753772188890944\n",
      "Epoch 2098, mean: 0.635032057762146, std: 0.48142173886299133\n",
      "training L: 0.7777766138713741\n",
      "validation L:0.7768805989806569\n",
      "Epoch 2099, mean: 0.6270028352737427, std: 0.4836019277572632\n",
      "training L: 0.7781830189907855\n",
      "validation L:0.7782205224918496\n",
      "Epoch 2100, mean: 0.6430996060371399, std: 0.47908562421798706\n",
      "training L: 0.7787656609193778\n",
      "validation L:0.7784499122805482\n",
      "Epoch 2101, mean: 0.6623973250389099, std: 0.4728928208351135\n",
      "training L: 0.7777161754763879\n",
      "validation L:0.7762347659889458\n",
      "Epoch 2102, mean: 0.6486302614212036, std: 0.47739875316619873\n",
      "training L: 0.7758915862456761\n",
      "validation L:0.7774175035134595\n",
      "Epoch 2103, mean: 0.6551018953323364, std: 0.47533559799194336\n",
      "training L: 0.775740201703693\n",
      "validation L:0.7803695983909711\n",
      "Epoch 2104, mean: 0.6570242047309875, std: 0.47470399737358093\n",
      "training L: 0.7770171326232551\n",
      "validation L:0.7792946556755294\n",
      "Epoch 2105, mean: 0.6290106773376465, std: 0.48307016491889954\n",
      "training L: 0.7787206092302341\n",
      "validation L:0.778708833809896\n",
      "Epoch 2106, mean: 0.6350343227386475, std: 0.4814210832118988\n",
      "training L: 0.7797414219053316\n",
      "validation L:0.7785829247608029\n",
      "Epoch 2107, mean: 0.6293821334838867, std: 0.48297080397605896\n",
      "training L: 0.7798920509471655\n",
      "validation L:0.7779346136877712\n",
      "Epoch 2108, mean: 0.6594327688217163, std: 0.47390052676200867\n",
      "training L: 0.7797505404519954\n",
      "validation L:0.7778946855485203\n",
      "Epoch 2109, mean: 0.6481800675392151, std: 0.47753867506980896\n",
      "training L: 0.7796450157845625\n",
      "validation L:0.7781083734503877\n",
      "Epoch 2110, mean: 0.649962842464447, std: 0.4769818186759949\n",
      "training L: 0.7807988201913969\n",
      "validation L:0.7783418238753534\n",
      "Epoch 2111, mean: 0.6476713418960571, std: 0.4776962101459503\n",
      "training L: 0.7815185434139349\n",
      "validation L:0.7784563961462906\n",
      "Epoch 2112, mean: 0.6578728556632996, std: 0.4744224548339844\n",
      "training L: 0.7818234291379272\n",
      "validation L:0.7795022939511138\n",
      "Epoch 2113, mean: 0.6453055739402771, std: 0.4784211814403534\n",
      "training L: 0.7819656681075111\n",
      "validation L:0.7801357010209613\n",
      "Epoch 2114, mean: 0.6472639441490173, std: 0.4778220057487488\n",
      "training L: 0.7825331727319862\n",
      "validation L:0.7816572611468362\n",
      "Epoch 2115, mean: 0.6329972147941589, std: 0.4819878339767456\n",
      "training L: 0.7824789263451719\n",
      "validation L:0.7815687660024618\n",
      "Epoch 2116, mean: 0.6429600715637207, std: 0.47912728786468506\n",
      "training L: 0.782715539388386\n",
      "validation L:0.7818373937139459\n",
      "Epoch 2117, mean: 0.6374564170837402, std: 0.4807351231575012\n",
      "training L: 0.7830017075150865\n",
      "validation L:0.7802358092344304\n",
      "Epoch 2118, mean: 0.6516398787498474, std: 0.4764513373374939\n",
      "training L: 0.7828055928513067\n",
      "validation L:0.78228705593908\n",
      "Epoch 2119, mean: 0.6353179812431335, std: 0.4813414216041565\n",
      "training L: 0.7828384892041194\n",
      "validation L:0.7830185340757769\n",
      "Epoch 2120, mean: 0.6420618891716003, std: 0.4793943464756012\n",
      "training L: 0.7838609282792806\n",
      "validation L:0.7830848074043568\n",
      "Epoch 2121, mean: 0.634975790977478, std: 0.48143747448921204\n",
      "training L: 0.7842432147263416\n",
      "validation L:0.783233423307897\n",
      "Epoch 2122, mean: 0.6501632332801819, std: 0.4769187867641449\n",
      "training L: 0.7842097542569213\n",
      "validation L:0.7827449262938186\n",
      "Epoch 2123, mean: 0.6399459838867188, std: 0.48001629114151\n",
      "training L: 0.7837946531125187\n",
      "validation L:0.7802007247553822\n",
      "Epoch 2124, mean: 0.6404547095298767, std: 0.47986769676208496\n",
      "training L: 0.7842768209318871\n",
      "validation L:0.7813332202507601\n",
      "Epoch 2125, mean: 0.6644839644432068, std: 0.4721710681915283\n",
      "training L: 0.7835665965854701\n",
      "validation L:0.7808035872287311\n",
      "Epoch 2126, mean: 0.6446843147277832, std: 0.4786094129085541\n",
      "training L: 0.7831827480285803\n",
      "validation L:0.7803795779909665\n",
      "Epoch 2127, mean: 0.6313719749450684, std: 0.48243334889411926\n",
      "training L: 0.7842163600321663\n",
      "validation L:0.7799765747623244\n",
      "Epoch 2128, mean: 0.6269803047180176, std: 0.48360782861709595\n",
      "training L: 0.7851558168355508\n",
      "validation L:0.7794129393256729\n",
      "Epoch 2129, mean: 0.683716356754303, std: 0.46502557396888733\n",
      "training L: 0.7800390439715434\n",
      "validation L:0.7781375813744758\n",
      "Epoch 2130, mean: 0.6248058676719666, std: 0.4841735363006592\n",
      "training L: 0.782773670152123\n",
      "validation L:0.7737265347375115\n",
      "Epoch 2131, mean: 0.6229397654533386, std: 0.48465076088905334\n",
      "training L: 0.7823603855126896\n",
      "validation L:0.773778324928899\n",
      "Epoch 2132, mean: 0.6563444137573242, std: 0.4749283492565155\n",
      "training L: 0.7814217357573817\n",
      "validation L:0.7771047648849791\n",
      "Epoch 2133, mean: 0.6881980895996094, std: 0.4632299244403839\n",
      "training L: 0.778231261269655\n",
      "validation L:0.7760617253785665\n",
      "Epoch 2134, mean: 0.6603264212608337, std: 0.4735989570617676\n",
      "training L: 0.7787838599449988\n",
      "validation L:0.7777729310178226\n",
      "Epoch 2135, mean: 0.6325807571411133, std: 0.48210257291793823\n",
      "training L: 0.7816977433030314\n",
      "validation L:0.7796777026724747\n",
      "Epoch 2136, mean: 0.6344310641288757, std: 0.4815898835659027\n",
      "training L: 0.7829989002331286\n",
      "validation L:0.7788750708116999\n",
      "Epoch 2137, mean: 0.6348002552986145, std: 0.4814866781234741\n",
      "training L: 0.7830301086342769\n",
      "validation L:0.7816161172643867\n",
      "Epoch 2138, mean: 0.656317412853241, std: 0.4749372601509094\n",
      "training L: 0.782537153063112\n",
      "validation L:0.7812233824502843\n",
      "Epoch 2139, mean: 0.6611412763595581, std: 0.4733222723007202\n",
      "training L: 0.7825300763335353\n",
      "validation L:0.7805884870105391\n",
      "Epoch 2140, mean: 0.6678289175033569, std: 0.4709925353527069\n",
      "training L: 0.7822590294448066\n",
      "validation L:0.77938818053102\n",
      "Epoch 2141, mean: 0.6552459001541138, std: 0.4752885699272156\n",
      "training L: 0.7805132932682748\n",
      "validation L:0.7799458206307506\n",
      "Epoch 2142, mean: 0.6643264293670654, std: 0.47222593426704407\n",
      "training L: 0.7790836207487131\n",
      "validation L:0.7820965991665589\n",
      "Epoch 2143, mean: 0.6489679217338562, std: 0.477293461561203\n",
      "training L: 0.7818529504091776\n",
      "validation L:0.7807673624400933\n",
      "Epoch 2144, mean: 0.629848062992096, std: 0.4828457236289978\n",
      "training L: 0.7831208609683393\n",
      "validation L:0.7834684255127884\n",
      "Epoch 2145, mean: 0.6627417206764221, std: 0.4727744162082672\n",
      "training L: 0.7825586884899587\n",
      "validation L:0.7811315679131456\n",
      "Epoch 2146, mean: 0.6716083288192749, std: 0.4696286618709564\n",
      "training L: 0.7793228910441361\n",
      "validation L:0.7811267428470179\n",
      "Epoch 2147, mean: 0.657478928565979, std: 0.4745534062385559\n",
      "training L: 0.7819702323957158\n",
      "validation L:0.7818318074340669\n",
      "Epoch 2148, mean: 0.6425076127052307, std: 0.47926202416419983\n",
      "training L: 0.7842080537881715\n",
      "validation L:0.7813080521649807\n",
      "Epoch 2149, mean: 0.6517366170883179, std: 0.4764205515384674\n",
      "training L: 0.7837268546865557\n",
      "validation L:0.7852741319280203\n",
      "Epoch 2150, mean: 0.645627498626709, std: 0.4783232510089874\n",
      "training L: 0.7828648287306114\n",
      "validation L:0.7875542456824405\n",
      "Epoch 2151, mean: 0.6334969401359558, std: 0.48184967041015625\n",
      "training L: 0.7850477496054199\n",
      "validation L:0.785527887236468\n",
      "Epoch 2152, mean: 0.6418142914772034, std: 0.47946763038635254\n",
      "training L: 0.7859742588142388\n",
      "validation L:0.7854645124069215\n",
      "Epoch 2153, mean: 0.6729769110679626, std: 0.46912631392478943\n",
      "training L: 0.7833219661572649\n",
      "validation L:0.7844231902310523\n",
      "Epoch 2154, mean: 0.659588098526001, std: 0.47384825348854065\n",
      "training L: 0.7832750832351714\n",
      "validation L:0.7869471533308494\n",
      "Epoch 2155, mean: 0.6449769139289856, std: 0.47852087020874023\n",
      "training L: 0.7856367147554992\n",
      "validation L:0.7870411454740687\n",
      "Epoch 2156, mean: 0.6376792192459106, std: 0.48067134618759155\n",
      "training L: 0.7857760142254491\n",
      "validation L:0.786545706638812\n",
      "Epoch 2157, mean: 0.661697268486023, std: 0.4731326699256897\n",
      "training L: 0.7853463829094869\n",
      "validation L:0.7854875684988014\n",
      "Epoch 2158, mean: 0.6634057760238647, std: 0.472545325756073\n",
      "training L: 0.7842667235791768\n",
      "validation L:0.7861622784524582\n",
      "Epoch 2159, mean: 0.6429150104522705, std: 0.4791406989097595\n",
      "training L: 0.7854520417959603\n",
      "validation L:0.7870487857170422\n",
      "Epoch 2160, mean: 0.6367923617362976, std: 0.48092448711395264\n",
      "training L: 0.7860247031630954\n",
      "validation L:0.787932043011745\n",
      "Epoch 2161, mean: 0.6464198231697083, std: 0.4780813455581665\n",
      "training L: 0.7860512008673695\n",
      "validation L:0.7883675537737884\n",
      "Epoch 2162, mean: 0.6481665968894958, std: 0.4775428771972656\n",
      "training L: 0.7868245871174719\n",
      "validation L:0.7877784852724219\n",
      "Epoch 2163, mean: 0.6532425284385681, std: 0.47593826055526733\n",
      "training L: 0.7863567043080356\n",
      "validation L:0.7889056985755688\n",
      "Epoch 2164, mean: 0.6510996222496033, std: 0.47662293910980225\n",
      "training L: 0.7869525635405411\n",
      "validation L:0.789283405348737\n",
      "Epoch 2165, mean: 0.6345233917236328, std: 0.48156410455703735\n",
      "training L: 0.7870330524063945\n",
      "validation L:0.7898123232599288\n",
      "Epoch 2166, mean: 0.6495261788368225, std: 0.4771188795566559\n",
      "training L: 0.7874233095946572\n",
      "validation L:0.7894915992349456\n",
      "Epoch 2167, mean: 0.6309285163879395, std: 0.4825538694858551\n",
      "training L: 0.7874856091320843\n",
      "validation L:0.78935780458752\n",
      "Epoch 2168, mean: 0.640877902507782, std: 0.4797435998916626\n",
      "training L: 0.7885987120524226\n",
      "validation L:0.789632356927646\n",
      "Epoch 2169, mean: 0.6454361081123352, std: 0.478381484746933\n",
      "training L: 0.7885479382643541\n",
      "validation L:0.791022050605886\n",
      "Epoch 2170, mean: 0.6515475511550903, std: 0.4764806926250458\n",
      "training L: 0.7880967772591504\n",
      "validation L:0.7886123959748457\n",
      "Epoch 2171, mean: 0.6500799059867859, std: 0.4769449830055237\n",
      "training L: 0.7885063262559064\n",
      "validation L:0.7900619584240477\n",
      "Epoch 2172, mean: 0.6410084366798401, std: 0.47970524430274963\n",
      "training L: 0.7888684896294333\n",
      "validation L:0.7903977761611714\n",
      "Epoch 2173, mean: 0.6478176712989807, std: 0.4776509702205658\n",
      "training L: 0.7892894276486181\n",
      "validation L:0.7900027233213098\n",
      "Epoch 2174, mean: 0.6462104916572571, std: 0.47814539074897766\n",
      "training L: 0.7891215183761625\n",
      "validation L:0.7901399308903836\n",
      "Epoch 2175, mean: 0.6390410661697388, std: 0.48027917742729187\n",
      "training L: 0.7889121054096521\n",
      "validation L:0.7848228623984161\n",
      "Epoch 2176, mean: 0.6230140924453735, std: 0.48463189601898193\n",
      "training L: 0.7825235401808252\n",
      "validation L:0.7742872427946117\n",
      "Epoch 2177, mean: 0.6734338998794556, std: 0.4689575731754303\n",
      "training L: 0.7754083654575891\n",
      "validation L:0.7681713362707868\n",
      "Epoch 2178, mean: 0.6982330083847046, std: 0.45902523398399353\n",
      "training L: 0.771056252511334\n",
      "validation L:0.7707760675813111\n",
      "Epoch 2179, mean: 0.6505008339881897, std: 0.47681233286857605\n",
      "training L: 0.772603144470283\n",
      "validation L:0.7706437613395065\n",
      "Epoch 2180, mean: 0.6146966814994812, std: 0.4866674244403839\n",
      "training L: 0.7754811534095428\n",
      "validation L:0.7706367558899914\n",
      "Epoch 2181, mean: 0.6740574240684509, std: 0.4687264859676361\n",
      "training L: 0.7656956614847583\n",
      "validation L:0.7657224763849816\n",
      "Epoch 2182, mean: 0.6838964819908142, std: 0.4649544060230255\n",
      "training L: 0.7617146337853384\n",
      "validation L:0.7673831104363105\n",
      "Epoch 2183, mean: 0.6825459003448486, std: 0.4654863178730011\n",
      "training L: 0.7645889093289545\n",
      "validation L:0.7695499985752838\n",
      "Epoch 2184, mean: 0.6767248511314392, std: 0.46772730350494385\n",
      "training L: 0.7668535012296516\n",
      "validation L:0.7723658764891641\n",
      "Epoch 2185, mean: 0.6723804473876953, std: 0.4693458378314972\n",
      "training L: 0.7688787143416811\n",
      "validation L:0.7734744217445058\n",
      "Epoch 2186, mean: 0.6447675824165344, std: 0.4785842299461365\n",
      "training L: 0.7717481334756336\n",
      "validation L:0.7725989013395006\n",
      "Epoch 2187, mean: 0.6340596675872803, std: 0.48169341683387756\n",
      "training L: 0.7741376952062926\n",
      "validation L:0.7718609018138602\n",
      "Epoch 2188, mean: 0.6640157699584961, std: 0.4723339080810547\n",
      "training L: 0.772995513226346\n",
      "validation L:0.7691950531697682\n",
      "Epoch 2189, mean: 0.6891367435455322, std: 0.46284744143486023\n",
      "training L: 0.7696633937654206\n",
      "validation L:0.7693383901554278\n",
      "Epoch 2190, mean: 0.6948227286338806, std: 0.46048298478126526\n",
      "training L: 0.7684483377140253\n",
      "validation L:0.7690780272296834\n",
      "Epoch 2191, mean: 0.6664896011352539, std: 0.4714675843715668\n",
      "training L: 0.7696184518284969\n",
      "validation L:0.7710402864516432\n",
      "Epoch 2192, mean: 0.6222262382507324, std: 0.48483118414878845\n",
      "training L: 0.7732416995479784\n",
      "validation L:0.772382936596034\n",
      "Epoch 2193, mean: 0.6083601713180542, std: 0.48811742663383484\n",
      "training L: 0.7746943299199124\n",
      "validation L:0.7776374989753811\n",
      "Epoch 2194, mean: 0.6349623203277588, std: 0.48144128918647766\n",
      "training L: 0.7756278399779479\n",
      "validation L:0.7752317703961391\n",
      "Epoch 2195, mean: 0.6797816753387451, std: 0.46656087040901184\n",
      "training L: 0.7727197045298091\n",
      "validation L:0.7722637793546147\n",
      "Epoch 2196, mean: 0.6884546875953674, std: 0.4631255865097046\n",
      "training L: 0.7715630907153328\n",
      "validation L:0.7747421302567276\n",
      "Epoch 2197, mean: 0.6446370482444763, std: 0.4786236882209778\n",
      "training L: 0.7761149342634848\n",
      "validation L:0.778020450962881\n",
      "Epoch 2198, mean: 0.6307709813117981, std: 0.48259657621383667\n",
      "training L: 0.7781943538610854\n",
      "validation L:0.7761984987012779\n",
      "Epoch 2199, mean: 0.6300416588783264, std: 0.4827936291694641\n",
      "training L: 0.7789980616761913\n",
      "validation L:0.7794041275775689\n",
      "Epoch 2200, mean: 0.6497085094451904, std: 0.4770617187023163\n",
      "training L: 0.7798402819471693\n",
      "validation L:0.7806934517378431\n",
      "Epoch 2201, mean: 0.6547799706459045, std: 0.47544050216674805\n",
      "training L: 0.7796615408569773\n",
      "validation L:0.7804130852327941\n",
      "Epoch 2202, mean: 0.6525154709815979, std: 0.4761717617511749\n",
      "training L: 0.7794814819715067\n",
      "validation L:0.7801817038622043\n",
      "Epoch 2203, mean: 0.6395768523216248, std: 0.4801237881183624\n",
      "training L: 0.7800008742604816\n",
      "validation L:0.7792647951505177\n",
      "Epoch 2204, mean: 0.6518672108650208, std: 0.4763789176940918\n",
      "training L: 0.7791424637835918\n",
      "validation L:0.7787153841121606\n",
      "Epoch 2205, mean: 0.626343309879303, std: 0.48377466201782227\n",
      "training L: 0.7787510064222672\n",
      "validation L:0.7768756921806867\n",
      "Epoch 2206, mean: 0.6161981225013733, std: 0.4863111674785614\n",
      "training L: 0.7774336776948566\n",
      "validation L:0.7751181887306515\n",
      "Epoch 2207, mean: 0.6497039794921875, std: 0.4770631492137909\n",
      "training L: 0.7766941836086749\n",
      "validation L:0.7745127931821487\n",
      "Epoch 2208, mean: 0.6664918661117554, std: 0.4714668095111847\n",
      "training L: 0.7750117125381526\n",
      "validation L:0.7749264620042172\n",
      "Epoch 2209, mean: 0.6759594678878784, std: 0.46801578998565674\n",
      "training L: 0.7763021780050616\n",
      "validation L:0.7751243121069005\n",
      "Epoch 2210, mean: 0.6261924505233765, std: 0.48381397128105164\n",
      "training L: 0.7776386974062062\n",
      "validation L:0.7736993898707115\n",
      "Epoch 2211, mean: 0.6166753172874451, std: 0.4861968755722046\n",
      "training L: 0.7783605168800274\n",
      "validation L:0.7769303158767634\n",
      "Epoch 2212, mean: 0.6281125545501709, std: 0.4833091199398041\n",
      "training L: 0.7787375063552505\n",
      "validation L:0.7765646223426013\n",
      "Epoch 2213, mean: 0.645800769329071, std: 0.4782705008983612\n",
      "training L: 0.7780297588792235\n",
      "validation L:0.7734583306256818\n",
      "Epoch 2214, mean: 0.6413123607635498, std: 0.47961583733558655\n",
      "training L: 0.776985461054555\n",
      "validation L:0.7715075984451563\n",
      "Epoch 2215, mean: 0.6622127294540405, std: 0.47295618057250977\n",
      "training L: 0.7753395187973201\n",
      "validation L:0.772849203202077\n",
      "Epoch 2216, mean: 0.6765762567520142, std: 0.4677834212779999\n",
      "training L: 0.7752289505354689\n",
      "validation L:0.7751659127115444\n",
      "Epoch 2217, mean: 0.6538503170013428, std: 0.4757421314716339\n",
      "training L: 0.7782318142236784\n",
      "validation L:0.7744639498160808\n",
      "Epoch 2218, mean: 0.6168756484985352, std: 0.48614877462387085\n",
      "training L: 0.7791666075308832\n",
      "validation L:0.7743251083817202\n",
      "Epoch 2219, mean: 0.6174159049987793, std: 0.486018568277359\n",
      "training L: 0.7801875245256236\n",
      "validation L:0.780240688974893\n",
      "Epoch 2220, mean: 0.6353674530982971, std: 0.48132750391960144\n",
      "training L: 0.78114445722464\n",
      "validation L:0.7804827323533583\n",
      "Epoch 2221, mean: 0.6619133353233337, std: 0.4730587303638458\n",
      "training L: 0.7809030238509478\n",
      "validation L:0.7816326128836129\n",
      "Epoch 2222, mean: 0.6446415185928345, std: 0.47862234711647034\n",
      "training L: 0.7815713033437446\n",
      "validation L:0.7811208541633021\n",
      "Epoch 2223, mean: 0.6447045803070068, std: 0.4786032736301422\n",
      "training L: 0.7821547574497877\n",
      "validation L:0.7809588195631161\n",
      "Epoch 2224, mean: 0.6477344036102295, std: 0.47767674922943115\n",
      "training L: 0.7825997777023581\n",
      "validation L:0.7815920202343294\n",
      "Epoch 2225, mean: 0.6486055254936218, std: 0.47740647196769714\n",
      "training L: 0.7831768681057121\n",
      "validation L:0.782440384429941\n",
      "Epoch 2226, mean: 0.6386809349060059, std: 0.4803833067417145\n",
      "training L: 0.7837209083624292\n",
      "validation L:0.7825450837103862\n",
      "Epoch 2227, mean: 0.6429105401039124, std: 0.4791420102119446\n",
      "training L: 0.7843708611369067\n",
      "validation L:0.7830208959609294\n",
      "Epoch 2228, mean: 0.6446505188941956, std: 0.47861963510513306\n",
      "training L: 0.7842037187628068\n",
      "validation L:0.7838171468320114\n",
      "Epoch 2229, mean: 0.6429128050804138, std: 0.47914138436317444\n",
      "training L: 0.7847632490097095\n",
      "validation L:0.7842015247061517\n",
      "Epoch 2230, mean: 0.637607216835022, std: 0.4806919991970062\n",
      "training L: 0.7850749236247907\n",
      "validation L:0.7838726027615734\n",
      "Epoch 2231, mean: 0.6356511116027832, std: 0.4812476634979248\n",
      "training L: 0.7854993456328037\n",
      "validation L:0.7832394552208211\n",
      "Epoch 2232, mean: 0.6462442278862, std: 0.47813504934310913\n",
      "training L: 0.7862813369017068\n",
      "validation L:0.7833389806293526\n",
      "Epoch 2233, mean: 0.6452763080596924, std: 0.47843003273010254\n",
      "training L: 0.786042741027701\n",
      "validation L:0.7854511911565338\n",
      "Epoch 2234, mean: 0.6529161334037781, std: 0.4760432541370392\n",
      "training L: 0.7860437132089366\n",
      "validation L:0.7852926464409232\n",
      "Epoch 2235, mean: 0.6373370885848999, std: 0.48076918721199036\n",
      "training L: 0.786625881144098\n",
      "validation L:0.7843274656521347\n",
      "Epoch 2236, mean: 0.6395093202590942, std: 0.4801434278488159\n",
      "training L: 0.7870555555562704\n",
      "validation L:0.7848431195012866\n",
      "Epoch 2237, mean: 0.6383612751960754, std: 0.4804754853248596\n",
      "training L: 0.787368709579207\n",
      "validation L:0.7852606349338964\n",
      "Epoch 2238, mean: 0.6463027596473694, std: 0.4781171381473541\n",
      "training L: 0.7873435784457126\n",
      "validation L:0.7851231958993178\n",
      "Epoch 2239, mean: 0.6399347186088562, std: 0.48001956939697266\n",
      "training L: 0.7879751656125669\n",
      "validation L:0.7848754473183677\n",
      "Epoch 2240, mean: 0.6502487659454346, std: 0.4768918454647064\n",
      "training L: 0.7878813001457523\n",
      "validation L:0.785781470438291\n",
      "Epoch 2241, mean: 0.6356128454208374, std: 0.48125842213630676\n",
      "training L: 0.7877773606868477\n",
      "validation L:0.7855940219407267\n",
      "Epoch 2242, mean: 0.6402521133422852, std: 0.4799269437789917\n",
      "training L: 0.7882242571747734\n",
      "validation L:0.785461357374226\n",
      "Epoch 2243, mean: 0.654525637626648, std: 0.4755232334136963\n",
      "training L: 0.7886912540329996\n",
      "validation L:0.7860967808733973\n",
      "Epoch 2244, mean: 0.6375644207000732, std: 0.4807042181491852\n",
      "training L: 0.7890621100397184\n",
      "validation L:0.7871152869175504\n",
      "Epoch 2245, mean: 0.6449229121208191, std: 0.4785372316837311\n",
      "training L: 0.788737050279463\n",
      "validation L:0.7867513951424856\n",
      "Epoch 2246, mean: 0.653055727481842, std: 0.4759984016418457\n",
      "training L: 0.7881860904604925\n",
      "validation L:0.7857833469874504\n",
      "Epoch 2247, mean: 0.6355745792388916, std: 0.4812692105770111\n",
      "training L: 0.7877627865795747\n",
      "validation L:0.7852858186698657\n",
      "Epoch 2248, mean: 0.6247720718383789, std: 0.4841822683811188\n",
      "training L: 0.7876584960422169\n",
      "validation L:0.785364437283022\n",
      "Epoch 2249, mean: 0.6566370129585266, std: 0.4748319387435913\n",
      "training L: 0.7865526714174155\n",
      "validation L:0.7856159109380232\n",
      "Epoch 2250, mean: 0.6490377187728882, std: 0.47727170586586\n",
      "training L: 0.786766987112155\n",
      "validation L:0.7867144053398818\n",
      "Epoch 2251, mean: 0.634649395942688, std: 0.4815288782119751\n",
      "training L: 0.7876376765375535\n",
      "validation L:0.7858015604582551\n",
      "Epoch 2252, mean: 0.6381294131278992, std: 0.4805421531200409\n",
      "training L: 0.7876768818608186\n",
      "validation L:0.7845958642424851\n",
      "Epoch 2253, mean: 0.6389735341072083, std: 0.4802987277507782\n",
      "training L: 0.7870352007052874\n",
      "validation L:0.7856195865556854\n",
      "Epoch 2254, mean: 0.636015772819519, std: 0.4811447262763977\n",
      "training L: 0.7876778462907994\n",
      "validation L:0.7851276688241762\n",
      "Epoch 2255, mean: 0.6592347025871277, std: 0.47396713495254517\n",
      "training L: 0.7872325056183463\n",
      "validation L:0.7853818061481392\n",
      "Epoch 2256, mean: 0.6538007855415344, std: 0.4757581651210785\n",
      "training L: 0.7862586434090224\n",
      "validation L:0.7856221641101828\n",
      "Epoch 2257, mean: 0.646827220916748, std: 0.4779563248157501\n",
      "training L: 0.7870657769021036\n",
      "validation L:0.7865960320584151\n",
      "Epoch 2258, mean: 0.6411885023117065, std: 0.4796523451805115\n",
      "training L: 0.7876548124617709\n",
      "validation L:0.7860566481874413\n",
      "Epoch 2259, mean: 0.6518964767456055, std: 0.47636955976486206\n",
      "training L: 0.7877522842139009\n",
      "validation L:0.7864067692913917\n",
      "Epoch 2260, mean: 0.6446505188941956, std: 0.47861960530281067\n",
      "training L: 0.7877852905731273\n",
      "validation L:0.7859114699747689\n",
      "Epoch 2261, mean: 0.6374698877334595, std: 0.4807312786579132\n",
      "training L: 0.7885132914873731\n",
      "validation L:0.7856708991703129\n",
      "Epoch 2262, mean: 0.6354823112487793, std: 0.4812951982021332\n",
      "training L: 0.7884777630752822\n",
      "validation L:0.784706356043376\n",
      "Epoch 2263, mean: 0.6476150751113892, std: 0.4777136445045471\n",
      "training L: 0.7886035536184604\n",
      "validation L:0.7862789561687162\n",
      "Epoch 2264, mean: 0.6531659960746765, std: 0.47596287727355957\n",
      "training L: 0.7891089683329988\n",
      "validation L:0.7868290820218\n",
      "Epoch 2265, mean: 0.643561065196991, std: 0.4789475202560425\n",
      "training L: 0.788755422923404\n",
      "validation L:0.787594340331023\n",
      "Epoch 2266, mean: 0.6258570551872253, std: 0.48390135169029236\n",
      "training L: 0.7891646530870375\n",
      "validation L:0.7861294914510109\n",
      "Epoch 2267, mean: 0.6324096918106079, std: 0.48214957118034363\n",
      "training L: 0.7897936537658543\n",
      "validation L:0.7873319465006818\n",
      "Epoch 2268, mean: 0.6422082185745239, std: 0.4793509542942047\n",
      "training L: 0.7898524682872337\n",
      "validation L:0.787820071346591\n",
      "Epoch 2269, mean: 0.6548069715499878, std: 0.4754317104816437\n",
      "training L: 0.7894433245916005\n",
      "validation L:0.7886039456825937\n",
      "Epoch 2270, mean: 0.6363263726234436, std: 0.4810567796230316\n",
      "training L: 0.7904814856313764\n",
      "validation L:0.787927445904283\n",
      "Epoch 2271, mean: 0.6405515074729919, std: 0.4798393249511719\n",
      "training L: 0.7904003653384253\n",
      "validation L:0.7887837856186538\n",
      "Epoch 2272, mean: 0.6428182125091553, std: 0.4791695773601532\n",
      "training L: 0.790513523390554\n",
      "validation L:0.7875428228878611\n",
      "Epoch 2273, mean: 0.6246055364608765, std: 0.48422515392303467\n",
      "training L: 0.7911812993983688\n",
      "validation L:0.7873892967582525\n",
      "Epoch 2274, mean: 0.6448711156845093, std: 0.478552907705307\n",
      "training L: 0.7909532847809713\n",
      "validation L:0.787676736724602\n",
      "Epoch 2275, mean: 0.6369386911392212, std: 0.480882853269577\n",
      "training L: 0.790304214504002\n",
      "validation L:0.7892147637041538\n",
      "Epoch 2276, mean: 0.6388024687767029, std: 0.4803481996059418\n",
      "training L: 0.7911633984067068\n",
      "validation L:0.7890943742210121\n",
      "Epoch 2277, mean: 0.6339133381843567, std: 0.48173409700393677\n",
      "training L: 0.7911643532884512\n",
      "validation L:0.7890114686983982\n",
      "Epoch 2278, mean: 0.6317028999328613, std: 0.4823431074619293\n",
      "training L: 0.7918513341452758\n",
      "validation L:0.7890392202820541\n",
      "Epoch 2279, mean: 0.6343973278999329, std: 0.4815993010997772\n",
      "training L: 0.7918804719981356\n",
      "validation L:0.7899711842441788\n",
      "Epoch 2280, mean: 0.6524839997291565, std: 0.47618183493614197\n",
      "training L: 0.7922389555536418\n",
      "validation L:0.7902055913534268\n",
      "Epoch 2281, mean: 0.6523061394691467, std: 0.4762387275695801\n",
      "training L: 0.791590627855418\n",
      "validation L:0.7898290892915476\n",
      "Epoch 2282, mean: 0.6392751932144165, std: 0.4802113473415375\n",
      "training L: 0.7921613223645985\n",
      "validation L:0.7896096115518116\n",
      "Epoch 2283, mean: 0.6296072006225586, std: 0.4829104244709015\n",
      "training L: 0.7926991877513565\n",
      "validation L:0.7894382638779277\n",
      "Epoch 2284, mean: 0.6412110328674316, std: 0.47964563965797424\n",
      "training L: 0.7919678138939786\n",
      "validation L:0.7895615863846239\n",
      "Epoch 2285, mean: 0.6334001421928406, std: 0.4818764626979828\n",
      "training L: 0.7932207172349568\n",
      "validation L:0.7887845512769204\n",
      "Epoch 2286, mean: 0.6408801674842834, std: 0.47974297404289246\n",
      "training L: 0.792655639214227\n",
      "validation L:0.789391051573828\n",
      "Epoch 2287, mean: 0.6421766877174377, std: 0.47936031222343445\n",
      "training L: 0.7914905206446472\n",
      "validation L:0.7893829177975176\n",
      "Epoch 2288, mean: 0.6421249508857727, std: 0.4793756604194641\n",
      "training L: 0.7928541941354423\n",
      "validation L:0.7893687221231214\n",
      "Epoch 2289, mean: 0.6499853730201721, std: 0.476974755525589\n",
      "training L: 0.7928304639759565\n",
      "validation L:0.7894675898780918\n",
      "Epoch 2290, mean: 0.6420979499816895, std: 0.4793836772441864\n",
      "training L: 0.7923883975872349\n",
      "validation L:0.7899344931968519\n",
      "Epoch 2291, mean: 0.6418638229370117, std: 0.4794529974460602\n",
      "training L: 0.7934819210225033\n",
      "validation L:0.7905321408383232\n",
      "Epoch 2292, mean: 0.6417985558509827, std: 0.479472279548645\n",
      "training L: 0.7922468135774\n",
      "validation L:0.7896252426091841\n",
      "Epoch 2293, mean: 0.6836983561515808, std: 0.465032696723938\n",
      "training L: 0.7854374829444085\n",
      "validation L:0.7751955491850215\n",
      "Epoch 2294, mean: 0.6395227909088135, std: 0.48013946413993835\n",
      "training L: 0.7766626348849719\n",
      "validation L:0.7676280327131542\n",
      "Epoch 2295, mean: 0.5714035034179688, std: 0.49487581849098206\n",
      "training L: 0.778314773749935\n",
      "validation L:0.7691471455092356\n",
      "Epoch 2296, mean: 0.6136409640312195, std: 0.48691505193710327\n",
      "training L: 0.777707031134251\n",
      "validation L:0.7725015615668595\n",
      "Epoch 2297, mean: 0.6963781714439392, std: 0.45982182025909424\n",
      "training L: 0.771565949940776\n",
      "validation L:0.7606007263801934\n",
      "Epoch 2298, mean: 0.7099673748016357, std: 0.45377764105796814\n",
      "training L: 0.7676491446316966\n",
      "validation L:0.7691912588759893\n",
      "Epoch 2299, mean: 0.646379292011261, std: 0.4780937433242798\n",
      "training L: 0.7728603198570025\n",
      "validation L:0.7629399219161308\n",
      "Epoch 2300, mean: 0.6135464310646057, std: 0.4869370758533478\n",
      "training L: 0.7745763148036395\n",
      "validation L:0.7667084928537677\n",
      "Epoch 2301, mean: 0.6282948851585388, std: 0.48326075077056885\n",
      "training L: 0.777653527307131\n",
      "validation L:0.7762208205008903\n",
      "Epoch 2302, mean: 0.6405267119407654, std: 0.4798465669155121\n",
      "training L: 0.7788991127290573\n",
      "validation L:0.7772455543168761\n",
      "Epoch 2303, mean: 0.6460123658180237, std: 0.47820591926574707\n",
      "training L: 0.7797079879131883\n",
      "validation L:0.7783164906363707\n",
      "Epoch 2304, mean: 0.6693055629730225, std: 0.47046372294425964\n",
      "training L: 0.7783997942667964\n",
      "validation L:0.777644612398278\n",
      "Epoch 2305, mean: 0.6570062041282654, std: 0.47470998764038086\n",
      "training L: 0.7770498832764158\n",
      "validation L:0.7767967842628474\n",
      "Epoch 2306, mean: 0.6366775631904602, std: 0.48095712065696716\n",
      "training L: 0.7798342450591345\n",
      "validation L:0.7715083880232614\n",
      "Epoch 2307, mean: 0.625956118106842, std: 0.483875572681427\n",
      "training L: 0.7784508422724778\n",
      "validation L:0.7707238163206611\n",
      "Epoch 2308, mean: 0.66010582447052, std: 0.473673552274704\n",
      "training L: 0.7783169595422847\n",
      "validation L:0.7733299832615809\n",
      "Epoch 2309, mean: 0.6884209513664246, std: 0.4631393253803253\n",
      "training L: 0.7746959905281667\n",
      "validation L:0.7710808709001714\n",
      "Epoch 2310, mean: 0.6812403202056885, std: 0.4659961760044098\n",
      "training L: 0.7743827853475798\n",
      "validation L:0.7764181723271641\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m train_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(training_dataset, config[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m valid_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(validation_dataset, config[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \u001b[39m\"\u001b[39;49m\u001b[39maversion_and_direction_larg_batch\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m2000\u001b[39;49m)\n\u001b[1;32m     27\u001b[0m run_obj\u001b[39m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m, in \u001b[0;36mtrain_model_with_for_direction\u001b[0;34m(model, config, train_data, valid_data, wandb, model_name, start)\u001b[0m\n\u001b[1;32m     35\u001b[0m prediction_std \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     36\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfor\u001b[39;00m _, (X, [Y, Y_vel, Y_dir]) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_data):\n\u001b[1;32m     38\u001b[0m     train_batch_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     39\u001b[0m     X, Y, Y_vel, Y_dir\u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), Y\u001b[39m.\u001b[39mto(device), Y_vel\u001b[39m.\u001b[39mto(device), Y_dir\u001b[39m.\u001b[39mto(device)   \n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/Gaze_project/Dataset_Util/dataloader.py:270\u001b[0m, in \u001b[0;36mAversion_and_Directions_SelfTap111.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m# output_target\u001b[39;00m\n\u001b[1;32m    269\u001b[0m output_target \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(aversion_label_path)\n\u001b[0;32m--> 270\u001b[0m output_target_2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(aversion_direction_path)\n\u001b[1;32m    271\u001b[0m \u001b[39m# see if we need to concat any thing\u001b[39;00m\n\u001b[1;32m    272\u001b[0m input_audio_on_screen \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(onscreen_audio_feature_path)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/npyio.py:438\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n\u001b[0;32m--> 438\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot load file containing pickled data \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mwhen allow_pickle=False\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m         \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39mload(fid, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/aversion_and_direction_larg_batch/config.json\", \"r\"))\n",
    "model = SentenceBaseline_Gaze_and_Direction_PredictionModel(config)\n",
    "config[\"load_model\"] = True\n",
    "if config[\"wandb\"]:\n",
    "    wandb.login()\n",
    "    if config[\"load_model\"]:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, save_code=True,\n",
    "            resume='allow', id='vgl6tnx8')\n",
    "        # checkpoint_name = \"gaze_prediction_team/gaze_prediction/8w9fyxan\"\n",
    "        checkpoint_path = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/aversion_and_direction_larg_batch/time=2023-04-18 09:01:11.503968_epoch=1986.pt\"\n",
    "        wandb.restore(checkpoint_path)\n",
    "        pretrained_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_weights(pretrained_dict)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "else:\n",
    "    run_obj = None\n",
    "\n",
    "training_dataset = Aversion_and_Directions_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "validation_dataset = Aversion_and_Directions_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "\n",
    "train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \"aversion_and_direction_larg_batch\", 2000)\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for sentence + word + audio. but also uses velocity as a loss function, also predict gaze Direction but only up and down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevanpan\u001b[0m (\u001b[33mgaze_prediction_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/artemis.dgpsrv/ondemand27/evanpan/Gaze_project/training/wandb/run-20230418_154651-hhcg5110</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/hhcg5110' target=\"_blank\">stilted-night-108</a></strong> to <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/hhcg5110' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/hhcg5110</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1, mean: 1.0, std: 0.0\n",
      "training L: 0.46306652828977884\n",
      "validation L:0.49493222482845023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[39m# training_dataset[0]\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \u001b[39m\"\u001b[39;49m\u001b[39maversion_and_direction_larg_batch_simple_dir\u001b[39;49m\u001b[39m\"\u001b[39;49m, num_of_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m run_obj\u001b[39m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[11], line 81\u001b[0m, in \u001b[0;36mtrain_model_with_for_direction\u001b[0;34m(model, config, train_data, valid_data, wandb, model_name, start, num_of_classes)\u001b[0m\n\u001b[1;32m     78\u001b[0m total_train_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_data)\n\u001b[1;32m     79\u001b[0m total_aversion_predicted \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m total_prediction_counter\n\u001b[0;32m---> 81\u001b[0m \u001b[39mfor\u001b[39;00m _, (X, [Y, Y_vel, Y_dir]) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(valid_data):\n\u001b[1;32m     82\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     83\u001b[0m         valid_batch_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/Gaze_project/Dataset_Util/dataloader.py:277\u001b[0m, in \u001b[0;36mAversion_and_Directions_SelfTap111.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m# see if we need to concat any thing\u001b[39;00m\n\u001b[1;32m    276\u001b[0m input_audio_on_screen \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(onscreen_audio_feature_path)\n\u001b[0;32m--> 277\u001b[0m input_audio_off_screen \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(offscreen_audio_feature_path)\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_only:\n\u001b[1;32m    279\u001b[0m     missing_frames \u001b[39m=\u001b[39m output_target\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m input_audio_on_screen\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mopen_memmap(file, mode\u001b[39m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(fid, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[1;32m    433\u001b[0m                                  pickle_kwargs\u001b[39m=\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    434\u001b[0m                                  max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/format.py:790\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    788\u001b[0m     \u001b[39mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m    789\u001b[0m         \u001b[39m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49mfromfile(fp, dtype\u001b[39m=\u001b[39;49mdtype, count\u001b[39m=\u001b[39;49mcount)\n\u001b[1;32m    791\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    792\u001b[0m         \u001b[39m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[1;32m    793\u001b[0m         \u001b[39m# memory-intensive way.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[39m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[1;32m    802\u001b[0m         \u001b[39m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m    803\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mndarray(count, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_obj = None\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config2.json\", \"r\"))\n",
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# config[\"wandb\"]=False\n",
    "print(device)\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_and_Directions_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True, simple_dir=True)\n",
    "validation_dataset = Aversion_and_Directions_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True, simple_dir=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SentenceBaseline_Gaze_and_Direction_PredictionModel_only_updown(config)\n",
    "model.to(device)\n",
    "# training_dataset[0]\n",
    "train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \"aversion_and_direction_larg_batch_simple_dir\", num_of_classes=3)\n",
    "run_obj.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/aversion_and_direction_larg_batch_simple_dir/config.json\", \"r\"))\n",
    "model = SentenceBaseline_Gaze_and_Direction_PredictionModel_only_updown(config)\n",
    "config[\"load_model\"] = True\n",
    "if config[\"wandb\"]:\n",
    "    wandb.login()\n",
    "    if config[\"load_model\"]:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, save_code=True,\n",
    "            resume='allow', id='l9vqiqwb')\n",
    "        # checkpoint_name = \"gaze_prediction_team/gaze_prediction/8w9fyxan\"\n",
    "        checkpoint_path = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/aversion_and_direction_larg_batch_simple_dir/time=2023-04-19 08:28:15.969363_epoch=973.pt\"\n",
    "        wandb.restore(checkpoint_path)\n",
    "        pretrained_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_weights(pretrained_dict)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "else:\n",
    "    run_obj = None\n",
    "\n",
    "training_dataset = Aversion_and_Directions_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "validation_dataset = Aversion_and_Directions_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "\n",
    "train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \"aversion_and_direction_larg_batch\", 2000)\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for concatenating input so the model is trained on longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_concat(model, config, train_data, valid_data, wandb, model_name):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train() \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    training_loss = []\n",
    "    valid_loss = []\n",
    "    training_f1 = []\n",
    "    valid_f1 = []\n",
    "    aversion_vs_start = []\n",
    "    count = 0\n",
    "    # f1_score = BinaryF1Score(num_classes=2).to(device)\n",
    "    f1_score = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\").to(device)\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "        total_aversion_predicted = 0\n",
    "        total_train_f1 = 0\n",
    "        total_valid_f1 = 0\n",
    "        train_batch_counter = 0\n",
    "        valid_batch_counter = 0\n",
    "        total_prediction_counter = 0\n",
    "        prediction_mean = 0\n",
    "        prediction_std = 0\n",
    "        model.zero_grad()\n",
    "        for _, (X, Y) in enumerate(train_data):\n",
    "            train_batch_counter += 1\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            if \"Transformer\" in config[\"model_type\"]:\n",
    "                all_zero = torch.zeros(Y.shape).to(device)\n",
    "                pred = model(X, all_zero)\n",
    "            else:\n",
    "                pred = model(X)\n",
    "            loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_train_loss += loss.item()\n",
    "            # binary_pred = torch.round(pred)\n",
    "            binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "            prediction_mean = torch.mean(binary_pred.float()).item()\n",
    "            prediction_std = torch.std(binary_pred.float()).item()            \n",
    "            f1_train = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "            total_aversion_predicted += torch.sum(binary_pred).item()\n",
    "            total_prediction_counter += binary_pred.size()[0] * binary_pred.size()[1] \n",
    "            total_train_f1 += f1_train\n",
    "            del X, Y, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        total_train_f1 /= train_batch_counter\n",
    "        total_train_loss /= len(train_data)\n",
    "        total_aversion_predicted /= total_prediction_counter\n",
    "\n",
    "        for _, (X, Y) in enumerate(valid_data):\n",
    "            with torch.no_grad():\n",
    "                valid_batch_counter += 1\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                if \"Transformer\" in config[\"model_type\"]:\n",
    "                    all_zero = torch.zeros(Y.shape).to(device)\n",
    "                    pred = model(X, all_zero)\n",
    "                else:\n",
    "                    pred = model(X)\n",
    "                loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "                # binary_pred = torch.round(pred)\n",
    "                binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "                f1_valid = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "                total_valid_f1 += f1_valid\n",
    "                del X, Y, pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        total_valid_f1 /= valid_batch_counter\n",
    "        total_valid_loss /= len(valid_data)\n",
    "\n",
    "        if config['wandb']:\n",
    "            wandb.log({'training loss': total_train_loss,\n",
    "                        'validation_loss': total_valid_loss,\n",
    "                        'training_f1': total_train_f1,\n",
    "                        'validation_f1': total_valid_f1, \n",
    "                        \"percentage_predicted_aversion\": total_aversion_predicted})\n",
    "        training_loss.append(total_train_loss)\n",
    "        valid_loss.append(total_valid_loss)\n",
    "        training_f1.append(total_train_f1)\n",
    "        valid_f1.append(total_valid_f1)\n",
    "        aversion_vs_start.append(total_aversion_predicted)\n",
    "        if total_valid_f1 == max(valid_f1):\n",
    "            try:\n",
    "                os.mkdir(os.path.join(*[model_save_location, model_name]))\n",
    "            except:\n",
    "                pass\n",
    "            config_save_path = os.path.join(*[model_save_location, model_name, \"config.json\"])\n",
    "            json.dump(config, open(config_save_path, \"w\"))\n",
    "            file_name = f'time={datetime.now()}_epoch={epoch}.pt'\n",
    "            save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        if config['early_stopping']>0:\n",
    "            if epoch > 1:\n",
    "                if total_valid_f1 < np.mean(valid_f1[epoch - 7:epoch - 2]):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "            if count >= config['early_stopping']:\n",
    "                print('\\n\\nStopping early due to decrease in performance on validation set\\n\\n')\n",
    "                break \n",
    "        if count == 0:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1))\n",
    "        else:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}, model have not improved for {} iterations\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1, count))\n",
    "    if config['wandb']:\n",
    "        save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "        wandb.save(save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the output of the model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\EVANSA~1\\AppData\\Local\\Temp/ipykernel_22060/546663576.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# obtain the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_default_tensor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDoubleTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvalidation_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAversion_SelfTap111\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_timing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
>>>>>>> d44adedfb4841131eb4dc3b1931b41658bb72d64
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\", \"r\"))\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
<<<<<<< HEAD
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, 2000, True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, 2000, True)\n",
=======
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, word_timing=True)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, word_timing=True)\n",
    "\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
>>>>>>> d44adedfb4841131eb4dc3b1931b41658bb72d64
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "checkpoint_path = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/sentence_and_words_with_vel/time=2023-04-11 04:27:23.917409_epoch=489.pt\"\n",
    "pretrained_dict = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(pretrained_dict)\n",
    "# train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 60,
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\EVANSA~1\\AppData\\Local\\Temp/ipykernel_22060/1241696724.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'validation_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
>>>>>>> d44adedfb4841131eb4dc3b1931b41658bb72d64
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "print(config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m prediction \u001b[39m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m label \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 7\u001b[0m \u001b[39mfor\u001b[39;00m _, (X, Y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      8\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      9\u001b[0m         valid_batch_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    204\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:171\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m collate([torch\u001b[39m.\u001b[39;49mas_tensor(b) \u001b[39mfor\u001b[39;49;00m b \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_batch_counter = 0\n",
    "f1_score = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\").to(device)\n",
    "total_valid_f1 = 0\n",
    "model.to(device=device)\n",
    "prediction = []\n",
    "label = []\n",
    "for _, (X, Y) in enumerate(train_dataloader):\n",
    "    with torch.no_grad():\n",
    "        valid_batch_counter += 1\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        if \"Transformer\" in config[\"model_type\"]:\n",
    "            all_zero = torch.zeros(Y.shape).to(device)\n",
    "            pred = model(X, all_zero)\n",
    "        else:\n",
    "            pred = model(X)\n",
    "        # binary_pred = torch.round(pred)\n",
    "        binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "        prediction.append(binary_pred.cpu().numpy().flatten())\n",
    "        label.append(Y.cpu().numpy().flatten())\n",
    "        \n",
    "        f1_valid = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "        total_valid_f1 += f1_valid\n",
    "        del X, Y, pred\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction_arr = np.concatenate(prediction)\n",
    "train_label_arr = np.concatenate(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batch_counter = 0\n",
    "f1_score = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\").to(device)\n",
    "total_valid_f1 = 0\n",
    "model.to(device=device)\n",
    "prediction = []\n",
    "label = []\n",
    "for _, (X, Y) in enumerate(valid_dataloader):\n",
    "    with torch.no_grad():\n",
    "        valid_batch_counter += 1\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        if \"Transformer\" in config[\"model_type\"]:\n",
    "            all_zero = torch.zeros(Y.shape).to(device)\n",
    "            pred = model(X, all_zero)\n",
    "        else:\n",
    "            pred = model(X)\n",
    "        # binary_pred = torch.round(pred)\n",
    "        binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "        prediction.append(binary_pred.cpu().numpy().flatten())\n",
    "        label.append(Y.cpu().numpy().flatten())\n",
    "        \n",
    "        f1_valid = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "        total_valid_f1 += f1_valid\n",
    "        del X, Y, pred\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prediction_arr = np.concatenate(prediction)\n",
    "valid_label_arr = np.concatenate(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85913473 0.96337538]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(label_arr, prediction_arr)\n",
    "\n",
    "# Calculate accuracy for each category\n",
    "accuracy = np.diag(cm) / cm.sum(axis=1)\n",
    "\n",
    "# Define class labels\n",
    "classes = ['Aversion', 'Direct Gaze']\n",
    "# Calculate accuracy for each category\n",
    "accuracy = np.diag(cm) / cm.sum(axis=1)\n",
    "normalized_accuracy = np.diag(cm) / cm.sum(axis=1)\n",
    "print(normalized_accuracy)\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "ax.set(xticks=np.arange(cm.shape[1]),\n",
    "       yticks=np.arange(cm.shape[0]),\n",
    "       xticklabels=classes, yticklabels=classes,\n",
    "       title='Validation set Confusion Matrix',\n",
    "       xlabel='Predicted label',\n",
    "       ylabel='True label')\n",
    "\n",
    "# Add values and accuracy to the confusion matrix heatmap\n",
    "thresh = cm.max() / 2.0\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        if i == j:\n",
    "            ax.text(j, i, f'{cm[i, j]}\\n({normalized_accuracy[i]*100:.2f}%)',\n",
    "                    ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black')\n",
    "        else:\n",
    "            ax.text(j, i, f'{cm[i, j]}\\n({(1 - normalized_accuracy[i])*100:.2f}%)',\n",
    "                ha='center', va='center',\n",
    "                color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "# Ensure correct aspect ratio and display the plot\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m11\u001b[39m\n\u001b[1;32m      3\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(validation_dataset[i][\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "i = 11\n",
    "x = torch.from_numpy(validation_dataset[i][0]).to(device)\n",
    "y = validation_dataset[i][1]\n",
    "pred = model(torch.unsqueeze(x, axis=0))\n",
    "pred = torch.softmax(pred, dim=2)\n",
    "pred = pred.cpu().detach().numpy()\n",
    "plt.plot(pred[0, :, 0], label=\"prediction\")\n",
    "plt.plot(y, label=\"label\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaligaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
