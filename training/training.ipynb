{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from torchmetrics.classification import BinaryF1Score, F1Score\n",
    "import wandb\n",
    "from tqdm import trange\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '/Users/evanpan/Documents/GitHub/EvansToolBox/Utils')\n",
    "sys.path.insert(0, '/Users/evanpan/Desktop/openpose/python/')\n",
    "sys.path.insert(0, '/scratch/ondemand27/evanpan/EvansToolBox/Utils/')\n",
    "sys.path.insert(0, '/scratch/ondemand27/evanpan/Gaze_project/')\n",
    "\n",
    "# from training.model import *\n",
    "from Dataset_Util.dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%aimport training.model\n",
    "%aimport Dataset_Util.dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaded Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = \"/scratch/ondemand27/evanpan/data/deep_learning_processed_dataset\"\n",
    "model_save_location = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models\"\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/baseline_config.json\", \"r\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the training test split here:\n",
    "dataset_metadata = \"/scratch/ondemand27/evanpan/data/deep_learning_processed_dataset/video_to_window_metadata.json\"\n",
    "dataset_metadata = json.load(open(dataset_metadata, \"r\"))\n",
    "all_videos = list(dataset_metadata.keys())\n",
    "training_set = []\n",
    "testing_set = []\n",
    "# get the name of the videos (this ensures no contamination because the same shot is split)\n",
    "for i in range(0, len(all_videos)):\n",
    "    if i / len(all_videos) < 0.9:\n",
    "        training_set.append(all_videos[i])\n",
    "    else:\n",
    "        testing_set.append(all_videos[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio_only_GazePredictionModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(Audio_only_GazePredictionModel, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * (config[\"input_layer_out\"]) * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)        \n",
    "        # output layers\n",
    "        self.output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3)\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        \n",
    "        text_feature_self = mod_audio_self[:, :, :6]\n",
    "        mod_audio_self = mod_audio_self[:, :, 6:]\n",
    "        text_feature_other = mod_audio_self[:, :, :6]\n",
    "        mod_audio_other = mod_audio_other[:, :, 6:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, x2_windowed], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "        return x\n",
    "    def load_weights(self, pretrained_dict):\n",
    "    #   not_copy = set(['fc.weight', 'fc.bias'])\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBaseline_GazePredictionModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(SentenceBaseline_GazePredictionModel, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * (config[\"input_layer_out\"] + 6) * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)        \n",
    "        # output layers\n",
    "        self.output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3)\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        \n",
    "        text_feature_self = mod_audio_self[:, :, :6]\n",
    "        mod_audio_self = mod_audio_self[:, :, 6:]\n",
    "        text_feature_other = mod_audio_self[:, :, :6]\n",
    "        mod_audio_other = mod_audio_other[:, :, 6:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, text_feature_self, x2_windowed, text_feature_other], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "        return x\n",
    "    def load_weights(self, pretrained_dict):\n",
    "    #   not_copy = set(['fc.weight', 'fc.bias'])\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBaseline_GazePredictionModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(SimpleBaseline_GazePredictionModel, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * config[\"input_layer_out\"] * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)\n",
    "        self.lstm = nn.LSTM(2 * config[\"input_layer_out\"] * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)\n",
    "        \n",
    "        # output layers\n",
    "        self.output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3, nn.Sigmoid())\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, x2_windowed], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBaselineTransformer_GazePredictionModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(SimpleBaselineTransformer_GazePredictionModel, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * config[\"input_layer_out\"] * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)\n",
    "        self.trasnformer = nn.Transformer(2 * config[\"input_layer_out\"] * (self.frames_ahead + self.frames_behind + 1), batch_first=True,)\n",
    "        \n",
    "        # output layers\n",
    "        self.output_layer_1 = nn.Linear(2 * config[\"input_layer_out\"] * (self.frames_ahead + self.frames_behind + 1), config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3, nn.Sigmoid())\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, x2_windowed], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBaseline_Gaze_and_Direction_PredictionModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(SentenceBaseline_Gaze_and_Direction_PredictionModel, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * (config[\"input_layer_out\"] + 6) * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)        \n",
    "        # aversion output layers\n",
    "        self.output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3)\n",
    "\n",
    "        # directional output layers\n",
    "        self.directional_output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.directional_output_layer_1 = nn.Sequential(self.directional_output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.directional_output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.directional_output_layer_2 = nn.Sequential(self.directional_output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.directional_output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], 4)\n",
    "        self.directional_output_layer_3 = nn.Sequential(self.directional_output_layer_3)\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        \n",
    "        text_feature_self = mod_audio_self[:, :, :6]\n",
    "        mod_audio_self = mod_audio_self[:, :, 6:]\n",
    "        text_feature_other = mod_audio_self[:, :, :6]\n",
    "        mod_audio_other = mod_audio_other[:, :, 6:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, text_feature_self, x2_windowed, text_feature_other], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "\n",
    "        x_dir = self.activation(out)\n",
    "        x_dir = self.directional_output_layer_1(x_dir)\n",
    "        x_dir = self.directional_output_layer_2(x_dir)\n",
    "        x_dir = self.directional_output_layer_3(x_dir)\n",
    "        return x, x_dir\n",
    "    def load_weights(self, pretrained_dict):\n",
    "    #   not_copy = set(['fc.weight', 'fc.bias'])\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBaseline_Gaze_and_Direction_PredictionModel_only_updown(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(SentenceBaseline_Gaze_and_Direction_PredictionModel_only_updown, self).__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2 - 6), config[\"input_layer_out\"])\n",
    "        \n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * (config[\"input_layer_out\"] + 6) * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)        \n",
    "        # aversion output layers\n",
    "        self.output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], config[\"output_layer_3_hidden\"])\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3)\n",
    "\n",
    "        # directional output layers\n",
    "        self.directional_output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.directional_output_layer_1 = nn.Sequential(self.directional_output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.directional_output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.directional_output_layer_2 = nn.Sequential(self.directional_output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.directional_output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], 3)\n",
    "        self.directional_output_layer_3 = nn.Sequential(self.directional_output_layer_3)\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        \n",
    "        text_feature_self = mod_audio_self[:, :, :6]\n",
    "        mod_audio_self = mod_audio_self[:, :, 6:]\n",
    "        text_feature_other = mod_audio_self[:, :, :6]\n",
    "        mod_audio_other = mod_audio_other[:, :, 6:]\n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self))\n",
    "        x2 = self.activation(self.input_layer_self(mod_audio_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x_combined = torch.concat([x1_windowed, text_feature_self, x2_windowed, text_feature_other], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        out, hidden_state = self.lstm(x_combined)\n",
    "        # bn\n",
    "        # x = self.bn(out.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = self.output_layer_3(x)\n",
    "\n",
    "        x_dir = self.activation(out)\n",
    "        x_dir = self.directional_output_layer_1(x_dir)\n",
    "        x_dir = self.directional_output_layer_2(x_dir)\n",
    "        x_dir = self.directional_output_layer_3(x_dir)\n",
    "        return x, x_dir\n",
    "    def load_weights(self, pretrained_dict):\n",
    "    #   not_copy = set(['fc.weight', 'fc.bias'])\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - For baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, config, train_data, valid_data, wandb, model_name):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train() \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    training_loss = []\n",
    "    valid_loss = []\n",
    "    training_f1 = []\n",
    "    valid_f1 = []\n",
    "    aversion_vs_start = []\n",
    "    count = 0\n",
    "    # f1_score = BinaryF1Score(num_classes=2).to(device)\n",
    "    f1_score = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\").to(device)\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "        total_aversion_predicted = 0\n",
    "        total_train_f1 = 0\n",
    "        total_valid_f1 = 0\n",
    "        train_batch_counter = 0\n",
    "        valid_batch_counter = 0\n",
    "        total_prediction_counter = 0\n",
    "        prediction_mean = 0\n",
    "        prediction_std = 0\n",
    "        model.zero_grad()\n",
    "        for _, (X, Y) in enumerate(train_data):\n",
    "            train_batch_counter += 1\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            if \"Transformer\" in config[\"model_type\"]:\n",
    "                all_zero = torch.zeros(Y.shape).to(device)\n",
    "                pred = model(X, all_zero)\n",
    "            else:\n",
    "                pred = model(X)\n",
    "            loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_train_loss += loss.item()\n",
    "            # binary_pred = torch.round(pred)\n",
    "            binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "            prediction_mean = torch.mean(binary_pred.float()).item()\n",
    "            prediction_std = torch.std(binary_pred.float()).item()            \n",
    "            f1_train = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "            total_aversion_predicted += torch.sum(binary_pred).item()\n",
    "            total_prediction_counter += binary_pred.size()[0] * binary_pred.size()[1] \n",
    "            total_train_f1 += f1_train\n",
    "            del X, Y, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        total_train_f1 /= train_batch_counter\n",
    "        total_train_loss /= len(train_data)\n",
    "        total_aversion_predicted /= total_prediction_counter\n",
    "\n",
    "        for _, (X, Y) in enumerate(valid_data):\n",
    "            with torch.no_grad():\n",
    "                valid_batch_counter += 1\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                if \"Transformer\" in config[\"model_type\"]:\n",
    "                    all_zero = torch.zeros(Y.shape).to(device)\n",
    "                    pred = model(X, all_zero)\n",
    "                else:\n",
    "                    pred = model(X)\n",
    "                loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "                # binary_pred = torch.round(pred)\n",
    "                binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "                f1_valid = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "                total_valid_f1 += f1_valid\n",
    "                del X, Y, pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        total_valid_f1 /= valid_batch_counter\n",
    "        total_valid_loss /= len(valid_data)\n",
    "\n",
    "        if config['wandb']:\n",
    "            wandb.log({'training loss': total_train_loss,\n",
    "                        'validation_loss': total_valid_loss,\n",
    "                        'training_f1': total_train_f1,\n",
    "                        'validation_f1': total_valid_f1, \n",
    "                        \"percentage_predicted_aversion\": total_aversion_predicted})\n",
    "        training_loss.append(total_train_loss)\n",
    "        valid_loss.append(total_valid_loss)\n",
    "        training_f1.append(total_train_f1)\n",
    "        valid_f1.append(total_valid_f1)\n",
    "        aversion_vs_start.append(total_aversion_predicted)\n",
    "        if total_valid_f1 == max(valid_f1):\n",
    "            try:\n",
    "                os.mkdir(os.path.join(*[model_save_location, model_name]))\n",
    "            except:\n",
    "                pass\n",
    "            config_save_path = os.path.join(*[model_save_location, model_name, \"config.json\"])\n",
    "            json.dump(config, open(config_save_path, \"w\"))\n",
    "            file_name = f'time={datetime.now()}_epoch={epoch}.pt'\n",
    "            save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        if config['early_stopping']>0:\n",
    "            if epoch > 1:\n",
    "                if total_valid_f1 < np.mean(valid_f1[epoch - 7:epoch - 2]):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "            if count >= config['early_stopping']:\n",
    "                print('\\n\\nStopping early due to decrease in performance on validation set\\n\\n')\n",
    "                break \n",
    "        if count == 0:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1))\n",
    "        else:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}, model have not improved for {} iterations\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1, count))\n",
    "    if config['wandb']:\n",
    "        save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "        wandb.save(save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight and Biases Stuff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03449014124949654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:02<07:47,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5974612089529724, std: 0.03449014124949654\n",
      "training L: 0.7498041014467824\n",
      "validation L:0.7801158889905458\n",
      "0.0387789817215484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  1%|          | 2/200 [00:04<06:59,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.648003122124935, std: 0.0387789817215484\n",
      "training L: 0.750510350090623\n",
      "validation L:0.7802088892277198\n",
      "0.029644162000120988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 3/200 [00:06<06:41,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6950838980002896, std: 0.029644162000120988\n",
      "training L: 0.7506199214130393\n",
      "validation L:0.7802088892277198\n",
      "0.02540355121551124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 4/200 [00:08<06:31,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6974240211738563, std: 0.02540355121551124\n",
      "training L: 0.7506199214130393\n",
      "validation L:0.7802088892277198\n",
      "0.014475230401421608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 5/200 [00:10<06:36,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6587029522108311, std: 0.014475230401421608\n",
      "training L: 0.7506199214130393\n",
      "validation L:0.7802088892277198\n",
      "0.008366695740317093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 6/200 [00:12<06:28,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6412970440137612, std: 0.008366695740317093\n",
      "training L: 0.7506199214130393\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 6/200 [00:12<06:37,  2.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m valid_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(validation_dataset, config[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m model \u001b[39m=\u001b[39m SimpleBaseline_GazePredictionModel(config)\n\u001b[0;32m----> 9\u001b[0m train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)\n\u001b[1;32m     10\u001b[0m run_obj\u001b[39m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[55], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, config, train_data, valid_data, wandb, model_save_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m prediction_std \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     26\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfor\u001b[39;00m _, (X, Y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_data):\n\u001b[1;32m     28\u001b[0m     train_batch_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     29\u001b[0m     X, Y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), Y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/Gaze_project/Dataset_Util/dataloader.py:184\u001b[0m, in \u001b[0;36mAversion_SelfTap111.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    182\u001b[0m output_target \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(aversion_label_path)\n\u001b[1;32m    183\u001b[0m \u001b[39m# see if we need to concat any thing\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m input_audio_on_screen \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(onscreen_audio_feature_path)\n\u001b[1;32m    185\u001b[0m input_audio_off_screen \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(offscreen_audio_feature_path)\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_only:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mopen_memmap(file, mode\u001b[39m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(fid, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[1;32m    433\u001b[0m                                  pickle_kwargs\u001b[39m=\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    434\u001b[0m                                  max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/format.py:770\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    768\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    769\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 770\u001b[0m     count \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49mmultiply\u001b[39m.\u001b[39;49mreduce(shape, dtype\u001b[39m=\u001b[39;49mnumpy\u001b[39m.\u001b[39;49mint64)\n\u001b[1;32m    772\u001b[0m \u001b[39m# Now read the actual data.\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[39mif\u001b[39;00m dtype\u001b[39m.\u001b[39mhasobject:\n\u001b[1;32m    774\u001b[0m     \u001b[39m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/baseline_config.json\", \"r\"))\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set[:10])\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set[:2])\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SimpleBaseline_GazePredictionModel(config)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)\n",
    "run_obj.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - For Audio Only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:01<05:35,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5375652030339769, std: 0.038422848050369815\n",
      "training L: 0.7323386721605636\n",
      "validation L:0.7280215162212137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/200 [00:03<05:08,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5396894879007264, std: 0.03841229046891618\n",
      "training L: 0.7376692695463082\n",
      "validation L:0.7348674337168585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 3/200 [00:04<04:59,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5418414783457995, std: 0.0382727172988321\n",
      "training L: 0.7430556688251317\n",
      "validation L:0.7396762471093492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 4/200 [00:05<04:43,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5443468853526254, std: 0.03826918974963933\n",
      "training L: 0.7486199438292928\n",
      "validation L:0.7408316961362148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 5/200 [00:07<04:44,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5468758275736018, std: 0.03847715916641862\n",
      "training L: 0.751742524009893\n",
      "validation L:0.7457544486877387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|         | 6/200 [00:08<04:34,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5487699903925971, std: 0.03846862209941646\n",
      "training L: 0.7545170146204431\n",
      "validation L:0.7470093760103459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 7/200 [00:10<04:38,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5512063178953902, std: 0.03835452302643252\n",
      "training L: 0.7570841856556142\n",
      "validation L:0.7537631811961684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 8/200 [00:11<04:39,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5532094666883071, std: 0.038496512912125684\n",
      "training L: 0.7617156097791649\n",
      "validation L:0.7549647661755285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 9/200 [00:13<04:39,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5554321769720268, std: 0.03805370844533758\n",
      "training L: 0.7656895903795565\n",
      "validation L:0.7606082318286761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 10/200 [00:14<04:38,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5574337472479528, std: 0.03852652032566119\n",
      "training L: 0.7645938095386826\n",
      "validation L:0.7613699280234122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 11/200 [00:16<04:29,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.559981626833095, std: 0.038347647409568156\n",
      "training L: 0.7698066319794983\n",
      "validation L:0.7640094711917916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 12/200 [00:17<04:30,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5622004870390143, std: 0.038257843264804414\n",
      "training L: 0.7722952821004156\n",
      "validation L:0.7646226415094339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 13/200 [00:18<04:22,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5640483085938395, std: 0.038382897538683555\n",
      "training L: 0.7718415896573535\n",
      "validation L:0.7659007136695161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 14/200 [00:20<04:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5665515094659646, std: 0.03814504706038931\n",
      "training L: 0.7751711855559309\n",
      "validation L:0.7667606516290727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 15/200 [00:21<04:12,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5685410694377985, std: 0.03836502495453606\n",
      "training L: 0.7761418131118908\n",
      "validation L:0.767887763055339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 16/200 [00:22<04:17,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5710678310108791, std: 0.038261168945831855\n",
      "training L: 0.7776878065176539\n",
      "validation L:0.7699121920895174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|         | 17/200 [00:24<04:20,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5729987377715186, std: 0.038263397836928795\n",
      "training L: 0.7794270039622067\n",
      "validation L:0.7713554193498332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|         | 18/200 [00:25<04:22,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.575360365072677, std: 0.03808680008983807\n",
      "training L: 0.7809720723049091\n",
      "validation L:0.7731926776295377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 19/200 [00:27<04:15,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5771138850246609, std: 0.038405858208403906\n",
      "training L: 0.7801478737455783\n",
      "validation L:0.7712205700123915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 20/200 [00:28<04:18,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5790664975047349, std: 0.03818548580533589\n",
      "training L: 0.7813007020576506\n",
      "validation L:0.7748400524165575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 21/200 [00:30<04:19,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5807451319438747, std: 0.038410857081182706\n",
      "training L: 0.7829657011271361\n",
      "validation L:0.775189014041043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 22/200 [00:31<04:11,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.582903799551212, std: 0.03838164564712754\n",
      "training L: 0.782283917014456\n",
      "validation L:0.7741985203452528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 23/200 [00:32<04:05,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5850501231411687, std: 0.03800898681254578\n",
      "training L: 0.7848788953444503\n",
      "validation L:0.7764959237040455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 24/200 [00:34<04:08,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5871901499704449, std: 0.03825333668878838\n",
      "training L: 0.7846295374685186\n",
      "validation L:0.7776071264014744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 25/200 [00:35<04:02,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5890066738191944, std: 0.03832396503706255\n",
      "training L: 0.7850146176798577\n",
      "validation L:0.7762248502534173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|        | 26/200 [00:37<03:57,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5907227826742486, std: 0.03842968105761962\n",
      "training L: 0.7855368728455089\n",
      "validation L:0.7766096232062006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 27/200 [00:38<03:54,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5928269195039755, std: 0.038216674116663824\n",
      "training L: 0.7860214730384049\n",
      "validation L:0.778169014084507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 28/200 [00:39<03:50,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5951770582878034, std: 0.0382402461495092\n",
      "training L: 0.786470234515935\n",
      "validation L:0.7770622508432996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|        | 29/200 [00:40<03:46,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5965046600255317, std: 0.038272955835638604\n",
      "training L: 0.7863828252933311\n",
      "validation L:0.7768582375478927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 30/200 [00:42<03:45,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5982373753965138, std: 0.038192511542916426\n",
      "training L: 0.7866228168971783\n",
      "validation L:0.7777267156862745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 31/200 [00:43<03:52,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5998975201858427, std: 0.03825841699566252\n",
      "training L: 0.7868739309185847\n",
      "validation L:0.7783386874713171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 32/200 [00:45<03:48,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6019111391163703, std: 0.038308990823368284\n",
      "training L: 0.7869959193470956\n",
      "validation L:0.7779735008041664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|        | 33/200 [00:46<03:52,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6036713508981063, std: 0.03826371443377106\n",
      "training L: 0.7872043671920694\n",
      "validation L:0.7781857121003518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|        | 34/200 [00:47<03:48,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6051162892788426, std: 0.03837444523661545\n",
      "training L: 0.7873779564167555\n",
      "validation L:0.7789152024446142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 35/200 [00:49<03:44,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6067495495697391, std: 0.03793514279397345\n",
      "training L: 0.7877943138048349\n",
      "validation L:0.7784742394129338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 36/200 [00:50<03:42,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6085021555963259, std: 0.03827496985733568\n",
      "training L: 0.7881376251273752\n",
      "validation L:0.7788652699189479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|        | 37/200 [00:52<03:48,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6098727395617883, std: 0.038379580377216525\n",
      "training L: 0.7877607288420043\n",
      "validation L:0.7788138184041578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 38/200 [00:53<03:43,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6117281921448884, std: 0.03800839217986361\n",
      "training L: 0.7881538738090959\n",
      "validation L:0.7788219115287646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 39/200 [00:54<03:39,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6132555046716315, std: 0.03813427039511276\n",
      "training L: 0.7882763475161373\n",
      "validation L:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 40/200 [00:56<03:36,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6146646101171072, std: 0.03815154347277368\n",
      "training L: 0.7881655387195304\n",
      "validation L:0.7792108677402122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|        | 41/200 [00:57<03:42,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6157974883200633, std: 0.03834291259328531\n",
      "training L: 0.7884848575577479\n",
      "validation L:0.7794488128864799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|        | 42/200 [00:58<03:37,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6175480401993695, std: 0.0380655845445438\n",
      "training L: 0.78839590443686\n",
      "validation L:0.7795678399633504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 43/200 [01:00<03:33,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6189577184437718, std: 0.03818574687252487\n",
      "training L: 0.7886725239855712\n",
      "validation L:0.7797955135052648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 44/200 [01:01<03:38,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6200836475331568, std: 0.03797256523459504\n",
      "training L: 0.7887201735357917\n",
      "validation L:0.7800747577999847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 45/200 [01:03<03:33,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6214130102648945, std: 0.03796793105375696\n",
      "training L: 0.7886042614316495\n",
      "validation L:0.7794566544566545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|       | 46/200 [01:04<03:30,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.62247247622866, std: 0.03803493969337708\n",
      "training L: 0.7886333368249361\n",
      "validation L:0.7800411993591211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 47/200 [01:05<03:27,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6238084493179571, std: 0.037875842012672904\n",
      "training L: 0.7886422117168332\n",
      "validation L:0.7798809342085178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 48/200 [01:07<03:26,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6248945816924095, std: 0.03827418552441704\n",
      "training L: 0.7889847875188847\n",
      "validation L:0.7793971766501335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 49/200 [01:08<03:23,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6261484635792381, std: 0.03790382461395846\n",
      "training L: 0.7887989708456119\n",
      "validation L:0.7801007172287502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 50/200 [01:09<03:21,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6274820380729639, std: 0.03795289443161719\n",
      "training L: 0.7888533221647172\n",
      "validation L:0.7798368031724243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 51/200 [01:11<03:19,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6287918735569356, std: 0.038010754836282835\n",
      "training L: 0.7889241263762565\n",
      "validation L:0.779769623922496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 52/200 [01:12<03:18,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6298811093429626, std: 0.037993665419687114\n",
      "training L: 0.7888297235850186\n",
      "validation L:0.7800152555301296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 53/200 [01:13<03:16,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6308418167201261, std: 0.03790918813462353\n",
      "training L: 0.7888596386263013\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|       | 54/200 [01:15<03:15,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6317639670979591, std: 0.03772842298266372\n",
      "training L: 0.7888322292174486\n",
      "validation L:0.7796170569837516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 55/200 [01:16<03:14,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6328735757656325, std: 0.03798885606805677\n",
      "training L: 0.7888549892318737\n",
      "validation L:0.7797437461866992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 56/200 [01:17<03:12,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6336673558270467, std: 0.03800663855394427\n",
      "training L: 0.7889274391338159\n",
      "validation L:0.779769623922496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|       | 57/200 [01:19<03:10,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6345197747454334, std: 0.037944321430231465\n",
      "training L: 0.7889455502549761\n",
      "validation L:0.7800823547353973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|       | 58/200 [01:20<03:16,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6357832789447202, std: 0.03807899012996211\n",
      "training L: 0.7889392375910389\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 59/200 [01:21<03:13,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6364019636121019, std: 0.03815005899083669\n",
      "training L: 0.7889935696126813\n",
      "validation L:0.7804133933338419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 60/200 [01:23<03:17,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6374840290487794, std: 0.03800236525374677\n",
      "training L: 0.7889219542103453\n",
      "validation L:0.7803538743136058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 61/200 [01:24<03:12,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.637778766308804, std: 0.0378343596086585\n",
      "training L: 0.7888865627897239\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|       | 62/200 [01:26<03:08,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6388077489404568, std: 0.03789524239582371\n",
      "training L: 0.7890487155716379\n",
      "validation L:0.7797437461866992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 63/200 [01:27<03:06,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6392297369299728, std: 0.037880109618109344\n",
      "training L: 0.7889707971350389\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 64/200 [01:28<03:04,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6401475170824421, std: 0.03775352116254244\n",
      "training L: 0.7890558421170666\n",
      "validation L:0.7798032186713447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|      | 65/200 [01:30<03:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6408517402368207, std: 0.03767392863007117\n",
      "training L: 0.7889589999700948\n",
      "validation L:0.7802943643712347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 66/200 [01:31<03:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6415438765502811, std: 0.0378771443774228\n",
      "training L: 0.7889526885579281\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 67/200 [01:32<02:58,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6418545848710732, std: 0.03785251692680205\n",
      "training L: 0.7888078004426632\n",
      "validation L:0.7796506750057204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 68/200 [01:34<02:56,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6430145007766597, std: 0.03793944414056601\n",
      "training L: 0.7890007027827205\n",
      "validation L:0.779955762336969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|      | 69/200 [01:35<02:54,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6431724290954164, std: 0.03794557136403669\n",
      "training L: 0.7889707971350389\n",
      "validation L:0.7800488102501525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|      | 70/200 [01:36<02:53,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6438113379585376, std: 0.037867618304883076\n",
      "training L: 0.7890377369774535\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 71/200 [01:38<02:52,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6443665282661462, std: 0.03755890204180918\n",
      "training L: 0.7890558421170666\n",
      "validation L:0.7800488102501525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 72/200 [01:39<02:51,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6449561462094501, std: 0.037872248734854085\n",
      "training L: 0.789103033746505\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|      | 73/200 [01:40<02:49,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6453280841816352, std: 0.03770579676780109\n",
      "training L: 0.789079437226201\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|      | 74/200 [01:42<02:47,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6457358736092284, std: 0.03751679354920816\n",
      "training L: 0.7890188103711235\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 75/200 [01:43<02:46,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.646295564651153, std: 0.037746101125036374\n",
      "training L: 0.789050352828609\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 76/200 [01:44<02:45,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6462463095208183, std: 0.03750683690394778\n",
      "training L: 0.7890676394952455\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|      | 77/200 [01:46<02:43,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6468440748456522, std: 0.0377434471441708\n",
      "training L: 0.789055023923445\n",
      "validation L:0.7800823547353973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|      | 78/200 [01:47<02:40,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6470649900498601, std: 0.03791573178697559\n",
      "training L: 0.7891101542877647\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 79/200 [01:48<02:40,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.647273252066047, std: 0.03790480812766759\n",
      "training L: 0.7889290947695804\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 80/200 [01:50<02:38,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6475985561213322, std: 0.03768843110730487\n",
      "training L: 0.788983418310133\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|      | 81/200 [01:51<02:37,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6478119237514677, std: 0.03795452502134771\n",
      "training L: 0.789133749962623\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 82/200 [01:52<02:36,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6477440890117648, std: 0.03784521988385853\n",
      "training L: 0.788947203157942\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 83/200 [01:54<02:35,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6483128562010473, std: 0.037709612182815766\n",
      "training L: 0.7890078344596615\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 84/200 [01:55<02:35,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6486097237679442, std: 0.03754975698081344\n",
      "training L: 0.7890920507721979\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|     | 85/200 [01:56<02:33,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6489740447107436, std: 0.03753976711385344\n",
      "training L: 0.7890676394952455\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|     | 86/200 [01:58<02:32,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6488574469954199, std: 0.03766541645707203\n",
      "training L: 0.7890613318979696\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 87/200 [01:59<02:30,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6489096932871466, std: 0.037728392982625425\n",
      "training L: 0.789103033746505\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 88/200 [02:00<02:29,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.649378472614487, std: 0.037616527532277295\n",
      "training L: 0.7890621495955866\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|     | 89/200 [02:02<02:28,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6495942640500949, std: 0.03741697010708341\n",
      "training L: 0.7890983569794735\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 90/200 [02:03<02:26,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6495154665553903, std: 0.03751850003918505\n",
      "training L: 0.789085744187785\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 91/200 [02:04<02:25,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498210439550074, std: 0.03762015187425466\n",
      "training L: 0.789049534261322\n",
      "validation L:0.7800823547353973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 92/200 [02:06<02:23,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500737481281029, std: 0.03747188069033617\n",
      "training L: 0.7890204520990313\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|     | 93/200 [02:07<02:22,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6497623566221192, std: 0.037333214636729145\n",
      "training L: 0.7889960379756298\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|     | 94/200 [02:08<02:21,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6497072156138498, std: 0.03765919770957377\n",
      "training L: 0.7891211387219281\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 95/200 [02:10<02:20,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498812003882101, std: 0.03746079489707398\n",
      "training L: 0.7889661359049114\n",
      "validation L:0.7799893235720278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 96/200 [02:11<02:19,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499510457127998, std: 0.03757139404685042\n",
      "training L: 0.7890259400463482\n",
      "validation L:0.7801418439716312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|     | 97/200 [02:12<02:17,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6504438735835141, std: 0.03743523975332393\n",
      "training L: 0.789085744187785\n",
      "validation L:0.7804468847708381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|     | 98/200 [02:14<02:16,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.649769088231089, std: 0.037543300462062286\n",
      "training L: 0.7890440450916485\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 99/200 [02:15<02:14,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500536357439018, std: 0.03735566876637967\n",
      "training L: 0.789133749962623\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 100/200 [02:16<02:13,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501871478072954, std: 0.03762670929697387\n",
      "training L: 0.7890314284859903\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 101/200 [02:18<02:12,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501271829508782, std: 0.03714695929733477\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7801418439716312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|     | 102/200 [02:19<02:10,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500918652320735, std: 0.03742554390392043\n",
      "training L: 0.7890322484189752\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 103/200 [02:20<02:09,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501412497418192, std: 0.03739204724230821\n",
      "training L: 0.789122765054117\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 104/200 [02:22<02:08,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6497717523605562, std: 0.03760921137923994\n",
      "training L: 0.7890920507721979\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|    | 105/200 [02:23<02:07,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501580367727385, std: 0.037468959594168946\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|    | 106/200 [02:24<02:05,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498953478366677, std: 0.037546487407533365\n",
      "training L: 0.7890314284859903\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 107/200 [02:26<02:04,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498872552868251, std: 0.037587378927069\n",
      "training L: 0.7890314284859903\n",
      "validation L:0.7801418439716312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|    | 108/200 [02:27<02:03,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500029294532585, std: 0.037289182435219455\n",
      "training L: 0.7891400550173424\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 109/200 [02:28<02:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6502461237072283, std: 0.03755941936064975\n",
      "training L: 0.7891282572620312\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|    | 110/200 [02:30<02:00,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501953252623862, std: 0.037394467274640464\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 111/200 [02:31<01:59,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498664065940516, std: 0.03725298950261908\n",
      "training L: 0.7891463596950217\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 112/200 [02:32<01:57,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499946135774257, std: 0.037446181779797444\n",
      "training L: 0.7890660047253043\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 113/200 [02:34<01:56,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500599020048369, std: 0.037402034475766774\n",
      "training L: 0.7890204520990313\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 114/200 [02:35<01:54,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6504418660385398, std: 0.03726827419105667\n",
      "training L: 0.7890739467153066\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|    | 115/200 [02:36<01:53,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6503700653919925, std: 0.03724308902750212\n",
      "training L: 0.7891880578271465\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 116/200 [02:38<01:52,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501551283243268, std: 0.03723439041383075\n",
      "training L: 0.7890684566969158\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|    | 117/200 [02:39<01:51,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6497153058598419, std: 0.037271555619692566\n",
      "training L: 0.7889960379756298\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|    | 118/200 [02:40<01:49,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6501052327897997, std: 0.03726821926702292\n",
      "training L: 0.789134562198203\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 119/200 [02:42<01:48,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6503252694044814, std: 0.03731639904103009\n",
      "training L: 0.7890613318979696\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 120/200 [02:43<01:46,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500738723572284, std: 0.037279360225059475\n",
      "training L: 0.7890621495955866\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|    | 121/200 [02:44<01:45,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498417167752963, std: 0.037079801938013406\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|    | 122/200 [02:46<01:44,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.650265070274585, std: 0.037034828333994324\n",
      "training L: 0.7890802535581868\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 123/200 [02:47<01:42,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6502467146360357, std: 0.037436068434452505\n",
      "training L: 0.7891164598594708\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 124/200 [02:48<01:41,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6496430102933335, std: 0.03721716628517319\n",
      "training L: 0.7890676394952455\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|   | 125/200 [02:50<01:40,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499345323683191, std: 0.0374692464488565\n",
      "training L: 0.789050352828609\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|   | 126/200 [02:51<01:38,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6495119998868814, std: 0.037422044042390024\n",
      "training L: 0.7891573470486215\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 127/200 [02:52<01:37,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6494766795521983, std: 0.037273700075502667\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 128/200 [02:54<01:39,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498283857833278, std: 0.037200995213504846\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|   | 129/200 [02:55<01:37,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6494224017071586, std: 0.037259201298920665\n",
      "training L: 0.789139243155959\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|   | 130/200 [02:57<01:35,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6491423779067859, std: 0.03721442709299281\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 131/200 [02:58<01:33,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499734358614128, std: 0.036943805382440485\n",
      "training L: 0.7891526639956945\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 132/200 [02:59<01:35,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500218696465765, std: 0.037401049570965614\n",
      "training L: 0.789133749962623\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|   | 133/200 [03:01<01:32,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6494513582998024, std: 0.037263276859249905\n",
      "training L: 0.7890739467153066\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|   | 134/200 [03:02<01:30,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6494726131571438, std: 0.03723299795554167\n",
      "training L: 0.789002347242364\n",
      "validation L:0.779929845966143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 135/200 [03:03<01:28,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6495496750291943, std: 0.03708250920514626\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 136/200 [03:05<01:26,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6494956205671558, std: 0.0371957410045398\n",
      "training L: 0.789085744187785\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|   | 137/200 [03:06<01:24,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.650535865233708, std: 0.037154148136127794\n",
      "training L: 0.7890802535581868\n",
      "validation L:0.7799893235720278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|   | 138/200 [03:07<01:22,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6495115698645602, std: 0.037151209592330675\n",
      "training L: 0.7890322484189752\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 139/200 [03:09<01:21,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6493358151127034, std: 0.03723563590252677\n",
      "training L: 0.7890377369774535\n",
      "validation L:0.7802088892277198\n",
      "mean: 0.6496482683585386, std: 0.03737806580611282\n",
      "training L: 0.7889417182500523\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|   | 141/200 [03:12<01:21,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6503415530453325, std: 0.03719735407667015\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|   | 142/200 [03:13<01:22,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6505178426903855, std: 0.037125989000501965\n",
      "training L: 0.789133749962623\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 143/200 [03:14<01:19,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6491846292292962, std: 0.03756563606990346\n",
      "training L: 0.7890377369774535\n",
      "validation L:0.7800823547353973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 144/200 [03:16<01:17,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6489230551477677, std: 0.03713717678777013\n",
      "training L: 0.7891463596950217\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|  | 145/200 [03:17<01:14,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6500091637951761, std: 0.03723965317236613\n",
      "training L: 0.7890920507721979\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|  | 146/200 [03:18<01:13,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6505028659865627, std: 0.03726119110477237\n",
      "training L: 0.7891282572620312\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 147/200 [03:20<01:11,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6508349413277549, std: 0.03710587107158111\n",
      "training L: 0.7890322484189752\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 148/200 [03:21<01:09,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6497215027685552, std: 0.03727335028403669\n",
      "training L: 0.789050352828609\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|  | 149/200 [03:23<01:11,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6484142505097515, std: 0.03713637206976326\n",
      "training L: 0.7890975420130375\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 150/200 [03:24<01:09,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6485851457429599, std: 0.03712819891392657\n",
      "training L: 0.7891101542877647\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 151/200 [03:25<01:07,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499649553057815, std: 0.037177752815117135\n",
      "training L: 0.7891518531254205\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 152/200 [03:27<01:05,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6509560817307467, std: 0.03726366366839285\n",
      "training L: 0.7891164598594708\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|  | 153/200 [03:28<01:03,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6511630880661888, std: 0.03718854248276555\n",
      "training L: 0.7890739467153066\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|  | 154/200 [03:29<01:02,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499539006502881, std: 0.03738341670309168\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 155/200 [03:31<01:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6503003855274889, std: 0.03720285269586973\n",
      "training L: 0.789050352828609\n",
      "validation L:0.7801418439716312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 156/200 [03:32<00:59,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498567126833039, std: 0.037093566403896376\n",
      "training L: 0.789079437226201\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|  | 157/200 [03:34<01:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6503346970339198, std: 0.037376419242272814\n",
      "training L: 0.7890802535581868\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|  | 158/200 [03:35<00:57,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.651107239786559, std: 0.037211889725529586\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 159/200 [03:36<00:58,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.650475750492941, std: 0.03715005283271281\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 160/200 [03:38<00:55,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6498578179262586, std: 0.037078632658453556\n",
      "training L: 0.7890865600239199\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|  | 161/200 [03:39<00:53,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6516970899085567, std: 0.03730244981036209\n",
      "training L: 0.7890259400463482\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|  | 162/200 [03:40<00:51,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6520823474398048, std: 0.037284116516145244\n",
      "training L: 0.7890747634210881\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 163/200 [03:42<00:50,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6507470263261494, std: 0.03741363351940397\n",
      "training L: 0.789056660188369\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 164/200 [03:43<00:48,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6504946189314396, std: 0.03720300507104151\n",
      "training L: 0.7890621495955866\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%| | 165/200 [03:45<00:48,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6502906632818455, std: 0.03728390134381776\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%| | 166/200 [03:46<00:46,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6499858704657006, std: 0.03723080418488043\n",
      "training L: 0.7890440450916485\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 167/200 [03:47<00:45,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6504895816566616, std: 0.03741216036682273\n",
      "training L: 0.789133749962623\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 168/200 [03:49<00:43,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6512761960166494, std: 0.03729500754168066\n",
      "training L: 0.789134562198203\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%| | 169/200 [03:50<00:41,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6518689578150552, std: 0.037488372087739356\n",
      "training L: 0.7890802535581868\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%| | 170/200 [03:51<00:40,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6519314242298094, std: 0.037489673191777144\n",
      "training L: 0.7891101542877647\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 171/200 [03:53<00:39,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.650693017283545, std: 0.03731097606676058\n",
      "training L: 0.7890920507721979\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 172/200 [03:54<00:37,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6510142397210906, std: 0.03743140358968756\n",
      "training L: 0.789050352828609\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%| | 173/200 [03:55<00:36,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6514361465637869, std: 0.03725046086016204\n",
      "training L: 0.7890684566969158\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%| | 174/200 [03:57<00:34,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6513559322656731, std: 0.03723085487433993\n",
      "training L: 0.7891581575445888\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 175/200 [03:58<00:33,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6509152845170848, std: 0.03743742924000529\n",
      "training L: 0.7891101542877647\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 176/200 [03:59<00:32,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.651684699399912, std: 0.03757594819922277\n",
      "training L: 0.7890802535581868\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%| | 177/200 [04:01<00:30,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6507555980330351, std: 0.03752838806628672\n",
      "training L: 0.7891400550173424\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%| | 178/200 [04:02<00:29,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6511253328075242, std: 0.03743421966639613\n",
      "training L: 0.7891164598594708\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 179/200 [04:03<00:28,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6521646833636132, std: 0.037542516726807904\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 180/200 [04:05<00:27,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6517703879675681, std: 0.03756854569544114\n",
      "training L: 0.7891699557469202\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%| | 181/200 [04:06<00:26,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6502751314481255, std: 0.037648384066966877\n",
      "training L: 0.789110968262748\n",
      "validation L:0.780327868852459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%| | 182/200 [04:08<00:24,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6514952344314373, std: 0.03747867401004681\n",
      "training L: 0.7891164598594708\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 183/200 [04:09<00:23,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6526221874426001, std: 0.037801697534765845\n",
      "training L: 0.7891164598594708\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 184/200 [04:10<00:21,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6525767401123828, std: 0.0376094175963354\n",
      "training L: 0.7890920507721979\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 185/200 [04:12<00:21,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6525164012595259, std: 0.037638741217422444\n",
      "training L: 0.789134562198203\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|| 186/200 [04:13<00:19,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6523958507033527, std: 0.03754099197440361\n",
      "training L: 0.7890684566969158\n",
      "validation L:0.7801158889905458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 187/200 [04:14<00:17,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6523987897332388, std: 0.037702384989835296\n",
      "training L: 0.7891282572620312\n",
      "validation L:0.7800228745711018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 188/200 [04:16<00:16,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6524044044291957, std: 0.03780756563922846\n",
      "training L: 0.7891282572620312\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|| 189/200 [04:17<00:15,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.651952320904239, std: 0.037778805295195735\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|| 190/200 [04:18<00:13,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6520589789247596, std: 0.03766379883222292\n",
      "training L: 0.7891046628096455\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 191/200 [04:20<00:12,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6527499982519376, std: 0.03785650292542167\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 192/200 [04:21<00:10,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6517534958450468, std: 0.037894126614088454\n",
      "training L: 0.7891518531254205\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|| 193/200 [04:23<00:09,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6502472349816975, std: 0.03800010367484725\n",
      "training L: 0.7891518531254205\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|| 194/200 [04:24<00:08,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6513283861060108, std: 0.03792280645361822\n",
      "training L: 0.7891400550173424\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 195/200 [04:25<00:06,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6521683720373683, std: 0.037860077419014454\n",
      "training L: 0.789122765054117\n",
      "validation L:0.7802088892277198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 196/200 [04:27<00:05,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.652412709201666, std: 0.03828000420982498\n",
      "training L: 0.7891219519488092\n",
      "validation L:0.7802683745044221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|| 197/200 [04:28<00:04,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6528608022047981, std: 0.038130725836294684\n",
      "training L: 0.7890975420130375\n",
      "validation L:0.7799893235720278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 198/200 [04:30<00:02,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6532184313266275, std: 0.03817823601465069\n",
      "training L: 0.7891400550173424\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 199/200 [04:31<00:01,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.652308709116172, std: 0.038421576275752184\n",
      "training L: 0.7891518531254205\n",
      "validation L:0.7801753717117804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [04:32<00:00,  1.36s/it]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.6524606512561358, std: 0.038428353510256905\n",
      "training L: 0.7890322484189752\n",
      "validation L:0.7802348635046515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'time=2023-04-04 20:54:46.715070_epoch=196.pt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/audio_only_config.json\", \"r\"))\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set[:12], audio_only=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set[:2], audio_only=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SimpleBaseline_GazePredictionModel(config)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - For sentence level feature Only model (This is actually for word level only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m         X, Y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), Y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m         pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m---> 17\u001b[0m A[\u001b[39m2\u001b[39m]\n\u001b[1;32m     18\u001b[0m train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/word_config.json\", \"r\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, word_timing=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, word_timing=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "model.to(device)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## further train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m SentenceBaseline_GazePredictionModel(config)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/word_config.json\", \"r\"))\n",
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "config[\"learning_rate\"] = 0.00001\n",
    "config[\"load_model\"] = True\n",
    "if config[\"wandb\"]:\n",
    "    wandb.login()\n",
    "    if config[\"load_model\"]:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, save_code=True,\n",
    "            resume='allow', id='8w9fyxan')\n",
    "        # checkpoint_name = \"gaze_prediction_team/gaze_prediction/8w9fyxan\"\n",
    "        checkpoint_path = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/time=2023-04-05 02:37:34.205141_epoch=200.pt\"\n",
    "        wandb.restore(checkpoint_path)\n",
    "        pretrained_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_weights(pretrained_dict)\n",
    "        model.to(config['device'])\n",
    "    else:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "else:\n",
    "    run_obj = None\n",
    "\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, sentence_timing=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, sentence_timing=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop - For sentence level feature Only model (It's real this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/artemis.dgpsrv/ondemand27/evanpan/Gaze_project/training/wandb/run-20230407_195055-2btkatcn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/2btkatcn' target=\"_blank\">pretty-donkey-78</a></strong> to <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/2btkatcn' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/2btkatcn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mean: 0.17789314687252045, std: 0.3824285864830017\n",
      "training L: 0.38361384790041525\n",
      "validation L:0.5202852326801098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, mean: 0.5589313507080078, std: 0.49652254581451416\n",
      "training L: 0.5135592779060695\n",
      "validation L:0.5422346910798534\n",
      "Epoch 3, mean: 0.8715725541114807, std: 0.3345702886581421\n",
      "training L: 0.5083247797239047\n",
      "validation L:0.5124457632512047, model have not improved for 1 iterations\n",
      "Epoch 4, mean: 0.9777710437774658, std: 0.14742977917194366\n",
      "training L: 0.4620488325757937\n",
      "validation L:0.5003760834120156, model have not improved for 2 iterations\n",
      "Epoch 5, mean: 0.9966717958450317, std: 0.05759573355317116\n",
      "training L: 0.45366407573439577\n",
      "validation L:0.49920991627236866\n",
      "Epoch 6, mean: 0.9993893504142761, std: 0.024704914540052414\n",
      "training L: 0.45135263240753554\n",
      "validation L:0.49904111077228025\n",
      "Epoch 7, mean: 0.999908447265625, std: 0.009570656344294548\n",
      "training L: 0.4510088849744354\n",
      "validation L:0.4989816254955779, model have not improved for 1 iterations\n",
      "Epoch 8, mean: 0.9999695420265198, std: 0.005525789689272642\n",
      "training L: 0.4510375310310284\n",
      "validation L:0.49904111077228025, model have not improved for 2 iterations\n",
      "Epoch 9, mean: 0.9999695420265198, std: 0.005525789689272642\n",
      "training L: 0.4510375310310284\n",
      "validation L:0.49904111077228025, model have not improved for 3 iterations\n",
      "Epoch 10, mean: 0.99993896484375, std: 0.007814527489244938\n",
      "training L: 0.45102320827550085\n",
      "validation L:0.49904111077228025, model have not improved for 4 iterations\n",
      "Epoch 11, mean: 0.99993896484375, std: 0.007814527489244938\n",
      "training L: 0.45093922834713546\n",
      "validation L:0.49904111077228025, model have not improved for 5 iterations\n",
      "Epoch 12, mean: 0.9999695420265198, std: 0.005525789689272642\n",
      "training L: 0.4510375310310284\n",
      "validation L:0.49904111077228025\n",
      "Epoch 13, mean: 0.999908447265625, std: 0.009570656344294548\n",
      "training L: 0.4510088849744354\n",
      "validation L:0.4998447436699742\n",
      "Epoch 14, mean: 0.9996947050094604, std: 0.0174716804176569\n",
      "training L: 0.4512443909607265\n",
      "validation L:0.4989221311475409, model have not improved for 1 iterations\n",
      "Epoch 15, mean: 0.9996641874313354, std: 0.018324172124266624\n",
      "training L: 0.45139793012303303\n",
      "validation L:0.49983279500184485\n",
      "Epoch 16, mean: 0.998870313167572, std: 0.03359358757734299\n",
      "training L: 0.45186288379531225\n",
      "validation L:0.5008425060411832\n",
      "Epoch 17, mean: 0.9972519874572754, std: 0.052350964397192\n",
      "training L: 0.452519805536843\n",
      "validation L:0.5012535185673744\n",
      "Epoch 18, mean: 0.9957863092422485, std: 0.06477741152048111\n",
      "training L: 0.4544919343355489\n",
      "validation L:0.5013703832400874\n",
      "Epoch 19, mean: 0.9925191402435303, std: 0.08616947382688522\n",
      "training L: 0.45632163234957185\n",
      "validation L:0.5010809447725445\n",
      "Epoch 20, mean: 0.9914199113845825, std: 0.09223227947950363\n",
      "training L: 0.45653083796501637\n",
      "validation L:0.5028141452381555\n",
      "Epoch 21, mean: 0.9878779053688049, std: 0.1094328835606575\n",
      "training L: 0.4594892573498276\n",
      "validation L:0.5048017116359494\n",
      "Epoch 22, mean: 0.989221453666687, std: 0.10326070338487625\n",
      "training L: 0.45710465582560234\n",
      "validation L:0.5064171298766909\n",
      "Epoch 23, mean: 0.9887023568153381, std: 0.10569017380475998\n",
      "training L: 0.4579207796515155\n",
      "validation L:0.504652077735601\n",
      "Epoch 24, mean: 0.9910229444503784, std: 0.09432275593280792\n",
      "training L: 0.4564193465842272\n",
      "validation L:0.5030513518203085, model have not improved for 1 iterations\n",
      "Epoch 25, mean: 0.9930382370948792, std: 0.08314792066812515\n",
      "training L: 0.45624247204732504\n",
      "validation L:0.5019633548883111, model have not improved for 2 iterations\n",
      "Epoch 26, mean: 0.9952672123908997, std: 0.06863358616828918\n",
      "training L: 0.45465923941069897\n",
      "validation L:0.5008425060411832, model have not improved for 3 iterations\n",
      "Epoch 27, mean: 0.9969771504402161, std: 0.054898589849472046\n",
      "training L: 0.453308506609264\n",
      "validation L:0.5015340993823107, model have not improved for 4 iterations\n",
      "Epoch 28, mean: 0.9982596039772034, std: 0.04168311133980751\n",
      "training L: 0.45207707721173696\n",
      "validation L:0.49937816606318464, model have not improved for 5 iterations\n",
      "Epoch 29, mean: 0.9988092184066772, std: 0.03448851779103279\n",
      "training L: 0.4518340638223524\n",
      "validation L:0.49966544120946266, model have not improved for 6 iterations\n",
      "Epoch 30, mean: 0.998961865901947, std: 0.03220437839627266\n",
      "training L: 0.4516545751435136\n",
      "validation L:0.4992587620971374, model have not improved for 7 iterations\n",
      "Epoch 31, mean: 0.9993283152580261, std: 0.025909939780831337\n",
      "training L: 0.4514078019772601\n",
      "validation L:0.49972521759451, model have not improved for 8 iterations\n",
      "Epoch 32, mean: 0.9995725750923157, std: 0.020671507343649864\n",
      "training L: 0.4514387969966117\n",
      "validation L:0.4999044933649173, model have not improved for 9 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Stopping early due to decrease in performance on validation set\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa38fc150c6040bab0a9445ac42207c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='2.544 MB of 2.544 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>percentage_predicted_aversion</td><td></td></tr><tr><td>training loss</td><td></td></tr><tr><td>training_f1</td><td></td></tr><tr><td>validation_f1</td><td></td></tr><tr><td>validation_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>percentage_predicted_aversion</td><td>0.99969</td></tr><tr><td>training loss</td><td>0.67176</td></tr><tr><td>training_f1</td><td>0.45141</td></tr><tr><td>validation_f1</td><td>0.49892</td></tr><tr><td>validation_loss</td><td>0.65346</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretty-donkey-78</strong> at: <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/2btkatcn' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/2btkatcn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230407_195055-2btkatcn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\", \"r\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "model.to(device)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, \"sentence_and_words\")\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for Audio only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevanpan\u001b[0m (\u001b[33mgaze_prediction_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help`\n",
      "       for usage information.\n",
      "\n",
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help`\n",
      "       for usage information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/artemis.dgpsrv/ondemand27/evanpan/Gaze_project/training/wandb/run-20230410_131445-rm9ad2qr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/rm9ad2qr' target=\"_blank\">atomic-dragon-82</a></strong> to <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/rm9ad2qr' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/rm9ad2qr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help`\n",
      "       for usage information.\n",
      "\n",
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help`\n",
      "       for usage information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mean: 0.9997344613075256, std: 0.01629398763179779\n",
      "training L: 0.47890092625845143\n",
      "validation L:0.45902620869931754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, mean: 0.9985726475715637, std: 0.03775409981608391\n",
      "training L: 0.4761081263856524\n",
      "validation L:0.49461128031147594\n",
      "Epoch 3, mean: 0.9990373849868774, std: 0.03101200982928276\n",
      "training L: 0.47342587689646776\n",
      "validation L:0.5123245934337597\n",
      "Epoch 4, mean: 0.9995021224021912, std: 0.022308869287371635\n",
      "training L: 0.47450191397238706\n",
      "validation L:0.505301360657689\n",
      "Epoch 5, mean: 0.9994522929191589, std: 0.023397155106067657\n",
      "training L: 0.4732315941850137\n",
      "validation L:0.4702578544176642\n",
      "Epoch 6, mean: 0.9996514916419983, std: 0.018666332587599754\n",
      "training L: 0.4739805054340718\n",
      "validation L:0.4757426025390734\n",
      "Epoch 7, mean: 0.9998008608818054, std: 0.014111476019024849\n",
      "training L: 0.47308891153647037\n",
      "validation L:0.5143512821763536\n",
      "Epoch 8, mean: 0.9998008608818054, std: 0.014111476019024849\n",
      "training L: 0.47313888515685837\n",
      "validation L:0.48575649921554076, model have not improved for 1 iterations\n",
      "Epoch 9, mean: 0.999817430973053, std: 0.01351082231849432\n",
      "training L: 0.47322355316679204\n",
      "validation L:0.48642505922336615, model have not improved for 2 iterations\n",
      "Epoch 10, mean: 1.0, std: 0.0\n",
      "training L: 0.4732101658543793\n",
      "validation L:0.5103125778229245\n",
      "Epoch 11, mean: 0.9995518922805786, std: 0.021164577454328537\n",
      "training L: 0.4732281842235089\n",
      "validation L:0.5122594530216742\n",
      "Epoch 12, mean: 0.9999834299087524, std: 0.004074004478752613\n",
      "training L: 0.4731833841313952\n",
      "validation L:0.48810799737853283, model have not improved for 1 iterations\n",
      "Epoch 13, mean: 0.9999502301216125, std: 0.007056265138089657\n",
      "training L: 0.47317066560178817\n",
      "validation L:0.5107655793398285\n",
      "Epoch 14, mean: 0.9999668002128601, std: 0.0057614641264081\n",
      "training L: 0.4731292041684151\n",
      "validation L:0.47604768494869654, model have not improved for 1 iterations\n",
      "Epoch 15, mean: 1.0, std: 0.0\n",
      "training L: 0.47320914449122137\n",
      "validation L:0.5083529456364775\n",
      "Epoch 16, mean: 0.9999668002128601, std: 0.0057614641264081\n",
      "training L: 0.47299633806345903\n",
      "validation L:0.49168015622834005, model have not improved for 1 iterations\n",
      "Epoch 17, mean: 1.0, std: 0.0\n",
      "training L: 0.4731449133789062\n",
      "validation L:0.5047179721190889\n",
      "Epoch 18, mean: 1.0, std: 0.0\n",
      "training L: 0.473253353296321\n",
      "validation L:0.5045688377853605\n",
      "Epoch 19, mean: 1.0, std: 0.0\n",
      "training L: 0.47308606246162266\n",
      "validation L:0.5032544271980076\n",
      "Epoch 20, mean: 1.0, std: 0.0\n",
      "training L: 0.47327838559576896\n",
      "validation L:0.5079922402790154\n",
      "Epoch 21, mean: 0.9999668002128601, std: 0.0057614641264081\n",
      "training L: 0.4732541312814542\n",
      "validation L:0.49872603260787296, model have not improved for 1 iterations\n",
      "Epoch 22, mean: 1.0, std: 0.0\n",
      "training L: 0.47297776930472407\n",
      "validation L:0.48022232030252654, model have not improved for 2 iterations\n",
      "Epoch 23, mean: 0.9998340606689453, std: 0.012882170267403126\n",
      "training L: 0.4732627362241529\n",
      "validation L:0.4775520243905039, model have not improved for 3 iterations\n",
      "Epoch 24, mean: 1.0, std: 0.0\n",
      "training L: 0.47310144763892337\n",
      "validation L:0.48969477989208887, model have not improved for 4 iterations\n",
      "Epoch 25, mean: 1.0, std: 0.0\n",
      "training L: 0.4730766133429731\n",
      "validation L:0.4808397170223903, model have not improved for 5 iterations\n",
      "Epoch 26, mean: 1.0, std: 0.0\n",
      "training L: 0.4730923509076206\n",
      "validation L:0.5062462513258066\n",
      "Epoch 27, mean: 0.9999834299087524, std: 0.004074004013091326\n",
      "training L: 0.4730872959382373\n",
      "validation L:0.49755506089552076\n",
      "Epoch 28, mean: 0.9999834299087524, std: 0.004074004478752613\n",
      "training L: 0.4731321674186042\n",
      "validation L:0.5141013592755255\n",
      "Epoch 29, mean: 0.9999834299087524, std: 0.004074004478752613\n",
      "training L: 0.47323874850737807\n",
      "validation L:0.48764281418346045, model have not improved for 1 iterations\n",
      "Epoch 30, mean: 1.0, std: 0.0\n",
      "training L: 0.473059946006616\n",
      "validation L:0.5115648121554232\n",
      "Epoch 31, mean: 1.0, std: 0.0\n",
      "training L: 0.47305495253273483\n",
      "validation L:0.5122821806336273\n",
      "Epoch 32, mean: 1.0, std: 0.0\n",
      "training L: 0.47326605898781854\n",
      "validation L:0.4831391643228248, model have not improved for 1 iterations\n",
      "Epoch 33, mean: 1.0, std: 0.0\n",
      "training L: 0.4732667703668105\n",
      "validation L:0.4942177830723359, model have not improved for 2 iterations\n",
      "Epoch 34, mean: 1.0, std: 0.0\n",
      "training L: 0.4730517641143523\n",
      "validation L:0.49957311990143727, model have not improved for 3 iterations\n",
      "Epoch 35, mean: 1.0, std: 0.0\n",
      "training L: 0.4731747550891171\n",
      "validation L:0.5033128565153515\n",
      "Epoch 36, mean: 1.0, std: 0.0\n",
      "training L: 0.4732490191197425\n",
      "validation L:0.47715519317583777, model have not improved for 1 iterations\n",
      "Epoch 37, mean: 1.0, std: 0.0\n",
      "training L: 0.47322421210818477\n",
      "validation L:0.4994767567377997\n",
      "Epoch 38, mean: 1.0, std: 0.0\n",
      "training L: 0.4729364606796372\n",
      "validation L:0.5239564668155683\n",
      "Epoch 39, mean: 1.0, std: 0.0\n",
      "training L: 0.4731707539618845\n",
      "validation L:0.49880295575963907\n",
      "Epoch 40, mean: 1.0, std: 0.0\n",
      "training L: 0.4729749725143775\n",
      "validation L:0.4888163984517686, model have not improved for 1 iterations\n",
      "Epoch 41, mean: 1.0, std: 0.0\n",
      "training L: 0.4731213412219422\n",
      "validation L:0.49089477365416373, model have not improved for 2 iterations\n",
      "Epoch 42, mean: 1.0, std: 0.0\n",
      "training L: 0.47298677751323603\n",
      "validation L:0.4944455741976068, model have not improved for 3 iterations\n",
      "Epoch 43, mean: 1.0, std: 0.0\n",
      "training L: 0.473138530561137\n",
      "validation L:0.4824986260302737, model have not improved for 4 iterations\n",
      "Epoch 44, mean: 1.0, std: 0.0\n",
      "training L: 0.4730458625579742\n",
      "validation L:0.500248401884985\n",
      "Epoch 45, mean: 1.0, std: 0.0\n",
      "training L: 0.4730100972022679\n",
      "validation L:0.48619589661552187, model have not improved for 1 iterations\n",
      "Epoch 46, mean: 1.0, std: 0.0\n",
      "training L: 0.47318101968843024\n",
      "validation L:0.5053050264586404\n",
      "Epoch 47, mean: 1.0, std: 0.0\n",
      "training L: 0.4731986692185262\n",
      "validation L:0.5017385669352246\n",
      "Epoch 48, mean: 1.0, std: 0.0\n",
      "training L: 0.4731289760408943\n",
      "validation L:0.4786485258232666, model have not improved for 1 iterations\n",
      "Epoch 49, mean: 1.0, std: 0.0\n",
      "training L: 0.47316234998771045\n",
      "validation L:0.4851612605381508, model have not improved for 2 iterations\n",
      "Epoch 50, mean: 1.0, std: 0.0\n",
      "training L: 0.4729444298051593\n",
      "validation L:0.47604488499717834, model have not improved for 3 iterations\n",
      "Epoch 51, mean: 1.0, std: 0.0\n",
      "training L: 0.4731658544441488\n",
      "validation L:0.4784321317783058, model have not improved for 4 iterations\n",
      "Epoch 52, mean: 1.0, std: 0.0\n",
      "training L: 0.4730014287055155\n",
      "validation L:0.4631774978253337, model have not improved for 5 iterations\n",
      "Epoch 53, mean: 1.0, std: 0.0\n",
      "training L: 0.4731317261759182\n",
      "validation L:0.47549140378144417, model have not improved for 6 iterations\n",
      "Epoch 54, mean: 1.0, std: 0.0\n",
      "training L: 0.4729670880438021\n",
      "validation L:0.486881412564133\n",
      "Epoch 55, mean: 1.0, std: 0.0\n",
      "training L: 0.47296220148202206\n",
      "validation L:0.4958346441774521\n",
      "Epoch 56, mean: 1.0, std: 0.0\n",
      "training L: 0.4730910098760686\n",
      "validation L:0.4881638658095039\n",
      "Epoch 57, mean: 1.0, std: 0.0\n",
      "training L: 0.4730656649098398\n",
      "validation L:0.5019907679743983\n",
      "Epoch 58, mean: 1.0, std: 0.0\n",
      "training L: 0.47309101171259105\n",
      "validation L:0.473465663402054, model have not improved for 1 iterations\n",
      "Epoch 59, mean: 1.0, std: 0.0\n",
      "training L: 0.47320477193523985\n",
      "validation L:0.49474953722405646\n",
      "Epoch 60, mean: 1.0, std: 0.0\n",
      "training L: 0.47324287610351845\n",
      "validation L:0.5122422924489989\n",
      "Epoch 61, mean: 1.0, std: 0.0\n",
      "training L: 0.4731821193077876\n",
      "validation L:0.4956059105097421\n",
      "Epoch 62, mean: 1.0, std: 0.0\n",
      "training L: 0.47306737144401634\n",
      "validation L:0.4908008626151837, model have not improved for 1 iterations\n",
      "Epoch 63, mean: 1.0, std: 0.0\n",
      "training L: 0.4729947058479987\n",
      "validation L:0.4870483897610455, model have not improved for 2 iterations\n",
      "Epoch 64, mean: 1.0, std: 0.0\n",
      "training L: 0.4729837225637233\n",
      "validation L:0.49051929358549917, model have not improved for 3 iterations\n",
      "Epoch 65, mean: 1.0, std: 0.0\n",
      "training L: 0.47319496360374086\n",
      "validation L:0.48833153076635916, model have not improved for 4 iterations\n",
      "Epoch 66, mean: 1.0, std: 0.0\n",
      "training L: 0.473254478781797\n",
      "validation L:0.5010217350627197\n",
      "Epoch 67, mean: 1.0, std: 0.0\n",
      "training L: 0.47313154590236245\n",
      "validation L:0.5009636766284334\n",
      "Epoch 68, mean: 1.0, std: 0.0\n",
      "training L: 0.47335712030324273\n",
      "validation L:0.5036441332621191\n",
      "Epoch 69, mean: 1.0, std: 0.0\n",
      "training L: 0.4730565020589641\n",
      "validation L:0.5131007781857977\n",
      "Epoch 70, mean: 1.0, std: 0.0\n",
      "training L: 0.47304930745023027\n",
      "validation L:0.4929677136121797, model have not improved for 1 iterations\n",
      "Epoch 71, mean: 1.0, std: 0.0\n",
      "training L: 0.4732177474159137\n",
      "validation L:0.4939901473644126, model have not improved for 2 iterations\n",
      "Epoch 72, mean: 1.0, std: 0.0\n",
      "training L: 0.4732118024261133\n",
      "validation L:0.49423675973189984, model have not improved for 3 iterations\n",
      "Epoch 73, mean: 1.0, std: 0.0\n",
      "training L: 0.47330823129500615\n",
      "validation L:0.4910450881611954, model have not improved for 4 iterations\n",
      "Epoch 74, mean: 1.0, std: 0.0\n",
      "training L: 0.4730659408991016\n",
      "validation L:0.47888313936802646, model have not improved for 5 iterations\n",
      "Epoch 75, mean: 1.0, std: 0.0\n",
      "training L: 0.4730271539738096\n",
      "validation L:0.49673200368192894, model have not improved for 6 iterations\n",
      "Epoch 76, mean: 1.0, std: 0.0\n",
      "training L: 0.47311382991238377\n",
      "validation L:0.4858629534183951, model have not improved for 7 iterations\n",
      "Epoch 77, mean: 1.0, std: 0.0\n",
      "training L: 0.472943441176352\n",
      "validation L:0.47410372840359455, model have not improved for 8 iterations\n",
      "Epoch 78, mean: 1.0, std: 0.0\n",
      "training L: 0.47302145642879195\n",
      "validation L:0.4778738805144055, model have not improved for 9 iterations\n",
      "Epoch 79, mean: 1.0, std: 0.0\n",
      "training L: 0.4730961897691232\n",
      "validation L:0.48284617520462536, model have not improved for 10 iterations\n",
      "Epoch 80, mean: 1.0, std: 0.0\n",
      "training L: 0.47299583556440383\n",
      "validation L:0.514121396526378\n",
      "Epoch 81, mean: 1.0, std: 0.0\n",
      "training L: 0.4732530267929096\n",
      "validation L:0.4836709426200486\n",
      "Epoch 82, mean: 1.0, std: 0.0\n",
      "training L: 0.47307779803071276\n",
      "validation L:0.49134592691460854\n",
      "Epoch 83, mean: 1.0, std: 0.0\n",
      "training L: 0.4730904355292005\n",
      "validation L:0.4992455930842452\n",
      "Epoch 84, mean: 1.0, std: 0.0\n",
      "training L: 0.4732431156119658\n",
      "validation L:0.49711457549166427\n",
      "Epoch 85, mean: 1.0, std: 0.0\n",
      "training L: 0.47303963160725665\n",
      "validation L:0.5142416411099593\n",
      "Epoch 86, mean: 1.0, std: 0.0\n",
      "training L: 0.47293502309077723\n",
      "validation L:0.48732688485281833, model have not improved for 1 iterations\n",
      "Epoch 87, mean: 1.0, std: 0.0\n",
      "training L: 0.4730524763282398\n",
      "validation L:0.49495867177242725, model have not improved for 2 iterations\n",
      "Epoch 88, mean: 1.0, std: 0.0\n",
      "training L: 0.47310283218906246\n",
      "validation L:0.48606637579960543, model have not improved for 3 iterations\n",
      "Epoch 89, mean: 1.0, std: 0.0\n",
      "training L: 0.4730107235830589\n",
      "validation L:0.5147229805306615\n",
      "Epoch 90, mean: 1.0, std: 0.0\n",
      "training L: 0.47311107748476383\n",
      "validation L:0.49780428033851415\n",
      "Epoch 91, mean: 1.0, std: 0.0\n",
      "training L: 0.4731885223554634\n",
      "validation L:0.5030013420291972\n",
      "Epoch 92, mean: 1.0, std: 0.0\n",
      "training L: 0.473246160606834\n",
      "validation L:0.4855673072399487, model have not improved for 1 iterations\n",
      "Epoch 93, mean: 1.0, std: 0.0\n",
      "training L: 0.47293188141037756\n",
      "validation L:0.4789914879669759, model have not improved for 2 iterations\n",
      "Epoch 94, mean: 1.0, std: 0.0\n",
      "training L: 0.4733426423471283\n",
      "validation L:0.49636895652031154, model have not improved for 3 iterations\n",
      "Epoch 95, mean: 1.0, std: 0.0\n",
      "training L: 0.47309533300231554\n",
      "validation L:0.5107295415151222\n",
      "Epoch 96, mean: 1.0, std: 0.0\n",
      "training L: 0.47304952426752933\n",
      "validation L:0.4908572059582705, model have not improved for 1 iterations\n",
      "Epoch 97, mean: 1.0, std: 0.0\n",
      "training L: 0.47313256341152765\n",
      "validation L:0.49378161794455666, model have not improved for 2 iterations\n",
      "Epoch 98, mean: 1.0, std: 0.0\n",
      "training L: 0.4732048943033988\n",
      "validation L:0.4994960272547313\n",
      "Epoch 99, mean: 1.0, std: 0.0\n",
      "training L: 0.4729615858777898\n",
      "validation L:0.5183513527455383\n",
      "Epoch 100, mean: 1.0, std: 0.0\n",
      "training L: 0.47324787470940777\n",
      "validation L:0.5189389326464549\n",
      "Epoch 101, mean: 1.0, std: 0.0\n",
      "training L: 0.473003173375865\n",
      "validation L:0.4876056294090624, model have not improved for 1 iterations\n",
      "Epoch 102, mean: 1.0, std: 0.0\n",
      "training L: 0.47308019898856085\n",
      "validation L:0.469407809392122, model have not improved for 2 iterations\n",
      "Epoch 103, mean: 1.0, std: 0.0\n",
      "training L: 0.47323633378993335\n",
      "validation L:0.48222453336577364, model have not improved for 3 iterations\n",
      "Epoch 104, mean: 1.0, std: 0.0\n",
      "training L: 0.473068062790088\n",
      "validation L:0.5101935277119289\n",
      "Epoch 105, mean: 1.0, std: 0.0\n",
      "training L: 0.4731078072583858\n",
      "validation L:0.49272204397769, model have not improved for 1 iterations\n",
      "Epoch 106, mean: 1.0, std: 0.0\n",
      "training L: 0.47307564110753625\n",
      "validation L:0.4958727814761152\n",
      "Epoch 107, mean: 1.0, std: 0.0\n",
      "training L: 0.4731167782105775\n",
      "validation L:0.498649126432424\n",
      "Epoch 108, mean: 1.0, std: 0.0\n",
      "training L: 0.473124626316731\n",
      "validation L:0.4841852816350595, model have not improved for 1 iterations\n",
      "Epoch 109, mean: 1.0, std: 0.0\n",
      "training L: 0.4732179216197953\n",
      "validation L:0.5049331170257045\n",
      "Epoch 110, mean: 1.0, std: 0.0\n",
      "training L: 0.4730244615403663\n",
      "validation L:0.4892646299771211, model have not improved for 1 iterations\n",
      "Epoch 111, mean: 1.0, std: 0.0\n",
      "training L: 0.47311178297996015\n",
      "validation L:0.5126014315454969\n",
      "Epoch 112, mean: 1.0, std: 0.0\n",
      "training L: 0.4729740944251946\n",
      "validation L:0.4957202580275223\n",
      "Epoch 113, mean: 1.0, std: 0.0\n",
      "training L: 0.4730184125848495\n",
      "validation L:0.5035661586888861\n",
      "Epoch 114, mean: 1.0, std: 0.0\n",
      "training L: 0.4731743598460476\n",
      "validation L:0.4875870386830753, model have not improved for 1 iterations\n",
      "Epoch 115, mean: 1.0, std: 0.0\n",
      "training L: 0.4731791505404631\n",
      "validation L:0.4948445822322688, model have not improved for 2 iterations\n",
      "Epoch 116, mean: 1.0, std: 0.0\n",
      "training L: 0.472985269945818\n",
      "validation L:0.4878474095997912, model have not improved for 3 iterations\n",
      "Epoch 117, mean: 1.0, std: 0.0\n",
      "training L: 0.473119203519745\n",
      "validation L:0.5197710656485265\n",
      "Epoch 118, mean: 1.0, std: 0.0\n",
      "training L: 0.47291799715512634\n",
      "validation L:0.4997852119230486\n",
      "Epoch 119, mean: 1.0, std: 0.0\n",
      "training L: 0.47301675355480394\n",
      "validation L:0.4987452618044756\n",
      "Epoch 120, mean: 1.0, std: 0.0\n",
      "training L: 0.47310888915300947\n",
      "validation L:0.46615234506562897, model have not improved for 1 iterations\n",
      "Epoch 121, mean: 1.0, std: 0.0\n",
      "training L: 0.473113124770968\n",
      "validation L:0.4806943286984784, model have not improved for 2 iterations\n",
      "Epoch 122, mean: 1.0, std: 0.0\n",
      "training L: 0.4730598958175303\n",
      "validation L:0.5208899403326197\n",
      "Epoch 123, mean: 1.0, std: 0.0\n",
      "training L: 0.4729387336658319\n",
      "validation L:0.48433239740480627, model have not improved for 1 iterations\n",
      "Epoch 124, mean: 1.0, std: 0.0\n",
      "training L: 0.4731413292151615\n",
      "validation L:0.5074064306834487\n",
      "Epoch 125, mean: 1.0, std: 0.0\n",
      "training L: 0.4731655017363324\n",
      "validation L:0.4710163441665979, model have not improved for 1 iterations\n",
      "Epoch 126, mean: 1.0, std: 0.0\n",
      "training L: 0.472974174052145\n",
      "validation L:0.4924199332089726\n",
      "Epoch 127, mean: 1.0, std: 0.0\n",
      "training L: 0.47315563216836176\n",
      "validation L:0.5034492280970063\n",
      "Epoch 128, mean: 1.0, std: 0.0\n",
      "training L: 0.4731348409036615\n",
      "validation L:0.4862144040448884, model have not improved for 1 iterations\n",
      "Epoch 129, mean: 1.0, std: 0.0\n",
      "training L: 0.4732176509109758\n",
      "validation L:0.4954725539214925\n",
      "Epoch 130, mean: 1.0, std: 0.0\n",
      "training L: 0.47313195543067654\n",
      "validation L:0.5137808998426611\n",
      "Epoch 131, mean: 1.0, std: 0.0\n",
      "training L: 0.4730423263990631\n",
      "validation L:0.5048939904336421\n",
      "Epoch 132, mean: 1.0, std: 0.0\n",
      "training L: 0.4731646338572154\n",
      "validation L:0.4843691875703525, model have not improved for 1 iterations\n",
      "Epoch 133, mean: 1.0, std: 0.0\n",
      "training L: 0.4730592100866238\n",
      "validation L:0.49332709886805765, model have not improved for 2 iterations\n",
      "Epoch 134, mean: 1.0, std: 0.0\n",
      "training L: 0.4730375812735906\n",
      "validation L:0.4906318916515564, model have not improved for 3 iterations\n",
      "Epoch 135, mean: 1.0, std: 0.0\n",
      "training L: 0.47306182330031\n",
      "validation L:0.5176837395989565\n",
      "Epoch 136, mean: 1.0, std: 0.0\n",
      "training L: 0.4731153851585436\n",
      "validation L:0.49096992216426993, model have not improved for 1 iterations\n",
      "Epoch 137, mean: 1.0, std: 0.0\n",
      "training L: 0.4731999298926169\n",
      "validation L:0.4845532057033061, model have not improved for 2 iterations\n",
      "Epoch 138, mean: 1.0, std: 0.0\n",
      "training L: 0.47317979207205635\n",
      "validation L:0.4782159032994927, model have not improved for 3 iterations\n",
      "Epoch 139, mean: 1.0, std: 0.0\n",
      "training L: 0.47302246316298974\n",
      "validation L:0.4959299954676166\n",
      "Epoch 140, mean: 1.0, std: 0.0\n",
      "training L: 0.47300583985424344\n",
      "validation L:0.5099951922860613\n",
      "Epoch 141, mean: 1.0, std: 0.0\n",
      "training L: 0.4732103200186108\n",
      "validation L:0.4957583824525339\n",
      "Epoch 142, mean: 1.0, std: 0.0\n",
      "training L: 0.4731094486527698\n",
      "validation L:0.4660663460647845, model have not improved for 1 iterations\n",
      "Epoch 143, mean: 1.0, std: 0.0\n",
      "training L: 0.47321442349984205\n",
      "validation L:0.4996116725711145\n",
      "Epoch 144, mean: 1.0, std: 0.0\n",
      "training L: 0.47290768073827405\n",
      "validation L:0.4698264546201518, model have not improved for 1 iterations\n",
      "Epoch 145, mean: 1.0, std: 0.0\n",
      "training L: 0.473002267685314\n",
      "validation L:0.49589185173422573\n",
      "Epoch 146, mean: 1.0, std: 0.0\n",
      "training L: 0.47306819389857696\n",
      "validation L:0.48844335710220793\n",
      "Epoch 147, mean: 1.0, std: 0.0\n",
      "training L: 0.47302197524911527\n",
      "validation L:0.5106103632915493\n",
      "Epoch 148, mean: 1.0, std: 0.0\n",
      "training L: 0.47299249455022024\n",
      "validation L:0.5087875542511486\n",
      "Epoch 149, mean: 1.0, std: 0.0\n",
      "training L: 0.47317272899142115\n",
      "validation L:0.5082739791341344\n",
      "Epoch 150, mean: 1.0, std: 0.0\n",
      "training L: 0.4731409676216782\n",
      "validation L:0.4918919090196553, model have not improved for 1 iterations\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[39m=\u001b[39m Audio_only_GazePredictionModel(config)\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m train_model(model, config, train_dataloader, valid_dataloader, run_obj, \u001b[39m\"\u001b[39;49m\u001b[39mauduo_only\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     13\u001b[0m run_obj\u001b[39m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, config, train_data, valid_data, wandb, model_name)\u001b[0m\n\u001b[1;32m     25\u001b[0m prediction_std \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     26\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfor\u001b[39;00m _, (X, Y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_data):\n\u001b[1;32m     28\u001b[0m     train_batch_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     29\u001b[0m     X, Y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), Y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/Gaze_project/Dataset_Util/dataloader.py:201\u001b[0m, in \u001b[0;36mAversion_SelfTap111.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    199\u001b[0m input_text_off_screen \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(offscreen_text_feature_path)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentence_and_word_timing:\n\u001b[0;32m--> 201\u001b[0m     input_text_on_screen1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(onscreen_text_feature1_path)\n\u001b[1;32m    202\u001b[0m     input_text_off_screen1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(offscreen_text_feature1_path)\n\u001b[1;32m    203\u001b[0m     input_text_on_screen2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(onscreen_text_feature2_path)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mopen_memmap(file, mode\u001b[39m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(fid, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[1;32m    433\u001b[0m                                  pickle_kwargs\u001b[39m=\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    434\u001b[0m                                  max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/format.py:765\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    763\u001b[0m version \u001b[39m=\u001b[39m read_magic(fp)\n\u001b[1;32m    764\u001b[0m _check_version(version)\n\u001b[0;32m--> 765\u001b[0m shape, fortran_order, dtype \u001b[39m=\u001b[39m _read_array_header(\n\u001b[1;32m    766\u001b[0m         fp, version, max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    767\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shape) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    768\u001b[0m     count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/format.py:616\u001b[0m, in \u001b[0;36m_read_array_header\u001b[0;34m(fp, version, max_header_size)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[39m# The header is a pretty-printed string representation of a literal\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[39m# Python dictionary with trailing newlines padded to a ARRAY_ALIGN byte\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[39m# boundary. The keys are strings.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[39m# Versions (2, 0) and (1, 0) could have been created by a Python 2\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[39m# implementation before header filtering was implemented.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[39mif\u001b[39;00m version \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m):\n\u001b[0;32m--> 616\u001b[0m     header \u001b[39m=\u001b[39m _filter_header(header)\n\u001b[1;32m    617\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     d \u001b[39m=\u001b[39m safe_eval(header)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/format.py:572\u001b[0m, in \u001b[0;36m_filter_header\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    570\u001b[0m token_type \u001b[39m=\u001b[39m token[\u001b[39m0\u001b[39m]\n\u001b[1;32m    571\u001b[0m token_string \u001b[39m=\u001b[39m token[\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 572\u001b[0m \u001b[39mif\u001b[39;00m (last_token_was_number \u001b[39mand\u001b[39;00m\n\u001b[1;32m    573\u001b[0m         token_type \u001b[39m==\u001b[39m tokenize\u001b[39m.\u001b[39mNAME \u001b[39mand\u001b[39;00m\n\u001b[1;32m    574\u001b[0m         token_string \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    575\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\", \"r\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = Audio_only_GazePredictionModel(config)\n",
    "model.to(device)\n",
    "train_model(model, config, train_dataloader, valid_dataloader, run_obj, \"auduo_only\")\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for sentence + word + audio. but also uses velocity as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity(out):\n",
    "    vel = torch.diff(out, dim=1, prepend=out[:, 0:1, :])\n",
    "    return vel\n",
    "def train_model_with_vel(model, config, train_data, valid_data, wandb, model_name, start = 1):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train() \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_fn_vel = nn.MSELoss()\n",
    "    training_loss = []\n",
    "    valid_loss = []\n",
    "    training_f1 = []\n",
    "    valid_f1 = []\n",
    "    aversion_vs_start = []\n",
    "    count = 0\n",
    "    # f1_score = BinaryF1Score(num_classes=2).to(device)\n",
    "    f1_score = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\").to(device)\n",
    "    for epoch in range(start, config['epochs'] + 1):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "        total_aversion_predicted = 0\n",
    "        total_train_f1 = 0\n",
    "        total_valid_f1 = 0\n",
    "        train_batch_counter = 0\n",
    "        valid_batch_counter = 0\n",
    "        total_prediction_counter = 0\n",
    "        prediction_mean = 0\n",
    "        prediction_std = 0\n",
    "        model.zero_grad()\n",
    "        for _, (X, [Y, Y_vel]) in enumerate(train_data):\n",
    "            train_batch_counter += 1\n",
    "            X, Y, Y_vel = X.to(device), Y.to(device), Y_vel.to(device)            \n",
    "            optimiser.zero_grad()\n",
    "            if \"Transformer\" in config[\"model_type\"]:\n",
    "                all_zero = torch.zeros(Y.shape).to(device)\n",
    "                pred = model(X, all_zero)\n",
    "            else:\n",
    "                pred = model(X)\n",
    "            loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "            # get the softmax\n",
    "            pred_vel = get_velocity(torch.softmax(pred, axis=2))\n",
    "            # add the velocity along dimension 1 and 2 respectively, the velocity are opposite \n",
    "            loss = loss + 0.3 * loss_fn_vel(pred_vel[:, :, 1], Y_vel)\n",
    "            loss = loss + 0.3 * loss_fn_vel(pred_vel[:, :, 0], -Y_vel)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_train_loss += loss.item()\n",
    "            # binary_pred = torch.round(pred)\n",
    "            binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "            prediction_mean = torch.mean(binary_pred.float()).item()\n",
    "            prediction_std = torch.std(binary_pred.float()).item()            \n",
    "            f1_train = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "            total_aversion_predicted += torch.sum(binary_pred).item()\n",
    "            total_prediction_counter += binary_pred.size()[0] * binary_pred.size()[1] \n",
    "            total_train_f1 += f1_train\n",
    "            del X, Y, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        total_train_f1 /= train_batch_counter\n",
    "        total_train_loss /= len(train_data)\n",
    "        total_aversion_predicted /= total_prediction_counter\n",
    "\n",
    "        for _, (X, [Y, Y_vel]) in enumerate(valid_data):\n",
    "            with torch.no_grad():\n",
    "                valid_batch_counter += 1\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                if \"Transformer\" in config[\"model_type\"]:\n",
    "                    all_zero = torch.zeros(Y.shape).to(device)\n",
    "                    pred = model(X, all_zero)\n",
    "                else:\n",
    "                    pred = model(X)\n",
    "                loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "                # binary_pred = torch.round(pred)\n",
    "                binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "                f1_valid = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "                total_valid_f1 += f1_valid\n",
    "                del X, Y, pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        total_valid_f1 /= valid_batch_counter\n",
    "        total_valid_loss /= len(valid_data)\n",
    "\n",
    "        if config['wandb']:\n",
    "            wandb.log({'training loss': total_train_loss,\n",
    "                        'validation_loss': total_valid_loss,\n",
    "                        'training_f1': total_train_f1,\n",
    "                        'validation_f1': total_valid_f1, \n",
    "                        \"percentage_predicted_aversion\": total_aversion_predicted})\n",
    "        training_loss.append(total_train_loss)\n",
    "        valid_loss.append(total_valid_loss)\n",
    "        training_f1.append(total_train_f1)\n",
    "        valid_f1.append(total_valid_f1)\n",
    "        aversion_vs_start.append(total_aversion_predicted)\n",
    "        if total_valid_f1 == max(valid_f1):\n",
    "            try:\n",
    "                os.mkdir(os.path.join(*[model_save_location, model_name]))\n",
    "            except:\n",
    "                pass\n",
    "            config_save_path = os.path.join(*[model_save_location, model_name, \"config.json\"])\n",
    "            json.dump(config, open(config_save_path, \"w\"))\n",
    "            file_name = f'time={datetime.now()}_epoch={epoch}.pt'\n",
    "            save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        if config['early_stopping']>0:\n",
    "            if epoch > 1:\n",
    "                if total_valid_f1 < np.mean(valid_f1[epoch - 7:epoch - 2]):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "            if count >= config['early_stopping']:\n",
    "                print('\\n\\nStopping early due to decrease in performance on validation set\\n\\n')\n",
    "                break \n",
    "        if count == 0:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1))\n",
    "        else:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}, model have not improved for {} iterations\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1, count))\n",
    "    if config['wandb']:\n",
    "        save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "        wandb.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:0x1rrp9p) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:0x1rrp9p). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35693a570f44b9ba6137febc6c8f93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669488116167485, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/artemis.dgpsrv/ondemand27/evanpan/Gaze_project/training/wandb/run-20230410_204035-3jnw3zl1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/3jnw3zl1' target=\"_blank\">legendary-snow-85</a></strong> to <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/3jnw3zl1' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/3jnw3zl1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_model_with_vel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m training_dataset[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m train_model_with_vel(model, config, train_dataloader, valid_dataloader, run_obj, \u001b[39m\"\u001b[39m\u001b[39msentence_and_words_with_vel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m run_obj\u001b[39m.\u001b[39mfinish()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model_with_vel' is not defined"
     ]
    }
   ],
   "source": [
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\", \"r\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "model.to(device)\n",
    "training_dataset[0]\n",
    "train_model_with_vel(model, config, train_dataloader, valid_dataloader, run_obj, \"sentence_and_words_with_vel\")\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:0x1rrp9p) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>percentage_predicted_aversion</td><td>0.63243</td></tr><tr><td>training loss</td><td>0.16717</td></tr><tr><td>training_f1</td><td>0.92708</td></tr><tr><td>validation_f1</td><td>0.91199</td></tr><tr><td>validation_loss</td><td>0.18008</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Word+sentence+audio with velocity loss</strong> at: <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/0x1rrp9p' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/0x1rrp9p</a><br/>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230410_205557-0x1rrp9p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:0x1rrp9p). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9249cd3a22425d880210783f6c6eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669333699004103, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/artemis.dgpsrv/ondemand27/evanpan/Gaze_project/training/wandb/run-20230410_205633-0x1rrp9p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/0x1rrp9p' target=\"_blank\">Word+sentence+audio with velocity loss</a></strong> to <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/0x1rrp9p' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/0x1rrp9p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289, mean: 0.727568507194519, std: 0.4452143907546997\n",
      "training L: 0.8862669987488125\n",
      "validation L:0.8699159448302187\n",
      "Epoch 290, mean: 0.6913692951202393, std: 0.46193215250968933\n",
      "training L: 0.9023071043049081\n",
      "validation L:0.9032333195554947\n",
      "Epoch 291, mean: 0.6280165910720825, std: 0.48333799839019775\n",
      "training L: 0.9212732141318789\n",
      "validation L:0.9204383766673967\n",
      "Epoch 292, mean: 0.6386224031448364, std: 0.4804036617279053\n",
      "training L: 0.9278142231326103\n",
      "validation L:0.9182339477268722\n",
      "Epoch 293, mean: 0.6577759385108948, std: 0.47445806860923767\n",
      "training L: 0.9279337160812661\n",
      "validation L:0.919067533646487\n",
      "Epoch 294, mean: 0.6244979500770569, std: 0.4842562973499298\n",
      "training L: 0.9294962620281899\n",
      "validation L:0.9166458217475921\n",
      "Epoch 295, mean: 0.636564314365387, std: 0.48099276423454285\n",
      "training L: 0.9304094280362858\n",
      "validation L:0.9090100685655462\n",
      "Epoch 296, mean: 0.6215767860412598, std: 0.4849979281425476\n",
      "training L: 0.9275395727425941\n",
      "validation L:0.8954359215854675\n",
      "Epoch 297, mean: 0.6678672432899475, std: 0.47098225355148315\n",
      "training L: 0.9284840907072885\n",
      "validation L:0.9088835050936316\n",
      "Epoch 298, mean: 0.599601686000824, std: 0.4899831712245941\n",
      "training L: 0.9307441635142087\n",
      "validation L:0.9146186530694254\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\", \"r\"))\n",
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "config[\"learning_rate\"] = 0.001\n",
    "config[\"load_model\"] = True\n",
    "if config[\"wandb\"]:\n",
    "    wandb.login()\n",
    "    if config[\"load_model\"]:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, save_code=True,\n",
    "            resume='allow', id='0x1rrp9p')\n",
    "        # checkpoint_name = \"gaze_prediction_team/gaze_prediction/8w9fyxan\"\n",
    "        checkpoint_path = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/sentence_and_words_with_vel/time=2023-04-10 17:21:49.696880_epoch=267.pt\"\n",
    "        wandb.restore(checkpoint_path)\n",
    "        pretrained_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_weights(pretrained_dict)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "else:\n",
    "    run_obj = None\n",
    "\n",
    "training_dataset = Aversion_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "\n",
    "train_model_with_vel(model, config, train_dataloader, valid_dataloader, run_obj, \"sentence_and_words_with_vel\", 289)\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for sentence + word + audio. but also uses velocity as a loss function, also predict gaze Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity(out):\n",
    "    vel = torch.diff(out, dim=1, prepend=out[:, 0:1, :])\n",
    "    return vel\n",
    "def train_model_with_for_direction(model, config, train_data, valid_data, wandb, model_name, start = 1, num_of_classes=4):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train() \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_fn2 = nn.CrossEntropyLoss()\n",
    "    loss_fn_vel = nn.MSELoss()\n",
    "    training_loss = []\n",
    "    valid_loss = []\n",
    "    training_f1 = []\n",
    "    valid_f1 = []\n",
    "    aversion_vs_start = []\n",
    "    training_direction_pred_f1 = []\n",
    "    valid_direction_pred_f1 = []\n",
    "    count = 0\n",
    "    # f1_score = BinaryF1Score(num_classes=2).to(device)\n",
    "    f1_score = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\").to(device)\n",
    "    f1_score_direction = F1Score(task=\"multiclass\", num_classes=num_of_classes, average=\"weighted\").to(device)\n",
    "    for epoch in range(start, start + config['epochs'] + 1):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "        total_aversion_predicted = 0\n",
    "        total_train_f1 = 0\n",
    "        total_valid_f1 = 0\n",
    "        total_train_direction_f1 = 0\n",
    "        total_valid_direction_f1 = 0\n",
    "        train_batch_counter = 0\n",
    "        valid_batch_counter = 0\n",
    "        total_prediction_counter = 0\n",
    "        prediction_mean = 0\n",
    "        prediction_std = 0\n",
    "        model.zero_grad()\n",
    "        for _, (X, [Y, Y_vel, Y_dir]) in enumerate(train_data):\n",
    "            train_batch_counter += 1\n",
    "            X, Y, Y_vel, Y_dir= X.to(device), Y.to(device), Y_vel.to(device), Y_dir.to(device)   \n",
    "            optimiser.zero_grad()\n",
    "            if \"Transformer\" in config[\"model_type\"]:\n",
    "                all_zero = torch.zeros(Y.shape).to(device)\n",
    "                pred, dire_pred = model(X, all_zero)\n",
    "            else:\n",
    "                pred, dire_pred = model(X)\n",
    "            loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "            # loss for the directional classification\n",
    "            if epoch <= 100:\n",
    "                factor = 0.1\n",
    "            else:\n",
    "                factor = 1\n",
    "            loss = loss + factor * loss_fn2(dire_pred.transpose(2, 1), torch.argmax(Y_dir, axis=2).long())\n",
    "            # get the softmax\n",
    "            pred_vel = get_velocity(torch.softmax(pred, axis=2))\n",
    "            # add the velocity along dimension 1 and 2 respectively, the velocity are opposite \n",
    "            loss = loss + 0.3 * loss_fn_vel(pred_vel[:, :, 1], Y_vel)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_train_loss += loss.item()\n",
    "            # binary_pred = torch.round(pred)\n",
    "            binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "            dire_pred = torch.argmax(dire_pred, axis=2, keepdim=True)\n",
    "            f1_train = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "            f1_dire_train = f1_score_direction(dire_pred, torch.argmax(Y_dir, axis=2, keepdim=True)).item()\n",
    "            \n",
    "            prediction_mean = torch.mean(binary_pred.float()).item()\n",
    "            prediction_std = torch.std(binary_pred.float()).item()            \n",
    "            \n",
    "            total_aversion_predicted += torch.sum(binary_pred).item()\n",
    "            total_prediction_counter += binary_pred.size()[0] * binary_pred.size()[1] \n",
    "            total_train_f1 += f1_train\n",
    "            total_train_direction_f1 += f1_dire_train\n",
    "            del X, Y, pred, Y_vel, Y_dir, dire_pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        total_train_f1 /= train_batch_counter\n",
    "        total_train_direction_f1 /= train_batch_counter\n",
    "        total_train_loss /= len(train_data)\n",
    "        total_aversion_predicted /= total_prediction_counter\n",
    "        \n",
    "        for _, (X, [Y, Y_vel, Y_dir]) in enumerate(valid_data):\n",
    "            with torch.no_grad():\n",
    "                valid_batch_counter += 1\n",
    "                X, Y, Y_vel, Y_dir= X.to(device), Y.to(device), Y_vel.to(device), Y_dir.to(device)   \n",
    "                if \"Transformer\" in config[\"model_type\"]:\n",
    "                    all_zero = torch.zeros(Y.shape).to(device)\n",
    "                    pred, dire_pred = model(X, all_zero)\n",
    "                else:\n",
    "                    pred, dire_pred = model(X)\n",
    "                loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "                loss = loss + loss_fn2(dire_pred.transpose(2, 1), torch.argmax(Y_dir, axis=2).long())\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "                # binary_pred = torch.round(pred)\n",
    "                binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "                f1_valid = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "            \n",
    "                dire_pred = torch.argmax(dire_pred, axis=2, keepdim=True)\n",
    "                f1_dire_valid = f1_score_direction(dire_pred, torch.argmax(Y_dir, axis=2, keepdim=True)).item()\n",
    "\n",
    "                total_valid_f1 += f1_valid\n",
    "                total_valid_direction_f1 += f1_dire_valid\n",
    "                del X, Y, pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        total_valid_f1 /= valid_batch_counter\n",
    "        total_valid_direction_f1 /= valid_batch_counter\n",
    "        total_valid_loss /= len(valid_data)\n",
    "\n",
    "        if config['wandb']:\n",
    "            wandb.log({'training loss': total_train_loss,\n",
    "                        'validation_loss': total_valid_loss,\n",
    "                        'training_f1': total_train_f1,\n",
    "                        'validation_f1': total_valid_f1, \n",
    "                        'training_f1_direction': total_train_direction_f1,\n",
    "                        'validation_f1_direction': total_valid_direction_f1,\n",
    "                        \"percentage_predicted_aversion\": total_aversion_predicted})\n",
    "        training_loss.append(total_train_loss)\n",
    "        valid_loss.append(total_valid_loss)\n",
    "        training_f1.append(total_train_f1)\n",
    "        valid_f1.append(total_valid_f1)\n",
    "        aversion_vs_start.append(total_aversion_predicted)\n",
    "        if total_valid_f1 == max(valid_f1):\n",
    "            try:\n",
    "                os.mkdir(os.path.join(*[model_save_location, model_name]))\n",
    "            except:\n",
    "                pass\n",
    "            config_save_path = os.path.join(*[model_save_location, model_name, \"config.json\"])\n",
    "            json.dump(config, open(config_save_path, \"w\"))\n",
    "            file_name = f'time={datetime.now()}_epoch={epoch}.pt'\n",
    "            save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        if config['early_stopping']>0:\n",
    "            if epoch > 1:\n",
    "                if total_valid_f1 < np.mean(valid_f1[epoch - 7:epoch - 2]):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "            if count >= config['early_stopping']:\n",
    "                print('\\n\\nStopping early due to decrease in performance on validation set\\n\\n')\n",
    "                break \n",
    "        if count == 0:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1))\n",
    "        else:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}, model have not improved for {} iterations\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1, count))\n",
    "    if config['wandb']:\n",
    "        save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "        wandb.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe4e5d2e89a44239eb0c1eaf92fe3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670152865117416, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /tmp/ipykernel_64833/1140664169.py 1 <module>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m run_obj \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39;49minit(project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgaze_prediction\u001b[39;49m\u001b[39m\"\u001b[39;49m, config\u001b[39m=\u001b[39;49mconfig, settings\u001b[39m=\u001b[39;49mwandb\u001b[39m.\u001b[39;49mSettings(start_method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfork\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      2\u001b[0m config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config2.json\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1166\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[39massert\u001b[39;00m logger\n\u001b[1;32m   1165\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39minterrupted\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[0;32m-> 1166\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m   1167\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1168\u001b[0m     error_seen \u001b[39m=\u001b[39m e\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1144\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1142\u001b[0m except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1143\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1144\u001b[0m     run \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m   1145\u001b[0m     except_exit \u001b[39m=\u001b[39m wi\u001b[39m.\u001b[39msettings\u001b[39m.\u001b[39m_except_exit\n\u001b[1;32m   1146\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:744\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    741\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcommunicating run to backend with \u001b[39m\u001b[39m{\u001b[39;00mtimeout\u001b[39m}\u001b[39;00m\u001b[39m second timeout\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    743\u001b[0m run_init_handle \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39minterface\u001b[39m.\u001b[39mdeliver_run(run_proto)\n\u001b[0;32m--> 744\u001b[0m result \u001b[39m=\u001b[39m run_init_handle\u001b[39m.\u001b[39;49mwait(\n\u001b[1;32m    745\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    746\u001b[0m     on_progress\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_progress_init,\n\u001b[1;32m    747\u001b[0m     cancel\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    748\u001b[0m )\n\u001b[1;32m    749\u001b[0m \u001b[39mif\u001b[39;00m result:\n\u001b[1;32m    750\u001b[0m     run_result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mrun_result\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:283\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interface\u001b[39m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m MailboxError(\u001b[39m\"\u001b[39m\u001b[39mtransport failed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 283\u001b[0m found, abandoned \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_slot\u001b[39m.\u001b[39;49m_get_and_clear(timeout\u001b[39m=\u001b[39;49mwait_timeout)\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m found:\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39mif\u001b[39;00m on_progress \u001b[39mand\u001b[39;00m progress_handle \u001b[39mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:130\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_and_clear\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Optional[pb\u001b[39m.\u001b[39mResult], \u001b[39mbool\u001b[39m]:\n\u001b[1;32m    129\u001b[0m     found \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout\u001b[39m=\u001b[39;49mtimeout):\n\u001b[1;32m    131\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m             found \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wait\u001b[39m(\u001b[39mself\u001b[39m, timeout: \u001b[39mfloat\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout\u001b[39m=\u001b[39;49mtimeout)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config2.json\", \"r\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_and_Directions_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "validation_dataset = Aversion_and_Directions_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SentenceBaseline_Gaze_and_Direction_PredictionModel(config)\n",
    "model.to(device)\n",
    "# training_dataset[0]\n",
    "train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \"aversion_and_direction_larg_batch\")\n",
    "run_obj.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevanpan\u001b[0m (\u001b[33mgaze_prediction_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b165acd33048edac2bd129ed7ac68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670304100262, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/artemis.dgpsrv/ondemand27/evanpan/Gaze_project/training/wandb/run-20230418_100007-vgl6tnx8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/vgl6tnx8' target=\"_blank\">unique-bee-92</a></strong> to <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/vgl6tnx8' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/vgl6tnx8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000, mean: 0.5964974761009216, std: 0.49060043692588806\n",
      "training L: 0.7636961073059446\n",
      "validation L:0.7689137324044213\n",
      "Epoch 2001, mean: 0.7000855207443237, std: 0.4582207500934601\n",
      "training L: 0.76545473908605\n",
      "validation L:0.7659171881928002\n",
      "Epoch 2002, mean: 0.6969318985939026, std: 0.45958492159843445\n",
      "training L: 0.7615947322489782\n",
      "validation L:0.7623851474615961\n",
      "Epoch 2003, mean: 0.6235925555229187, std: 0.4844846725463867\n",
      "training L: 0.7680084032317864\n",
      "validation L:0.762308164834449\n",
      "Epoch 2004, mean: 0.6037479043006897, std: 0.4891184866428375\n",
      "training L: 0.7716686227492106\n",
      "validation L:0.7716569796785826\n",
      "Epoch 2005, mean: 0.6100979447364807, std: 0.48772838711738586\n",
      "training L: 0.7708461095801616\n",
      "validation L:0.7730724027016194\n",
      "Epoch 2006, mean: 0.6527360677719116, std: 0.4761010408401489\n",
      "training L: 0.7702094992859493\n",
      "validation L:0.7732597568345345\n",
      "Epoch 2007, mean: 0.6747980117797852, std: 0.4684508144855499\n",
      "training L: 0.7696129638568975\n",
      "validation L:0.7695217552467748\n",
      "Epoch 2008, mean: 0.6792886853218079, std: 0.46675053238868713\n",
      "training L: 0.7687486765577729\n",
      "validation L:0.7685967563306427\n",
      "Epoch 2009, mean: 0.690136194229126, std: 0.46243777871131897\n",
      "training L: 0.7689987021358811\n",
      "validation L:0.7698396419321869\n",
      "Epoch 2010, mean: 0.6628227233886719, std: 0.47274652123451233\n",
      "training L: 0.7710965457366501\n",
      "validation L:0.7712337903363602\n",
      "Epoch 2011, mean: 0.6548812985420227, std: 0.47540751099586487\n",
      "training L: 0.7733001405819733\n",
      "validation L:0.7726966462233229\n",
      "Epoch 2012, mean: 0.6477771401405334, std: 0.477663516998291\n",
      "training L: 0.7749214867606964\n",
      "validation L:0.7744532127754531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2013, mean: 0.6394800543785095, std: 0.480151891708374\n",
      "training L: 0.7756743898189092\n",
      "validation L:0.7756401977547731\n",
      "Epoch 2014, mean: 0.6344783306121826, std: 0.48157668113708496\n",
      "training L: 0.7753428178748879\n",
      "validation L:0.7770119938721615\n",
      "Epoch 2015, mean: 0.6487113237380981, std: 0.47737351059913635\n",
      "training L: 0.7757421687511182\n",
      "validation L:0.7768912798836576\n",
      "Epoch 2016, mean: 0.6484209299087524, std: 0.47746387124061584\n",
      "training L: 0.7760938260563196\n",
      "validation L:0.7752476759948539\n",
      "Epoch 2017, mean: 0.6456319689750671, std: 0.47832190990448\n",
      "training L: 0.7760918199074245\n",
      "validation L:0.775854221338377\n",
      "Epoch 2018, mean: 0.6496117115020752, std: 0.47709208726882935\n",
      "training L: 0.7763709893244655\n",
      "validation L:0.7750653899924546\n",
      "Epoch 2019, mean: 0.6668227314949036, std: 0.471349835395813\n",
      "training L: 0.7767098275815247\n",
      "validation L:0.776819389594618\n",
      "Epoch 2020, mean: 0.6682025790214539, std: 0.47085919976234436\n",
      "training L: 0.7758314223377502\n",
      "validation L:0.7777598101194084\n",
      "Epoch 2021, mean: 0.6588249802589417, std: 0.47410455346107483\n",
      "training L: 0.7767718158613881\n",
      "validation L:0.7777068849587121\n",
      "Epoch 2022, mean: 0.6374136209487915, std: 0.48074737191200256\n",
      "training L: 0.7783528990296348\n",
      "validation L:0.7791644884166055\n",
      "Epoch 2023, mean: 0.6343635320663452, std: 0.48160871863365173\n",
      "training L: 0.778638872459579\n",
      "validation L:0.7786785204634723\n",
      "Epoch 2024, mean: 0.6710343360900879, std: 0.4698379933834076\n",
      "training L: 0.7768668014472866\n",
      "validation L:0.7798167515152168\n",
      "Epoch 2025, mean: 0.6611727476119995, std: 0.47331157326698303\n",
      "training L: 0.7761622895971338\n",
      "validation L:0.7797231098097186\n",
      "Epoch 2026, mean: 0.6564614772796631, std: 0.4748898148536682\n",
      "training L: 0.7781231170128922\n",
      "validation L:0.7794362921209212\n",
      "Epoch 2027, mean: 0.6195813417434692, std: 0.4854903221130371\n",
      "training L: 0.7797468603286994\n",
      "validation L:0.7762613288125786\n",
      "Epoch 2028, mean: 0.6416837573051453, std: 0.4795062243938446\n",
      "training L: 0.7794087961986798\n",
      "validation L:0.7778498646249521\n",
      "Epoch 2029, mean: 0.6625481247901917, std: 0.4728409945964813\n",
      "training L: 0.7783026371896021\n",
      "validation L:0.7800237022150187\n",
      "Epoch 2030, mean: 0.6629285216331482, std: 0.4727100729942322\n",
      "training L: 0.7766106875997343\n",
      "validation L:0.7793090050925137\n",
      "Epoch 2031, mean: 0.6524209380149841, std: 0.4762020409107208\n",
      "training L: 0.7789343036660916\n",
      "validation L:0.7783036931504621\n",
      "Epoch 2032, mean: 0.6277073621749878, std: 0.4834163188934326\n",
      "training L: 0.780018092573225\n",
      "validation L:0.7786181816986468\n",
      "Epoch 2033, mean: 0.6515408158302307, std: 0.4764828085899353\n",
      "training L: 0.7801804673575304\n",
      "validation L:0.7791549284195871\n",
      "Epoch 2034, mean: 0.6663388013839722, std: 0.47152087092399597\n",
      "training L: 0.7792463213471649\n",
      "validation L:0.7799497283099224\n",
      "Epoch 2035, mean: 0.6399234533309937, std: 0.4800228476524353\n",
      "training L: 0.7795181155082834\n",
      "validation L:0.7807654881347725\n",
      "Epoch 2036, mean: 0.6459020972251892, std: 0.47823959589004517\n",
      "training L: 0.7799546602416523\n",
      "validation L:0.7810838216833084\n",
      "Epoch 2037, mean: 0.6417040228843689, std: 0.47950026392936707\n",
      "training L: 0.7804409366670664\n",
      "validation L:0.7795644030548801\n",
      "Epoch 2038, mean: 0.6364277005195618, std: 0.48102807998657227\n",
      "training L: 0.7804066702478382\n",
      "validation L:0.7804011238299416\n",
      "Epoch 2039, mean: 0.6495869755744934, std: 0.47709983587265015\n",
      "training L: 0.7805165999110277\n",
      "validation L:0.781269361371761\n",
      "Epoch 2040, mean: 0.6444907188415527, std: 0.47866788506507874\n",
      "training L: 0.7806394892288394\n",
      "validation L:0.781431560143052\n",
      "Epoch 2041, mean: 0.6488125920295715, std: 0.4773419499397278\n",
      "training L: 0.7811157340671474\n",
      "validation L:0.7827997456796063\n",
      "Epoch 2042, mean: 0.645926833152771, std: 0.4782320261001587\n",
      "training L: 0.7818790640465609\n",
      "validation L:0.7828322876180807\n",
      "Epoch 2043, mean: 0.6391491293907166, std: 0.4802478849887848\n",
      "training L: 0.7816370035783893\n",
      "validation L:0.7829044453599765\n",
      "Epoch 2044, mean: 0.6470523476600647, std: 0.4778871238231659\n",
      "training L: 0.7819700372226405\n",
      "validation L:0.7817544234607634\n",
      "Epoch 2045, mean: 0.6566212773323059, std: 0.47483712434768677\n",
      "training L: 0.7810511836956027\n",
      "validation L:0.7808197838863687\n",
      "Epoch 2046, mean: 0.6551874279975891, std: 0.4753077030181885\n",
      "training L: 0.7811188602892569\n",
      "validation L:0.782530907194935\n",
      "Epoch 2047, mean: 0.6430996060371399, std: 0.4790855944156647\n",
      "training L: 0.7820109103266242\n",
      "validation L:0.7825038056667892\n",
      "Epoch 2048, mean: 0.6305188536643982, std: 0.4826648533344269\n",
      "training L: 0.7827316375261795\n",
      "validation L:0.7831026725132512\n",
      "Epoch 2049, mean: 0.6455959677696228, std: 0.47833287715911865\n",
      "training L: 0.7827760332938192\n",
      "validation L:0.7839270049544618\n",
      "Epoch 2050, mean: 0.655743420124054, std: 0.475125789642334\n",
      "training L: 0.782031258688373\n",
      "validation L:0.7839778947631766\n",
      "Epoch 2051, mean: 0.6472819447517395, std: 0.4778164327144623\n",
      "training L: 0.781920183895849\n",
      "validation L:0.7856625335347469\n",
      "Epoch 2052, mean: 0.6445087194442749, std: 0.47866249084472656\n",
      "training L: 0.7832256943014\n",
      "validation L:0.7848637644575248\n",
      "Epoch 2053, mean: 0.6327495574951172, std: 0.48205605149269104\n",
      "training L: 0.7838557941793658\n",
      "validation L:0.7865880803610755\n",
      "Epoch 2054, mean: 0.6441598534584045, std: 0.47876763343811035\n",
      "training L: 0.7844509789409769\n",
      "validation L:0.7874360980643142\n",
      "Epoch 2055, mean: 0.6457805633544922, std: 0.47827666997909546\n",
      "training L: 0.7838359623798247\n",
      "validation L:0.7860552244622636\n",
      "Epoch 2056, mean: 0.6413753628730774, std: 0.479597270488739\n",
      "training L: 0.7840502562581011\n",
      "validation L:0.7860773541221124\n",
      "Epoch 2057, mean: 0.6372740864753723, std: 0.4807872176170349\n",
      "training L: 0.7845036387997767\n",
      "validation L:0.785968899862103\n",
      "Epoch 2058, mean: 0.6438356637954712, std: 0.4788651168346405\n",
      "training L: 0.7849007151405527\n",
      "validation L:0.7854511911565338\n",
      "Epoch 2059, mean: 0.6501497030258179, std: 0.47692301869392395\n",
      "training L: 0.7839515488305527\n",
      "validation L:0.7849017240409445\n",
      "Epoch 2060, mean: 0.6429398059844971, std: 0.4791333079338074\n",
      "training L: 0.7841867768995889\n",
      "validation L:0.7872132131038034\n",
      "Epoch 2061, mean: 0.6376139521598816, std: 0.48069003224372864\n",
      "training L: 0.7852454589353953\n",
      "validation L:0.788320125210688\n",
      "Epoch 2062, mean: 0.6461856961250305, std: 0.47815296053886414\n",
      "training L: 0.7855430009028337\n",
      "validation L:0.7886967305497696\n",
      "Epoch 2063, mean: 0.6444299221038818, std: 0.47868624329566956\n",
      "training L: 0.7839367443944558\n",
      "validation L:0.78865469759768\n",
      "Epoch 2064, mean: 0.6458390355110168, std: 0.47825878858566284\n",
      "training L: 0.7846600882466127\n",
      "validation L:0.7866774593499231\n",
      "Epoch 2065, mean: 0.6398199200630188, std: 0.48005303740501404\n",
      "training L: 0.7854265638388833\n",
      "validation L:0.7859165656567173\n",
      "Epoch 2066, mean: 0.6389105319976807, std: 0.4803169369697571\n",
      "training L: 0.7848914180905282\n",
      "validation L:0.7860139357096634\n",
      "Epoch 2067, mean: 0.6523624062538147, std: 0.4762207567691803\n",
      "training L: 0.7844065332786208\n",
      "validation L:0.7848874556263454\n",
      "Epoch 2068, mean: 0.6415982246398926, std: 0.47953152656555176\n",
      "training L: 0.7843841729590105\n",
      "validation L:0.785933396937402\n",
      "Epoch 2069, mean: 0.6515138149261475, std: 0.47649142146110535\n",
      "training L: 0.7859806116097505\n",
      "validation L:0.784659108678922\n",
      "Epoch 2070, mean: 0.6373550891876221, std: 0.48076409101486206\n",
      "training L: 0.7859658347470765\n",
      "validation L:0.7843206573458825\n",
      "Epoch 2071, mean: 0.6361733078956604, std: 0.4811001420021057\n",
      "training L: 0.7863912296551865\n",
      "validation L:0.7869277713716455\n",
      "Epoch 2072, mean: 0.6537242531776428, std: 0.4757828712463379\n",
      "training L: 0.7861708937848144\n",
      "validation L:0.7872657163202186\n",
      "Epoch 2073, mean: 0.6389060020446777, std: 0.4803183078765869\n",
      "training L: 0.7857741875821587\n",
      "validation L:0.7885817627441496\n",
      "Epoch 2074, mean: 0.6382577419281006, std: 0.4805052578449249\n",
      "training L: 0.7867663785000194\n",
      "validation L:0.7876498715448937\n",
      "Epoch 2075, mean: 0.6439121961593628, std: 0.4788421392440796\n",
      "training L: 0.7872435865197107\n",
      "validation L:0.7885953979675648\n",
      "Epoch 2076, mean: 0.6496477127075195, std: 0.477080762386322\n",
      "training L: 0.7869915137054184\n",
      "validation L:0.7868721352748735\n",
      "Epoch 2077, mean: 0.6291975378990173, std: 0.48302021622657776\n",
      "training L: 0.7875403292587566\n",
      "validation L:0.7855871263252038\n",
      "Epoch 2078, mean: 0.6538143157958984, std: 0.4757537841796875\n",
      "training L: 0.7867690877305429\n",
      "validation L:0.78669386877983\n",
      "Epoch 2079, mean: 0.6524366736412048, std: 0.4761969745159149\n",
      "training L: 0.7857085809620563\n",
      "validation L:0.7867269059282471\n",
      "Epoch 2080, mean: 0.6436331272125244, std: 0.4789259433746338\n",
      "training L: 0.7860463506207153\n",
      "validation L:0.7866953233321419\n",
      "Epoch 2081, mean: 0.6485154628753662, std: 0.4774344563484192\n",
      "training L: 0.787324196340579\n",
      "validation L:0.7879858638046251\n",
      "Epoch 2082, mean: 0.6551851630210876, std: 0.4753083884716034\n",
      "training L: 0.7864584145933277\n",
      "validation L:0.7865321885976686\n",
      "Epoch 2083, mean: 0.6417310237884521, std: 0.47949227690696716\n",
      "training L: 0.7860426553585278\n",
      "validation L:0.7884158002903242\n",
      "Epoch 2084, mean: 0.6430433392524719, std: 0.47910240292549133\n",
      "training L: 0.787500112925018\n",
      "validation L:0.7858049113085599\n",
      "Epoch 2085, mean: 0.6415148973464966, std: 0.4795560836791992\n",
      "training L: 0.7870712799861701\n",
      "validation L:0.7857908401812719\n",
      "Epoch 2086, mean: 0.6273742318153381, std: 0.48350420594215393\n",
      "training L: 0.7869380538192172\n",
      "validation L:0.7815448647960632\n",
      "Epoch 2087, mean: 0.6570737361907959, std: 0.4746876358985901\n",
      "training L: 0.7842580164830162\n",
      "validation L:0.7835665459856439\n",
      "Epoch 2088, mean: 0.6626583933830261, std: 0.4728030860424042\n",
      "training L: 0.7827882069330234\n",
      "validation L:0.7857466106202946\n",
      "Epoch 2089, mean: 0.622689962387085, std: 0.4847140312194824\n",
      "training L: 0.7858060436656797\n",
      "validation L:0.7786668196779165\n",
      "Epoch 2090, mean: 0.6231648921966553, std: 0.48459357023239136\n",
      "training L: 0.7854796522980569\n",
      "validation L:0.7843457174649682\n",
      "Epoch 2091, mean: 0.6520382761955261, std: 0.4763243496417999\n",
      "training L: 0.7852484769907533\n",
      "validation L:0.7830046711162381\n",
      "Epoch 2092, mean: 0.6644074320793152, std: 0.4721977412700653\n",
      "training L: 0.7826252103801326\n",
      "validation L:0.7766683368931815\n",
      "Epoch 2093, mean: 0.6650174260139465, std: 0.4719848930835724\n",
      "training L: 0.7788815033401878\n",
      "validation L:0.7747387361206279\n",
      "Epoch 2094, mean: 0.6105931401252747, std: 0.4876163601875305\n",
      "training L: 0.7797927702289948\n",
      "validation L:0.7698528477927827\n",
      "Epoch 2095, mean: 0.6301901936531067, std: 0.48275360465049744\n",
      "training L: 0.7775023822839096\n",
      "validation L:0.7716381321715675\n",
      "Epoch 2096, mean: 0.69545978307724, std: 0.46021294593811035\n",
      "training L: 0.7710068170934574\n",
      "validation L:0.7694653234754926\n",
      "Epoch 2097, mean: 0.679050087928772, std: 0.4668421149253845\n",
      "training L: 0.7706739853981639\n",
      "validation L:0.7753772188890944\n",
      "Epoch 2098, mean: 0.635032057762146, std: 0.48142173886299133\n",
      "training L: 0.7777766138713741\n",
      "validation L:0.7768805989806569\n",
      "Epoch 2099, mean: 0.6270028352737427, std: 0.4836019277572632\n",
      "training L: 0.7781830189907855\n",
      "validation L:0.7782205224918496\n",
      "Epoch 2100, mean: 0.6430996060371399, std: 0.47908562421798706\n",
      "training L: 0.7787656609193778\n",
      "validation L:0.7784499122805482\n",
      "Epoch 2101, mean: 0.6623973250389099, std: 0.4728928208351135\n",
      "training L: 0.7777161754763879\n",
      "validation L:0.7762347659889458\n",
      "Epoch 2102, mean: 0.6486302614212036, std: 0.47739875316619873\n",
      "training L: 0.7758915862456761\n",
      "validation L:0.7774175035134595\n",
      "Epoch 2103, mean: 0.6551018953323364, std: 0.47533559799194336\n",
      "training L: 0.775740201703693\n",
      "validation L:0.7803695983909711\n",
      "Epoch 2104, mean: 0.6570242047309875, std: 0.47470399737358093\n",
      "training L: 0.7770171326232551\n",
      "validation L:0.7792946556755294\n",
      "Epoch 2105, mean: 0.6290106773376465, std: 0.48307016491889954\n",
      "training L: 0.7787206092302341\n",
      "validation L:0.778708833809896\n",
      "Epoch 2106, mean: 0.6350343227386475, std: 0.4814210832118988\n",
      "training L: 0.7797414219053316\n",
      "validation L:0.7785829247608029\n",
      "Epoch 2107, mean: 0.6293821334838867, std: 0.48297080397605896\n",
      "training L: 0.7798920509471655\n",
      "validation L:0.7779346136877712\n",
      "Epoch 2108, mean: 0.6594327688217163, std: 0.47390052676200867\n",
      "training L: 0.7797505404519954\n",
      "validation L:0.7778946855485203\n",
      "Epoch 2109, mean: 0.6481800675392151, std: 0.47753867506980896\n",
      "training L: 0.7796450157845625\n",
      "validation L:0.7781083734503877\n",
      "Epoch 2110, mean: 0.649962842464447, std: 0.4769818186759949\n",
      "training L: 0.7807988201913969\n",
      "validation L:0.7783418238753534\n",
      "Epoch 2111, mean: 0.6476713418960571, std: 0.4776962101459503\n",
      "training L: 0.7815185434139349\n",
      "validation L:0.7784563961462906\n",
      "Epoch 2112, mean: 0.6578728556632996, std: 0.4744224548339844\n",
      "training L: 0.7818234291379272\n",
      "validation L:0.7795022939511138\n",
      "Epoch 2113, mean: 0.6453055739402771, std: 0.4784211814403534\n",
      "training L: 0.7819656681075111\n",
      "validation L:0.7801357010209613\n",
      "Epoch 2114, mean: 0.6472639441490173, std: 0.4778220057487488\n",
      "training L: 0.7825331727319862\n",
      "validation L:0.7816572611468362\n",
      "Epoch 2115, mean: 0.6329972147941589, std: 0.4819878339767456\n",
      "training L: 0.7824789263451719\n",
      "validation L:0.7815687660024618\n",
      "Epoch 2116, mean: 0.6429600715637207, std: 0.47912728786468506\n",
      "training L: 0.782715539388386\n",
      "validation L:0.7818373937139459\n",
      "Epoch 2117, mean: 0.6374564170837402, std: 0.4807351231575012\n",
      "training L: 0.7830017075150865\n",
      "validation L:0.7802358092344304\n",
      "Epoch 2118, mean: 0.6516398787498474, std: 0.4764513373374939\n",
      "training L: 0.7828055928513067\n",
      "validation L:0.78228705593908\n",
      "Epoch 2119, mean: 0.6353179812431335, std: 0.4813414216041565\n",
      "training L: 0.7828384892041194\n",
      "validation L:0.7830185340757769\n",
      "Epoch 2120, mean: 0.6420618891716003, std: 0.4793943464756012\n",
      "training L: 0.7838609282792806\n",
      "validation L:0.7830848074043568\n",
      "Epoch 2121, mean: 0.634975790977478, std: 0.48143747448921204\n",
      "training L: 0.7842432147263416\n",
      "validation L:0.783233423307897\n",
      "Epoch 2122, mean: 0.6501632332801819, std: 0.4769187867641449\n",
      "training L: 0.7842097542569213\n",
      "validation L:0.7827449262938186\n",
      "Epoch 2123, mean: 0.6399459838867188, std: 0.48001629114151\n",
      "training L: 0.7837946531125187\n",
      "validation L:0.7802007247553822\n",
      "Epoch 2124, mean: 0.6404547095298767, std: 0.47986769676208496\n",
      "training L: 0.7842768209318871\n",
      "validation L:0.7813332202507601\n",
      "Epoch 2125, mean: 0.6644839644432068, std: 0.4721710681915283\n",
      "training L: 0.7835665965854701\n",
      "validation L:0.7808035872287311\n",
      "Epoch 2126, mean: 0.6446843147277832, std: 0.4786094129085541\n",
      "training L: 0.7831827480285803\n",
      "validation L:0.7803795779909665\n",
      "Epoch 2127, mean: 0.6313719749450684, std: 0.48243334889411926\n",
      "training L: 0.7842163600321663\n",
      "validation L:0.7799765747623244\n",
      "Epoch 2128, mean: 0.6269803047180176, std: 0.48360782861709595\n",
      "training L: 0.7851558168355508\n",
      "validation L:0.7794129393256729\n",
      "Epoch 2129, mean: 0.683716356754303, std: 0.46502557396888733\n",
      "training L: 0.7800390439715434\n",
      "validation L:0.7781375813744758\n",
      "Epoch 2130, mean: 0.6248058676719666, std: 0.4841735363006592\n",
      "training L: 0.782773670152123\n",
      "validation L:0.7737265347375115\n",
      "Epoch 2131, mean: 0.6229397654533386, std: 0.48465076088905334\n",
      "training L: 0.7823603855126896\n",
      "validation L:0.773778324928899\n",
      "Epoch 2132, mean: 0.6563444137573242, std: 0.4749283492565155\n",
      "training L: 0.7814217357573817\n",
      "validation L:0.7771047648849791\n",
      "Epoch 2133, mean: 0.6881980895996094, std: 0.4632299244403839\n",
      "training L: 0.778231261269655\n",
      "validation L:0.7760617253785665\n",
      "Epoch 2134, mean: 0.6603264212608337, std: 0.4735989570617676\n",
      "training L: 0.7787838599449988\n",
      "validation L:0.7777729310178226\n",
      "Epoch 2135, mean: 0.6325807571411133, std: 0.48210257291793823\n",
      "training L: 0.7816977433030314\n",
      "validation L:0.7796777026724747\n",
      "Epoch 2136, mean: 0.6344310641288757, std: 0.4815898835659027\n",
      "training L: 0.7829989002331286\n",
      "validation L:0.7788750708116999\n",
      "Epoch 2137, mean: 0.6348002552986145, std: 0.4814866781234741\n",
      "training L: 0.7830301086342769\n",
      "validation L:0.7816161172643867\n",
      "Epoch 2138, mean: 0.656317412853241, std: 0.4749372601509094\n",
      "training L: 0.782537153063112\n",
      "validation L:0.7812233824502843\n",
      "Epoch 2139, mean: 0.6611412763595581, std: 0.4733222723007202\n",
      "training L: 0.7825300763335353\n",
      "validation L:0.7805884870105391\n",
      "Epoch 2140, mean: 0.6678289175033569, std: 0.4709925353527069\n",
      "training L: 0.7822590294448066\n",
      "validation L:0.77938818053102\n",
      "Epoch 2141, mean: 0.6552459001541138, std: 0.4752885699272156\n",
      "training L: 0.7805132932682748\n",
      "validation L:0.7799458206307506\n",
      "Epoch 2142, mean: 0.6643264293670654, std: 0.47222593426704407\n",
      "training L: 0.7790836207487131\n",
      "validation L:0.7820965991665589\n",
      "Epoch 2143, mean: 0.6489679217338562, std: 0.477293461561203\n",
      "training L: 0.7818529504091776\n",
      "validation L:0.7807673624400933\n",
      "Epoch 2144, mean: 0.629848062992096, std: 0.4828457236289978\n",
      "training L: 0.7831208609683393\n",
      "validation L:0.7834684255127884\n",
      "Epoch 2145, mean: 0.6627417206764221, std: 0.4727744162082672\n",
      "training L: 0.7825586884899587\n",
      "validation L:0.7811315679131456\n",
      "Epoch 2146, mean: 0.6716083288192749, std: 0.4696286618709564\n",
      "training L: 0.7793228910441361\n",
      "validation L:0.7811267428470179\n",
      "Epoch 2147, mean: 0.657478928565979, std: 0.4745534062385559\n",
      "training L: 0.7819702323957158\n",
      "validation L:0.7818318074340669\n",
      "Epoch 2148, mean: 0.6425076127052307, std: 0.47926202416419983\n",
      "training L: 0.7842080537881715\n",
      "validation L:0.7813080521649807\n",
      "Epoch 2149, mean: 0.6517366170883179, std: 0.4764205515384674\n",
      "training L: 0.7837268546865557\n",
      "validation L:0.7852741319280203\n",
      "Epoch 2150, mean: 0.645627498626709, std: 0.4783232510089874\n",
      "training L: 0.7828648287306114\n",
      "validation L:0.7875542456824405\n",
      "Epoch 2151, mean: 0.6334969401359558, std: 0.48184967041015625\n",
      "training L: 0.7850477496054199\n",
      "validation L:0.785527887236468\n",
      "Epoch 2152, mean: 0.6418142914772034, std: 0.47946763038635254\n",
      "training L: 0.7859742588142388\n",
      "validation L:0.7854645124069215\n",
      "Epoch 2153, mean: 0.6729769110679626, std: 0.46912631392478943\n",
      "training L: 0.7833219661572649\n",
      "validation L:0.7844231902310523\n",
      "Epoch 2154, mean: 0.659588098526001, std: 0.47384825348854065\n",
      "training L: 0.7832750832351714\n",
      "validation L:0.7869471533308494\n",
      "Epoch 2155, mean: 0.6449769139289856, std: 0.47852087020874023\n",
      "training L: 0.7856367147554992\n",
      "validation L:0.7870411454740687\n",
      "Epoch 2156, mean: 0.6376792192459106, std: 0.48067134618759155\n",
      "training L: 0.7857760142254491\n",
      "validation L:0.786545706638812\n",
      "Epoch 2157, mean: 0.661697268486023, std: 0.4731326699256897\n",
      "training L: 0.7853463829094869\n",
      "validation L:0.7854875684988014\n",
      "Epoch 2158, mean: 0.6634057760238647, std: 0.472545325756073\n",
      "training L: 0.7842667235791768\n",
      "validation L:0.7861622784524582\n",
      "Epoch 2159, mean: 0.6429150104522705, std: 0.4791406989097595\n",
      "training L: 0.7854520417959603\n",
      "validation L:0.7870487857170422\n",
      "Epoch 2160, mean: 0.6367923617362976, std: 0.48092448711395264\n",
      "training L: 0.7860247031630954\n",
      "validation L:0.787932043011745\n",
      "Epoch 2161, mean: 0.6464198231697083, std: 0.4780813455581665\n",
      "training L: 0.7860512008673695\n",
      "validation L:0.7883675537737884\n",
      "Epoch 2162, mean: 0.6481665968894958, std: 0.4775428771972656\n",
      "training L: 0.7868245871174719\n",
      "validation L:0.7877784852724219\n",
      "Epoch 2163, mean: 0.6532425284385681, std: 0.47593826055526733\n",
      "training L: 0.7863567043080356\n",
      "validation L:0.7889056985755688\n",
      "Epoch 2164, mean: 0.6510996222496033, std: 0.47662293910980225\n",
      "training L: 0.7869525635405411\n",
      "validation L:0.789283405348737\n",
      "Epoch 2165, mean: 0.6345233917236328, std: 0.48156410455703735\n",
      "training L: 0.7870330524063945\n",
      "validation L:0.7898123232599288\n",
      "Epoch 2166, mean: 0.6495261788368225, std: 0.4771188795566559\n",
      "training L: 0.7874233095946572\n",
      "validation L:0.7894915992349456\n",
      "Epoch 2167, mean: 0.6309285163879395, std: 0.4825538694858551\n",
      "training L: 0.7874856091320843\n",
      "validation L:0.78935780458752\n",
      "Epoch 2168, mean: 0.640877902507782, std: 0.4797435998916626\n",
      "training L: 0.7885987120524226\n",
      "validation L:0.789632356927646\n",
      "Epoch 2169, mean: 0.6454361081123352, std: 0.478381484746933\n",
      "training L: 0.7885479382643541\n",
      "validation L:0.791022050605886\n",
      "Epoch 2170, mean: 0.6515475511550903, std: 0.4764806926250458\n",
      "training L: 0.7880967772591504\n",
      "validation L:0.7886123959748457\n",
      "Epoch 2171, mean: 0.6500799059867859, std: 0.4769449830055237\n",
      "training L: 0.7885063262559064\n",
      "validation L:0.7900619584240477\n",
      "Epoch 2172, mean: 0.6410084366798401, std: 0.47970524430274963\n",
      "training L: 0.7888684896294333\n",
      "validation L:0.7903977761611714\n",
      "Epoch 2173, mean: 0.6478176712989807, std: 0.4776509702205658\n",
      "training L: 0.7892894276486181\n",
      "validation L:0.7900027233213098\n",
      "Epoch 2174, mean: 0.6462104916572571, std: 0.47814539074897766\n",
      "training L: 0.7891215183761625\n",
      "validation L:0.7901399308903836\n",
      "Epoch 2175, mean: 0.6390410661697388, std: 0.48027917742729187\n",
      "training L: 0.7889121054096521\n",
      "validation L:0.7848228623984161\n",
      "Epoch 2176, mean: 0.6230140924453735, std: 0.48463189601898193\n",
      "training L: 0.7825235401808252\n",
      "validation L:0.7742872427946117\n",
      "Epoch 2177, mean: 0.6734338998794556, std: 0.4689575731754303\n",
      "training L: 0.7754083654575891\n",
      "validation L:0.7681713362707868\n",
      "Epoch 2178, mean: 0.6982330083847046, std: 0.45902523398399353\n",
      "training L: 0.771056252511334\n",
      "validation L:0.7707760675813111\n",
      "Epoch 2179, mean: 0.6505008339881897, std: 0.47681233286857605\n",
      "training L: 0.772603144470283\n",
      "validation L:0.7706437613395065\n",
      "Epoch 2180, mean: 0.6146966814994812, std: 0.4866674244403839\n",
      "training L: 0.7754811534095428\n",
      "validation L:0.7706367558899914\n",
      "Epoch 2181, mean: 0.6740574240684509, std: 0.4687264859676361\n",
      "training L: 0.7656956614847583\n",
      "validation L:0.7657224763849816\n",
      "Epoch 2182, mean: 0.6838964819908142, std: 0.4649544060230255\n",
      "training L: 0.7617146337853384\n",
      "validation L:0.7673831104363105\n",
      "Epoch 2183, mean: 0.6825459003448486, std: 0.4654863178730011\n",
      "training L: 0.7645889093289545\n",
      "validation L:0.7695499985752838\n",
      "Epoch 2184, mean: 0.6767248511314392, std: 0.46772730350494385\n",
      "training L: 0.7668535012296516\n",
      "validation L:0.7723658764891641\n",
      "Epoch 2185, mean: 0.6723804473876953, std: 0.4693458378314972\n",
      "training L: 0.7688787143416811\n",
      "validation L:0.7734744217445058\n",
      "Epoch 2186, mean: 0.6447675824165344, std: 0.4785842299461365\n",
      "training L: 0.7717481334756336\n",
      "validation L:0.7725989013395006\n",
      "Epoch 2187, mean: 0.6340596675872803, std: 0.48169341683387756\n",
      "training L: 0.7741376952062926\n",
      "validation L:0.7718609018138602\n",
      "Epoch 2188, mean: 0.6640157699584961, std: 0.4723339080810547\n",
      "training L: 0.772995513226346\n",
      "validation L:0.7691950531697682\n",
      "Epoch 2189, mean: 0.6891367435455322, std: 0.46284744143486023\n",
      "training L: 0.7696633937654206\n",
      "validation L:0.7693383901554278\n",
      "Epoch 2190, mean: 0.6948227286338806, std: 0.46048298478126526\n",
      "training L: 0.7684483377140253\n",
      "validation L:0.7690780272296834\n",
      "Epoch 2191, mean: 0.6664896011352539, std: 0.4714675843715668\n",
      "training L: 0.7696184518284969\n",
      "validation L:0.7710402864516432\n",
      "Epoch 2192, mean: 0.6222262382507324, std: 0.48483118414878845\n",
      "training L: 0.7732416995479784\n",
      "validation L:0.772382936596034\n",
      "Epoch 2193, mean: 0.6083601713180542, std: 0.48811742663383484\n",
      "training L: 0.7746943299199124\n",
      "validation L:0.7776374989753811\n",
      "Epoch 2194, mean: 0.6349623203277588, std: 0.48144128918647766\n",
      "training L: 0.7756278399779479\n",
      "validation L:0.7752317703961391\n",
      "Epoch 2195, mean: 0.6797816753387451, std: 0.46656087040901184\n",
      "training L: 0.7727197045298091\n",
      "validation L:0.7722637793546147\n",
      "Epoch 2196, mean: 0.6884546875953674, std: 0.4631255865097046\n",
      "training L: 0.7715630907153328\n",
      "validation L:0.7747421302567276\n",
      "Epoch 2197, mean: 0.6446370482444763, std: 0.4786236882209778\n",
      "training L: 0.7761149342634848\n",
      "validation L:0.778020450962881\n",
      "Epoch 2198, mean: 0.6307709813117981, std: 0.48259657621383667\n",
      "training L: 0.7781943538610854\n",
      "validation L:0.7761984987012779\n",
      "Epoch 2199, mean: 0.6300416588783264, std: 0.4827936291694641\n",
      "training L: 0.7789980616761913\n",
      "validation L:0.7794041275775689\n",
      "Epoch 2200, mean: 0.6497085094451904, std: 0.4770617187023163\n",
      "training L: 0.7798402819471693\n",
      "validation L:0.7806934517378431\n",
      "Epoch 2201, mean: 0.6547799706459045, std: 0.47544050216674805\n",
      "training L: 0.7796615408569773\n",
      "validation L:0.7804130852327941\n",
      "Epoch 2202, mean: 0.6525154709815979, std: 0.4761717617511749\n",
      "training L: 0.7794814819715067\n",
      "validation L:0.7801817038622043\n",
      "Epoch 2203, mean: 0.6395768523216248, std: 0.4801237881183624\n",
      "training L: 0.7800008742604816\n",
      "validation L:0.7792647951505177\n",
      "Epoch 2204, mean: 0.6518672108650208, std: 0.4763789176940918\n",
      "training L: 0.7791424637835918\n",
      "validation L:0.7787153841121606\n",
      "Epoch 2205, mean: 0.626343309879303, std: 0.48377466201782227\n",
      "training L: 0.7787510064222672\n",
      "validation L:0.7768756921806867\n",
      "Epoch 2206, mean: 0.6161981225013733, std: 0.4863111674785614\n",
      "training L: 0.7774336776948566\n",
      "validation L:0.7751181887306515\n",
      "Epoch 2207, mean: 0.6497039794921875, std: 0.4770631492137909\n",
      "training L: 0.7766941836086749\n",
      "validation L:0.7745127931821487\n",
      "Epoch 2208, mean: 0.6664918661117554, std: 0.4714668095111847\n",
      "training L: 0.7750117125381526\n",
      "validation L:0.7749264620042172\n",
      "Epoch 2209, mean: 0.6759594678878784, std: 0.46801578998565674\n",
      "training L: 0.7763021780050616\n",
      "validation L:0.7751243121069005\n",
      "Epoch 2210, mean: 0.6261924505233765, std: 0.48381397128105164\n",
      "training L: 0.7776386974062062\n",
      "validation L:0.7736993898707115\n",
      "Epoch 2211, mean: 0.6166753172874451, std: 0.4861968755722046\n",
      "training L: 0.7783605168800274\n",
      "validation L:0.7769303158767634\n",
      "Epoch 2212, mean: 0.6281125545501709, std: 0.4833091199398041\n",
      "training L: 0.7787375063552505\n",
      "validation L:0.7765646223426013\n",
      "Epoch 2213, mean: 0.645800769329071, std: 0.4782705008983612\n",
      "training L: 0.7780297588792235\n",
      "validation L:0.7734583306256818\n",
      "Epoch 2214, mean: 0.6413123607635498, std: 0.47961583733558655\n",
      "training L: 0.776985461054555\n",
      "validation L:0.7715075984451563\n",
      "Epoch 2215, mean: 0.6622127294540405, std: 0.47295618057250977\n",
      "training L: 0.7753395187973201\n",
      "validation L:0.772849203202077\n",
      "Epoch 2216, mean: 0.6765762567520142, std: 0.4677834212779999\n",
      "training L: 0.7752289505354689\n",
      "validation L:0.7751659127115444\n",
      "Epoch 2217, mean: 0.6538503170013428, std: 0.4757421314716339\n",
      "training L: 0.7782318142236784\n",
      "validation L:0.7744639498160808\n",
      "Epoch 2218, mean: 0.6168756484985352, std: 0.48614877462387085\n",
      "training L: 0.7791666075308832\n",
      "validation L:0.7743251083817202\n",
      "Epoch 2219, mean: 0.6174159049987793, std: 0.486018568277359\n",
      "training L: 0.7801875245256236\n",
      "validation L:0.780240688974893\n",
      "Epoch 2220, mean: 0.6353674530982971, std: 0.48132750391960144\n",
      "training L: 0.78114445722464\n",
      "validation L:0.7804827323533583\n",
      "Epoch 2221, mean: 0.6619133353233337, std: 0.4730587303638458\n",
      "training L: 0.7809030238509478\n",
      "validation L:0.7816326128836129\n",
      "Epoch 2222, mean: 0.6446415185928345, std: 0.47862234711647034\n",
      "training L: 0.7815713033437446\n",
      "validation L:0.7811208541633021\n",
      "Epoch 2223, mean: 0.6447045803070068, std: 0.4786032736301422\n",
      "training L: 0.7821547574497877\n",
      "validation L:0.7809588195631161\n",
      "Epoch 2224, mean: 0.6477344036102295, std: 0.47767674922943115\n",
      "training L: 0.7825997777023581\n",
      "validation L:0.7815920202343294\n",
      "Epoch 2225, mean: 0.6486055254936218, std: 0.47740647196769714\n",
      "training L: 0.7831768681057121\n",
      "validation L:0.782440384429941\n",
      "Epoch 2226, mean: 0.6386809349060059, std: 0.4803833067417145\n",
      "training L: 0.7837209083624292\n",
      "validation L:0.7825450837103862\n",
      "Epoch 2227, mean: 0.6429105401039124, std: 0.4791420102119446\n",
      "training L: 0.7843708611369067\n",
      "validation L:0.7830208959609294\n",
      "Epoch 2228, mean: 0.6446505188941956, std: 0.47861963510513306\n",
      "training L: 0.7842037187628068\n",
      "validation L:0.7838171468320114\n",
      "Epoch 2229, mean: 0.6429128050804138, std: 0.47914138436317444\n",
      "training L: 0.7847632490097095\n",
      "validation L:0.7842015247061517\n",
      "Epoch 2230, mean: 0.637607216835022, std: 0.4806919991970062\n",
      "training L: 0.7850749236247907\n",
      "validation L:0.7838726027615734\n",
      "Epoch 2231, mean: 0.6356511116027832, std: 0.4812476634979248\n",
      "training L: 0.7854993456328037\n",
      "validation L:0.7832394552208211\n",
      "Epoch 2232, mean: 0.6462442278862, std: 0.47813504934310913\n",
      "training L: 0.7862813369017068\n",
      "validation L:0.7833389806293526\n",
      "Epoch 2233, mean: 0.6452763080596924, std: 0.47843003273010254\n",
      "training L: 0.786042741027701\n",
      "validation L:0.7854511911565338\n",
      "Epoch 2234, mean: 0.6529161334037781, std: 0.4760432541370392\n",
      "training L: 0.7860437132089366\n",
      "validation L:0.7852926464409232\n",
      "Epoch 2235, mean: 0.6373370885848999, std: 0.48076918721199036\n",
      "training L: 0.786625881144098\n",
      "validation L:0.7843274656521347\n",
      "Epoch 2236, mean: 0.6395093202590942, std: 0.4801434278488159\n",
      "training L: 0.7870555555562704\n",
      "validation L:0.7848431195012866\n",
      "Epoch 2237, mean: 0.6383612751960754, std: 0.4804754853248596\n",
      "training L: 0.787368709579207\n",
      "validation L:0.7852606349338964\n",
      "Epoch 2238, mean: 0.6463027596473694, std: 0.4781171381473541\n",
      "training L: 0.7873435784457126\n",
      "validation L:0.7851231958993178\n",
      "Epoch 2239, mean: 0.6399347186088562, std: 0.48001956939697266\n",
      "training L: 0.7879751656125669\n",
      "validation L:0.7848754473183677\n",
      "Epoch 2240, mean: 0.6502487659454346, std: 0.4768918454647064\n",
      "training L: 0.7878813001457523\n",
      "validation L:0.785781470438291\n",
      "Epoch 2241, mean: 0.6356128454208374, std: 0.48125842213630676\n",
      "training L: 0.7877773606868477\n",
      "validation L:0.7855940219407267\n",
      "Epoch 2242, mean: 0.6402521133422852, std: 0.4799269437789917\n",
      "training L: 0.7882242571747734\n",
      "validation L:0.785461357374226\n",
      "Epoch 2243, mean: 0.654525637626648, std: 0.4755232334136963\n",
      "training L: 0.7886912540329996\n",
      "validation L:0.7860967808733973\n",
      "Epoch 2244, mean: 0.6375644207000732, std: 0.4807042181491852\n",
      "training L: 0.7890621100397184\n",
      "validation L:0.7871152869175504\n",
      "Epoch 2245, mean: 0.6449229121208191, std: 0.4785372316837311\n",
      "training L: 0.788737050279463\n",
      "validation L:0.7867513951424856\n",
      "Epoch 2246, mean: 0.653055727481842, std: 0.4759984016418457\n",
      "training L: 0.7881860904604925\n",
      "validation L:0.7857833469874504\n",
      "Epoch 2247, mean: 0.6355745792388916, std: 0.4812692105770111\n",
      "training L: 0.7877627865795747\n",
      "validation L:0.7852858186698657\n",
      "Epoch 2248, mean: 0.6247720718383789, std: 0.4841822683811188\n",
      "training L: 0.7876584960422169\n",
      "validation L:0.785364437283022\n",
      "Epoch 2249, mean: 0.6566370129585266, std: 0.4748319387435913\n",
      "training L: 0.7865526714174155\n",
      "validation L:0.7856159109380232\n",
      "Epoch 2250, mean: 0.6490377187728882, std: 0.47727170586586\n",
      "training L: 0.786766987112155\n",
      "validation L:0.7867144053398818\n",
      "Epoch 2251, mean: 0.634649395942688, std: 0.4815288782119751\n",
      "training L: 0.7876376765375535\n",
      "validation L:0.7858015604582551\n",
      "Epoch 2252, mean: 0.6381294131278992, std: 0.4805421531200409\n",
      "training L: 0.7876768818608186\n",
      "validation L:0.7845958642424851\n",
      "Epoch 2253, mean: 0.6389735341072083, std: 0.4802987277507782\n",
      "training L: 0.7870352007052874\n",
      "validation L:0.7856195865556854\n",
      "Epoch 2254, mean: 0.636015772819519, std: 0.4811447262763977\n",
      "training L: 0.7876778462907994\n",
      "validation L:0.7851276688241762\n",
      "Epoch 2255, mean: 0.6592347025871277, std: 0.47396713495254517\n",
      "training L: 0.7872325056183463\n",
      "validation L:0.7853818061481392\n",
      "Epoch 2256, mean: 0.6538007855415344, std: 0.4757581651210785\n",
      "training L: 0.7862586434090224\n",
      "validation L:0.7856221641101828\n",
      "Epoch 2257, mean: 0.646827220916748, std: 0.4779563248157501\n",
      "training L: 0.7870657769021036\n",
      "validation L:0.7865960320584151\n",
      "Epoch 2258, mean: 0.6411885023117065, std: 0.4796523451805115\n",
      "training L: 0.7876548124617709\n",
      "validation L:0.7860566481874413\n",
      "Epoch 2259, mean: 0.6518964767456055, std: 0.47636955976486206\n",
      "training L: 0.7877522842139009\n",
      "validation L:0.7864067692913917\n",
      "Epoch 2260, mean: 0.6446505188941956, std: 0.47861960530281067\n",
      "training L: 0.7877852905731273\n",
      "validation L:0.7859114699747689\n",
      "Epoch 2261, mean: 0.6374698877334595, std: 0.4807312786579132\n",
      "training L: 0.7885132914873731\n",
      "validation L:0.7856708991703129\n",
      "Epoch 2262, mean: 0.6354823112487793, std: 0.4812951982021332\n",
      "training L: 0.7884777630752822\n",
      "validation L:0.784706356043376\n",
      "Epoch 2263, mean: 0.6476150751113892, std: 0.4777136445045471\n",
      "training L: 0.7886035536184604\n",
      "validation L:0.7862789561687162\n",
      "Epoch 2264, mean: 0.6531659960746765, std: 0.47596287727355957\n",
      "training L: 0.7891089683329988\n",
      "validation L:0.7868290820218\n",
      "Epoch 2265, mean: 0.643561065196991, std: 0.4789475202560425\n",
      "training L: 0.788755422923404\n",
      "validation L:0.787594340331023\n",
      "Epoch 2266, mean: 0.6258570551872253, std: 0.48390135169029236\n",
      "training L: 0.7891646530870375\n",
      "validation L:0.7861294914510109\n",
      "Epoch 2267, mean: 0.6324096918106079, std: 0.48214957118034363\n",
      "training L: 0.7897936537658543\n",
      "validation L:0.7873319465006818\n",
      "Epoch 2268, mean: 0.6422082185745239, std: 0.4793509542942047\n",
      "training L: 0.7898524682872337\n",
      "validation L:0.787820071346591\n",
      "Epoch 2269, mean: 0.6548069715499878, std: 0.4754317104816437\n",
      "training L: 0.7894433245916005\n",
      "validation L:0.7886039456825937\n",
      "Epoch 2270, mean: 0.6363263726234436, std: 0.4810567796230316\n",
      "training L: 0.7904814856313764\n",
      "validation L:0.787927445904283\n",
      "Epoch 2271, mean: 0.6405515074729919, std: 0.4798393249511719\n",
      "training L: 0.7904003653384253\n",
      "validation L:0.7887837856186538\n",
      "Epoch 2272, mean: 0.6428182125091553, std: 0.4791695773601532\n",
      "training L: 0.790513523390554\n",
      "validation L:0.7875428228878611\n",
      "Epoch 2273, mean: 0.6246055364608765, std: 0.48422515392303467\n",
      "training L: 0.7911812993983688\n",
      "validation L:0.7873892967582525\n",
      "Epoch 2274, mean: 0.6448711156845093, std: 0.478552907705307\n",
      "training L: 0.7909532847809713\n",
      "validation L:0.787676736724602\n",
      "Epoch 2275, mean: 0.6369386911392212, std: 0.480882853269577\n",
      "training L: 0.790304214504002\n",
      "validation L:0.7892147637041538\n",
      "Epoch 2276, mean: 0.6388024687767029, std: 0.4803481996059418\n",
      "training L: 0.7911633984067068\n",
      "validation L:0.7890943742210121\n",
      "Epoch 2277, mean: 0.6339133381843567, std: 0.48173409700393677\n",
      "training L: 0.7911643532884512\n",
      "validation L:0.7890114686983982\n",
      "Epoch 2278, mean: 0.6317028999328613, std: 0.4823431074619293\n",
      "training L: 0.7918513341452758\n",
      "validation L:0.7890392202820541\n",
      "Epoch 2279, mean: 0.6343973278999329, std: 0.4815993010997772\n",
      "training L: 0.7918804719981356\n",
      "validation L:0.7899711842441788\n",
      "Epoch 2280, mean: 0.6524839997291565, std: 0.47618183493614197\n",
      "training L: 0.7922389555536418\n",
      "validation L:0.7902055913534268\n",
      "Epoch 2281, mean: 0.6523061394691467, std: 0.4762387275695801\n",
      "training L: 0.791590627855418\n",
      "validation L:0.7898290892915476\n",
      "Epoch 2282, mean: 0.6392751932144165, std: 0.4802113473415375\n",
      "training L: 0.7921613223645985\n",
      "validation L:0.7896096115518116\n",
      "Epoch 2283, mean: 0.6296072006225586, std: 0.4829104244709015\n",
      "training L: 0.7926991877513565\n",
      "validation L:0.7894382638779277\n",
      "Epoch 2284, mean: 0.6412110328674316, std: 0.47964563965797424\n",
      "training L: 0.7919678138939786\n",
      "validation L:0.7895615863846239\n",
      "Epoch 2285, mean: 0.6334001421928406, std: 0.4818764626979828\n",
      "training L: 0.7932207172349568\n",
      "validation L:0.7887845512769204\n",
      "Epoch 2286, mean: 0.6408801674842834, std: 0.47974297404289246\n",
      "training L: 0.792655639214227\n",
      "validation L:0.789391051573828\n",
      "Epoch 2287, mean: 0.6421766877174377, std: 0.47936031222343445\n",
      "training L: 0.7914905206446472\n",
      "validation L:0.7893829177975176\n",
      "Epoch 2288, mean: 0.6421249508857727, std: 0.4793756604194641\n",
      "training L: 0.7928541941354423\n",
      "validation L:0.7893687221231214\n",
      "Epoch 2289, mean: 0.6499853730201721, std: 0.476974755525589\n",
      "training L: 0.7928304639759565\n",
      "validation L:0.7894675898780918\n",
      "Epoch 2290, mean: 0.6420979499816895, std: 0.4793836772441864\n",
      "training L: 0.7923883975872349\n",
      "validation L:0.7899344931968519\n",
      "Epoch 2291, mean: 0.6418638229370117, std: 0.4794529974460602\n",
      "training L: 0.7934819210225033\n",
      "validation L:0.7905321408383232\n",
      "Epoch 2292, mean: 0.6417985558509827, std: 0.479472279548645\n",
      "training L: 0.7922468135774\n",
      "validation L:0.7896252426091841\n",
      "Epoch 2293, mean: 0.6836983561515808, std: 0.465032696723938\n",
      "training L: 0.7854374829444085\n",
      "validation L:0.7751955491850215\n",
      "Epoch 2294, mean: 0.6395227909088135, std: 0.48013946413993835\n",
      "training L: 0.7766626348849719\n",
      "validation L:0.7676280327131542\n",
      "Epoch 2295, mean: 0.5714035034179688, std: 0.49487581849098206\n",
      "training L: 0.778314773749935\n",
      "validation L:0.7691471455092356\n",
      "Epoch 2296, mean: 0.6136409640312195, std: 0.48691505193710327\n",
      "training L: 0.777707031134251\n",
      "validation L:0.7725015615668595\n",
      "Epoch 2297, mean: 0.6963781714439392, std: 0.45982182025909424\n",
      "training L: 0.771565949940776\n",
      "validation L:0.7606007263801934\n",
      "Epoch 2298, mean: 0.7099673748016357, std: 0.45377764105796814\n",
      "training L: 0.7676491446316966\n",
      "validation L:0.7691912588759893\n",
      "Epoch 2299, mean: 0.646379292011261, std: 0.4780937433242798\n",
      "training L: 0.7728603198570025\n",
      "validation L:0.7629399219161308\n",
      "Epoch 2300, mean: 0.6135464310646057, std: 0.4869370758533478\n",
      "training L: 0.7745763148036395\n",
      "validation L:0.7667084928537677\n",
      "Epoch 2301, mean: 0.6282948851585388, std: 0.48326075077056885\n",
      "training L: 0.777653527307131\n",
      "validation L:0.7762208205008903\n",
      "Epoch 2302, mean: 0.6405267119407654, std: 0.4798465669155121\n",
      "training L: 0.7788991127290573\n",
      "validation L:0.7772455543168761\n",
      "Epoch 2303, mean: 0.6460123658180237, std: 0.47820591926574707\n",
      "training L: 0.7797079879131883\n",
      "validation L:0.7783164906363707\n",
      "Epoch 2304, mean: 0.6693055629730225, std: 0.47046372294425964\n",
      "training L: 0.7783997942667964\n",
      "validation L:0.777644612398278\n",
      "Epoch 2305, mean: 0.6570062041282654, std: 0.47470998764038086\n",
      "training L: 0.7770498832764158\n",
      "validation L:0.7767967842628474\n",
      "Epoch 2306, mean: 0.6366775631904602, std: 0.48095712065696716\n",
      "training L: 0.7798342450591345\n",
      "validation L:0.7715083880232614\n",
      "Epoch 2307, mean: 0.625956118106842, std: 0.483875572681427\n",
      "training L: 0.7784508422724778\n",
      "validation L:0.7707238163206611\n",
      "Epoch 2308, mean: 0.66010582447052, std: 0.473673552274704\n",
      "training L: 0.7783169595422847\n",
      "validation L:0.7733299832615809\n",
      "Epoch 2309, mean: 0.6884209513664246, std: 0.4631393253803253\n",
      "training L: 0.7746959905281667\n",
      "validation L:0.7710808709001714\n",
      "Epoch 2310, mean: 0.6812403202056885, std: 0.4659961760044098\n",
      "training L: 0.7743827853475798\n",
      "validation L:0.7764181723271641\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m train_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(training_dataset, config[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m valid_dataloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(validation_dataset, config[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m], \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 26\u001b[0m train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \u001b[39m\"\u001b[39;49m\u001b[39maversion_and_direction_larg_batch\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m2000\u001b[39;49m)\n\u001b[1;32m     27\u001b[0m run_obj\u001b[39m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m, in \u001b[0;36mtrain_model_with_for_direction\u001b[0;34m(model, config, train_data, valid_data, wandb, model_name, start)\u001b[0m\n\u001b[1;32m     35\u001b[0m prediction_std \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     36\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfor\u001b[39;00m _, (X, [Y, Y_vel, Y_dir]) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_data):\n\u001b[1;32m     38\u001b[0m     train_batch_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     39\u001b[0m     X, Y, Y_vel, Y_dir\u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), Y\u001b[39m.\u001b[39mto(device), Y_vel\u001b[39m.\u001b[39mto(device), Y_dir\u001b[39m.\u001b[39mto(device)   \n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/Gaze_project/Dataset_Util/dataloader.py:270\u001b[0m, in \u001b[0;36mAversion_and_Directions_SelfTap111.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m# output_target\u001b[39;00m\n\u001b[1;32m    269\u001b[0m output_target \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(aversion_label_path)\n\u001b[0;32m--> 270\u001b[0m output_target_2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(aversion_direction_path)\n\u001b[1;32m    271\u001b[0m \u001b[39m# see if we need to concat any thing\u001b[39;00m\n\u001b[1;32m    272\u001b[0m input_audio_on_screen \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(onscreen_audio_feature_path)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/npyio.py:438\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n\u001b[0;32m--> 438\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot load file containing pickled data \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mwhen allow_pickle=False\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m         \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39mload(fid, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/aversion_and_direction_larg_batch/config.json\", \"r\"))\n",
    "model = SentenceBaseline_Gaze_and_Direction_PredictionModel(config)\n",
    "config[\"load_model\"] = True\n",
    "if config[\"wandb\"]:\n",
    "    wandb.login()\n",
    "    if config[\"load_model\"]:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, save_code=True,\n",
    "            resume='allow', id='vgl6tnx8')\n",
    "        # checkpoint_name = \"gaze_prediction_team/gaze_prediction/8w9fyxan\"\n",
    "        checkpoint_path = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/aversion_and_direction_larg_batch/time=2023-04-18 09:01:11.503968_epoch=1986.pt\"\n",
    "        wandb.restore(checkpoint_path)\n",
    "        pretrained_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_weights(pretrained_dict)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "else:\n",
    "    run_obj = None\n",
    "\n",
    "training_dataset = Aversion_and_Directions_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "validation_dataset = Aversion_and_Directions_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "\n",
    "train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \"aversion_and_direction_larg_batch\", 2000)\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for sentence + word + audio. but also uses velocity as a loss function, also predict gaze Direction but only up and down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mevanpan\u001b[0m (\u001b[33mgaze_prediction_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/artemis.dgpsrv/ondemand27/evanpan/Gaze_project/training/wandb/run-20230418_154651-hhcg5110</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/hhcg5110' target=\"_blank\">stilted-night-108</a></strong> to <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/hhcg5110' target=\"_blank\">https://wandb.ai/gaze_prediction_team/gaze_prediction/runs/hhcg5110</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch 1, mean: 1.0, std: 0.0\n",
      "training L: 0.46306652828977884\n",
      "validation L:0.49493222482845023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[39m# training_dataset[0]\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \u001b[39m\"\u001b[39;49m\u001b[39maversion_and_direction_larg_batch_simple_dir\u001b[39;49m\u001b[39m\"\u001b[39;49m, num_of_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m     17\u001b[0m run_obj\u001b[39m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[11], line 81\u001b[0m, in \u001b[0;36mtrain_model_with_for_direction\u001b[0;34m(model, config, train_data, valid_data, wandb, model_name, start, num_of_classes)\u001b[0m\n\u001b[1;32m     78\u001b[0m total_train_loss \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_data)\n\u001b[1;32m     79\u001b[0m total_aversion_predicted \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m total_prediction_counter\n\u001b[0;32m---> 81\u001b[0m \u001b[39mfor\u001b[39;00m _, (X, [Y, Y_vel, Y_dir]) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(valid_data):\n\u001b[1;32m     82\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     83\u001b[0m         valid_batch_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/Gaze_project/Dataset_Util/dataloader.py:277\u001b[0m, in \u001b[0;36mAversion_and_Directions_SelfTap111.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m# see if we need to concat any thing\u001b[39;00m\n\u001b[1;32m    276\u001b[0m input_audio_on_screen \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(onscreen_audio_feature_path)\n\u001b[0;32m--> 277\u001b[0m input_audio_off_screen \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(offscreen_audio_feature_path)\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_only:\n\u001b[1;32m    279\u001b[0m     missing_frames \u001b[39m=\u001b[39m output_target\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m input_audio_on_screen\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mopen_memmap(file, mode\u001b[39m=\u001b[39mmmap_mode,\n\u001b[1;32m    430\u001b[0m                                   max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[1;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 432\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(fid, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[1;32m    433\u001b[0m                                  pickle_kwargs\u001b[39m=\u001b[39;49mpickle_kwargs,\n\u001b[1;32m    434\u001b[0m                                  max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[1;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/numpy/lib/format.py:790\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    788\u001b[0m     \u001b[39mif\u001b[39;00m isfileobj(fp):\n\u001b[1;32m    789\u001b[0m         \u001b[39m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49mfromfile(fp, dtype\u001b[39m=\u001b[39;49mdtype, count\u001b[39m=\u001b[39;49mcount)\n\u001b[1;32m    791\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    792\u001b[0m         \u001b[39m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[1;32m    793\u001b[0m         \u001b[39m# memory-intensive way.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[39m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[1;32m    802\u001b[0m         \u001b[39m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[1;32m    803\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mndarray(count, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_obj = None\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config2.json\", \"r\"))\n",
    "run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# config[\"wandb\"]=False\n",
    "print(device)\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "training_dataset = Aversion_and_Directions_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True, simple_dir=True)\n",
    "validation_dataset = Aversion_and_Directions_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True, simple_dir=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SentenceBaseline_Gaze_and_Direction_PredictionModel_only_updown(config)\n",
    "model.to(device)\n",
    "# training_dataset[0]\n",
    "train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \"aversion_and_direction_larg_batch_simple_dir\", num_of_classes=3)\n",
    "run_obj.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/aversion_and_direction_larg_batch_simple_dir/config.json\", \"r\"))\n",
    "model = SentenceBaseline_Gaze_and_Direction_PredictionModel_only_updown(config)\n",
    "config[\"load_model\"] = True\n",
    "if config[\"wandb\"]:\n",
    "    wandb.login()\n",
    "    if config[\"load_model\"]:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, save_code=True,\n",
    "            resume='allow', id='l9vqiqwb')\n",
    "        # checkpoint_name = \"gaze_prediction_team/gaze_prediction/8w9fyxan\"\n",
    "        checkpoint_path = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/aversion_and_direction_larg_batch_simple_dir/time=2023-04-19 08:28:15.969363_epoch=973.pt\"\n",
    "        wandb.restore(checkpoint_path)\n",
    "        pretrained_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_weights(pretrained_dict)\n",
    "        model.to(device)\n",
    "    else:\n",
    "        run_obj = wandb.init(project=\"gaze_prediction\", config=config, settings=wandb.Settings(start_method=\"fork\"))\n",
    "else:\n",
    "    run_obj = None\n",
    "\n",
    "training_dataset = Aversion_and_Directions_SelfTap111(dataset_location, training_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "validation_dataset = Aversion_and_Directions_SelfTap111(dataset_location, testing_set, sentence_and_word_timing=True, velocity_label=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "\n",
    "train_model_with_for_direction(model, config, train_dataloader, valid_dataloader, run_obj, \"aversion_and_direction_larg_batch\", 2000)\n",
    "run_obj.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop for concatenating input so the model is trained on longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_concat(model, config, train_data, valid_data, wandb, model_name):\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.train() \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    training_loss = []\n",
    "    valid_loss = []\n",
    "    training_f1 = []\n",
    "    valid_f1 = []\n",
    "    aversion_vs_start = []\n",
    "    count = 0\n",
    "    # f1_score = BinaryF1Score(num_classes=2).to(device)\n",
    "    f1_score = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\").to(device)\n",
    "    for epoch in range(1, config['epochs'] + 1):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "        total_aversion_predicted = 0\n",
    "        total_train_f1 = 0\n",
    "        total_valid_f1 = 0\n",
    "        train_batch_counter = 0\n",
    "        valid_batch_counter = 0\n",
    "        total_prediction_counter = 0\n",
    "        prediction_mean = 0\n",
    "        prediction_std = 0\n",
    "        model.zero_grad()\n",
    "        for _, (X, Y) in enumerate(train_data):\n",
    "            train_batch_counter += 1\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            optimiser.zero_grad()\n",
    "            if \"Transformer\" in config[\"model_type\"]:\n",
    "                all_zero = torch.zeros(Y.shape).to(device)\n",
    "                pred = model(X, all_zero)\n",
    "            else:\n",
    "                pred = model(X)\n",
    "            loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_train_loss += loss.item()\n",
    "            # binary_pred = torch.round(pred)\n",
    "            binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "            prediction_mean = torch.mean(binary_pred.float()).item()\n",
    "            prediction_std = torch.std(binary_pred.float()).item()            \n",
    "            f1_train = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "            total_aversion_predicted += torch.sum(binary_pred).item()\n",
    "            total_prediction_counter += binary_pred.size()[0] * binary_pred.size()[1] \n",
    "            total_train_f1 += f1_train\n",
    "            del X, Y, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        total_train_f1 /= train_batch_counter\n",
    "        total_train_loss /= len(train_data)\n",
    "        total_aversion_predicted /= total_prediction_counter\n",
    "\n",
    "        for _, (X, Y) in enumerate(valid_data):\n",
    "            with torch.no_grad():\n",
    "                valid_batch_counter += 1\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                if \"Transformer\" in config[\"model_type\"]:\n",
    "                    all_zero = torch.zeros(Y.shape).to(device)\n",
    "                    pred = model(X, all_zero)\n",
    "                else:\n",
    "                    pred = model(X)\n",
    "                loss = loss_fn(pred.transpose(2, 1), Y.long())\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "                # binary_pred = torch.round(pred)\n",
    "                binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "                f1_valid = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "                total_valid_f1 += f1_valid\n",
    "                del X, Y, pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        total_valid_f1 /= valid_batch_counter\n",
    "        total_valid_loss /= len(valid_data)\n",
    "\n",
    "        if config['wandb']:\n",
    "            wandb.log({'training loss': total_train_loss,\n",
    "                        'validation_loss': total_valid_loss,\n",
    "                        'training_f1': total_train_f1,\n",
    "                        'validation_f1': total_valid_f1, \n",
    "                        \"percentage_predicted_aversion\": total_aversion_predicted})\n",
    "        training_loss.append(total_train_loss)\n",
    "        valid_loss.append(total_valid_loss)\n",
    "        training_f1.append(total_train_f1)\n",
    "        valid_f1.append(total_valid_f1)\n",
    "        aversion_vs_start.append(total_aversion_predicted)\n",
    "        if total_valid_f1 == max(valid_f1):\n",
    "            try:\n",
    "                os.mkdir(os.path.join(*[model_save_location, model_name]))\n",
    "            except:\n",
    "                pass\n",
    "            config_save_path = os.path.join(*[model_save_location, model_name, \"config.json\"])\n",
    "            json.dump(config, open(config_save_path, \"w\"))\n",
    "            file_name = f'time={datetime.now()}_epoch={epoch}.pt'\n",
    "            save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        if config['early_stopping']>0:\n",
    "            if epoch > 1:\n",
    "                if total_valid_f1 < np.mean(valid_f1[epoch - 7:epoch - 2]):\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count = 0\n",
    "            if count >= config['early_stopping']:\n",
    "                print('\\n\\nStopping early due to decrease in performance on validation set\\n\\n')\n",
    "                break \n",
    "        if count == 0:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1))\n",
    "        else:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}, model have not improved for {} iterations\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1, count))\n",
    "    if config['wandb']:\n",
    "        save_path = os.path.join(*[model_save_location, model_name, file_name])\n",
    "        wandb.save(save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help`\n",
      "       for usage information.\n",
      "\n",
      "/usr/bin/nvidia-modprobe: unrecognized option: \"-s\"\n",
      "\n",
      "ERROR: Invalid commandline, please run `/usr/bin/nvidia-modprobe --help`\n",
      "       for usage information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/sentence_config.json\", \"r\"))\n",
    "# obtain the dataset\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "validation_dataset = Aversion_SelfTap111(dataset_location, testing_set, word_timing=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "model = SentenceBaseline_GazePredictionModel(config)\n",
    "checkpoint_path = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models/time=2023-04-05 02:37:34.205141_epoch=200.pt\"\n",
    "pretrained_dict = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(pretrained_dict)\n",
    "# train_model(model, config, train_dataloader, valid_dataloader, run_obj, model_save_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m _, (X, Y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(valid_data):\n\u001b[1;32m      2\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      3\u001b[0m         valid_batch_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_data' is not defined"
     ]
    }
   ],
   "source": [
    "for _, (X, Y) in enumerate(valid_data):\n",
    "    with torch.no_grad():\n",
    "        valid_batch_counter += 1\n",
    "        X, Y = X.to(device), Y.to(device)\n",
    "        if \"Transformer\" in config[\"model_type\"]:\n",
    "            all_zero = torch.zeros(Y.shape).to(device)\n",
    "            pred = model(X, all_zero)\n",
    "        else:\n",
    "            pred = model(X)\n",
    "        # binary_pred = torch.round(pred)\n",
    "        binary_pred = torch.argmax(pred, axis=2, keepdim=True)\n",
    "        f1_valid = f1_score(binary_pred, torch.unsqueeze(Y, axis=2)).item()\n",
    "        total_valid_f1 += f1_valid\n",
    "        del X, Y, pred\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 2 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m y \u001b[39m=\u001b[39m validation_dataset[i][\u001b[39m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m pred \u001b[39m=\u001b[39m model(torch\u001b[39m.\u001b[39munsqueeze(x, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49msoftmax(pred, dim\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)[:, :, \u001b[39m2\u001b[39;49m]\n\u001b[1;32m      7\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m plt\u001b[39m.\u001b[39mplot(pred[\u001b[39m0\u001b[39m, :, \u001b[39m0\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 2 with size 2"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "i = 11\n",
    "x = torch.from_numpy(validation_dataset[i][0]).to(device)\n",
    "y = validation_dataset[i][1]\n",
    "pred = model(torch.unsqueeze(x, axis=0))\n",
    "pred = torch.softmax(pred, dim=2)\n",
    "pred = pred.cpu().detach().numpy()\n",
    "plt.plot(pred[0, :, 0], label=\"prediction\")\n",
    "plt.plot(y, label=\"label\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaligaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
