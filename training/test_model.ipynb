{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "from torchmetrics.classification import BinaryF1Score, F1Score\n",
    "import wandb\n",
    "from tqdm import trange\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '/Users/evanpan/Documents/GitHub/EvansToolBox/Utils')\n",
    "sys.path.insert(0, '/Users/evanpan/Desktop/openpose/python/')\n",
    "sys.path.insert(0, '/scratch/ondemand27/evanpan/EvansToolBox/Utils/')\n",
    "sys.path.insert(0, '/scratch/ondemand27/evanpan/Gaze_project/')\n",
    "\n",
    "# from training.model import *\n",
    "from Dataset_Util.dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%aimport prototypes.InputDataStructures\n",
    "%aimport Speech_Data_util\n",
    "%aimport Signal_processing_utils\n",
    "%aimport Geometry_Util\n",
    "%aimport prototypes.MVP.MVP_static_saliency_list\n",
    "%aimport prototypes.EyeCatch.Saccade_model_with_internal_model\n",
    "%aimport prototypes.InputDataStructures\n",
    "%aimport prototypes.Jin2019.EyeHeadDecomposition\n",
    "%aimport prototypes.Optimization_based_head_eye_seperator.Baseline_optimization\n",
    "%aimport prototypes.Boccignone2020.Improved_gaze_target_planner\n",
    "%aimport prototypes.MVP.MVP_gaze_path_planner\n",
    "%aimport prototypes.JaliNeck.JaliNeck\n",
    "%aimport Dataset_Util.dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Explicit_context_GazePredictionModel_mel_only_with_word_vec(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "        # initialize model\n",
    "        super(Explicit_context_GazePredictionModel_mel_only_with_word_vec, self).__init__()\n",
    "        self.device = torch.device(config[\"device\"])\n",
    "        print(\"The model is on:\", self.device)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.num_layers = config[\"num_layers\"]\n",
    "        self.config = config\n",
    "        # the feature of each speaker are encoded with a separate Linear Layer\n",
    "        self.word_vec_layer_self = nn.Linear(768, 64)\n",
    "        self.word_vec_layer_other = nn.Linear(768, 64)\n",
    "        self.input_layer_self = nn.Linear(int(config[\"input_dims\"]/2 - 6 - 14), config[\"input_layer_out\"])\n",
    "        self.input_layer_other = nn.Linear(int(config[\"input_dims\"]/2 - 6 - 14), config[\"input_layer_out\"])\n",
    "        self.pos_layer_self = nn.Linear(14, config[\"pos_feature_size\"])\n",
    "        self.pos_layer_other = nn.Linear(14, config[\"pos_feature_size\"])\n",
    "\n",
    "        # the Recurrent Layer will take care of the next step\n",
    "        self.lstm_hidden_dims = config[\"lstm_output_feature_size\"]\n",
    "        self.num_lstm_layer = config[\"lstm_layer_num\"]\n",
    "        self.frames_ahead = config[\"frames_ahead\"]\n",
    "        self.frames_behind = config[\"frames_behind\"]\n",
    "        self.lstm = nn.LSTM(2 * (64 + config[\"input_layer_out\"] + 6 + config[\"pos_feature_size\"]) * (self.frames_ahead + self.frames_behind + 1), \n",
    "                            self.lstm_hidden_dims, \n",
    "                            self.num_lstm_layer, \n",
    "                            batch_first=True)\n",
    "        h_0_0 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n",
    "        c_0_0 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n",
    "        h_0_1 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n",
    "        c_0_1 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n",
    "        self.h_0_0 = torch.nn.Parameter(h_0_0)\n",
    "        self.h_0_1 = torch.nn.Parameter(h_0_1)\n",
    "        self.c_0_0 = torch.nn.Parameter(c_0_0)\n",
    "        self.c_0_1 = torch.nn.Parameter(c_0_1)\n",
    "\n",
    "        self.bn_layer_1 = nn.BatchNorm1d(config[\"output_layer_1_hidden\"], track_running_stats=False)\n",
    "        self.bn_layer_2 = nn.BatchNorm1d(config[\"output_layer_2_hidden\"], track_running_stats=False)\n",
    "        # output layers\n",
    "        self.output_layer_1 = nn.Linear(self.lstm_hidden_dims, config[\"output_layer_1_hidden\"])\n",
    "        self.output_layer_1 = nn.Sequential(self.output_layer_1, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_2 = nn.Linear(config[\"output_layer_1_hidden\"], config[\"output_layer_2_hidden\"])\n",
    "        self.output_layer_2 = nn.Sequential(self.output_layer_2, self.activation, nn.Dropout(self.config[\"dropout\"]))\n",
    "        self.output_layer_3 = nn.Linear(config[\"output_layer_2_hidden\"], 1)\n",
    "        self.output_layer_3 = nn.Sequential(self.output_layer_3, self.activation)\n",
    "\n",
    "        # audio_filler = torch.tensor([[[-36.04365338911715,0.0,0.0,0.0,0.0,0.0,-3.432169450445466e-14,0.0,0.0,0.0,9.64028691651994e-15,0.0,0.0,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715,-36.04365338911715]]]).to(self.device)\n",
    "        # text_filler = torch.ones([1, 1, 772]).to(self.device) * -15\n",
    "        # text_filler[:, :, -4:] = 0\n",
    "        # self.filler = torch.concat([audio_filler, text_filler], axis=2)\n",
    "    def concate_frames(self, input_feature):\n",
    "        # here I expect the \n",
    "        padding_front = torch.zeros([input_feature.shape[0], self.frames_ahead, input_feature.shape[2]]).to(self.device)\n",
    "        padding_back = torch.zeros([input_feature.shape[0], self.frames_behind, input_feature.shape[2]]).to(self.device)\n",
    "        padded_input_audio = torch.cat([padding_front, input_feature, padding_back], dim=1)\n",
    "        window_audio = []\n",
    "        for i in range(0, input_feature.shape[1]):\n",
    "            window_count = i + self.frames_ahead\n",
    "            current_window = padded_input_audio[:, window_count-self.frames_ahead:window_count+self.frames_behind+1]\n",
    "            s = current_window.shape\n",
    "            current_window = current_window.view((s[0], s[1] * s[2]))\n",
    "            current_window = torch.unsqueeze(current_window, 1)\n",
    "            window_audio.append(current_window)\n",
    "        rtv = torch.cat(window_audio, dim=1)\n",
    "        return rtv\n",
    "    def forward(self, input_feature, initial_state):\n",
    "        feature_size = int(input_feature.size()[2] / 2)\n",
    "        mod_audio_self = input_feature[:, :, :feature_size]\n",
    "        mod_audio_other = input_feature[:, :, feature_size:]\n",
    "        \n",
    "        word_vector_feature_self = mod_audio_self[:, :, :768] # these are word embeddings\n",
    "        text_feature_self = mod_audio_self[:, :, -6:] # these are sentence and word boundaries\n",
    "        pos_features_self = mod_audio_self[:, :, 794:-6] # these are pos features\n",
    "        mod_audio_self = mod_audio_self[:, :, 768:794] # these are mel features\n",
    "        word_vector_feature_other = mod_audio_other[:, :, :768] # these are word embeddings\n",
    "        text_feature_other = mod_audio_other[:, :, -6:] # these are sentence and word boundaries\n",
    "        pos_features_other = mod_audio_other[:, :, 794:-6] # these are pos features\n",
    "        mod_audio_other = mod_audio_other[:, :, 768:794] # these are mel features\n",
    "        \n",
    "        x1 = self.activation(self.input_layer_self(mod_audio_self)) # audio going though a linear layer\n",
    "        x2 = self.activation(self.input_layer_other(mod_audio_other)) # audio going though a linear layer\n",
    "        x1_word_vector_feature = self.activation(self.word_vec_layer_self(word_vector_feature_self)) # word embedding also going through a linear layer\n",
    "        x2_word_vector_feature = self.activation(self.word_vec_layer_other(word_vector_feature_other)) # word embedding also going through a linear layer\n",
    "        pos_features_self = self.activation(self.pos_layer_self(pos_features_self))\n",
    "        pos_features_other = self.activation(self.pos_layer_other(pos_features_other))\n",
    "        x1_windowed = self.concate_frames(x1)\n",
    "        x2_windowed = self.concate_frames(x2)\n",
    "        x1_word_vector_windowed = self.concate_frames(x1_word_vector_feature)\n",
    "        x2_word_vector_windowed = self.concate_frames(x2_word_vector_feature)\n",
    "        \n",
    "        x_combined = torch.cat([x1_windowed, x1_word_vector_windowed, pos_features_self, text_feature_self, x2_windowed, x2_word_vector_windowed, pos_features_other, text_feature_other], dim=2)\n",
    "        # x_combined = torch.concat([x1_windowed, pos_features_self, text_feature_self, x2_windowed, pos_features_other, text_feature_other], axis=2)\n",
    "        # here I'm assuming that the input_audio is of proper shape\n",
    "        # initial_state's shape = [batch_size, 1]\n",
    "        # self.h_0_1's shape = [num_layers, hidden_size]\n",
    "        initial_state = torch.unsqueeze(initial_state, 0) # hidden state are time_step first dispite the batch_first parameter\n",
    "        h_0 = initial_state * torch.unsqueeze(self.h_0_1, axis=1) + (1 - initial_state) * torch.unsqueeze(self.h_0_0, axis=1)\n",
    "        c_0 = initial_state * torch.unsqueeze(self.c_0_1, axis=1) + (1 - initial_state) * torch.unsqueeze(self.c_0_0, axis=1)\n",
    "        out, hidden_state = self.lstm(x_combined, (h_0, c_0))\n",
    "        # bn\n",
    "        x = self.activation(out)\n",
    "        x = self.output_layer_1(x)\n",
    "        # x has the shape (N x T x C)\n",
    "        x = self.output_layer_2(x)\n",
    "        x = 1.0 - self.output_layer_3(x)\n",
    "        return x\n",
    "    def load_weights(self, pretrained_dict):\n",
    "    #   not_copy = set(['fc.weight', 'fc.bias'])\n",
    "        model_dict = self.state_dict()\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()}\n",
    "        model_dict.update(pretrained_dict)\n",
    "        self.load_state_dict(model_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def get_velocity(out):\n",
    "    vel = torch.diff(out, dim=1, prepend=out[:, 0:1])\n",
    "    return vel\n",
    "def train_model_with_vel(model, config, dataset_location, training_set, testing_set, wandb, model_name, start = 1, initial_training_dataset=None, initial_validation_dataset=None):\n",
    "    if initial_validation_dataset is None:\n",
    "        training_dataset = Runtime_parsing_Aversion_SelfTape111_with_word_vec(dataset_location, training_set)\n",
    "        validation_dataset = Runtime_parsing_Aversion_SelfTape111_with_word_vec(dataset_location, testing_set)\n",
    "    else:\n",
    "        training_dataset = initial_training_dataset\n",
    "        validation_dataset = initial_validation_dataset\n",
    "\n",
    "    train_data = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "    valid_data = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    device = torch.device(config[\"device\"])\n",
    "    model.to(device)\n",
    "    model.train() \n",
    "    loss_fn = nn.BCELoss()\n",
    "    loss_fn_vel = nn.L1Loss()\n",
    "    training_loss = []\n",
    "    valid_loss = []\n",
    "    training_f1 = []\n",
    "    valid_f1 = []\n",
    "    aversion_vs_start = []\n",
    "    count = 0\n",
    "    # f1_score = BinaryF1Score(num_classes=2).to(device)\n",
    "    f1_score = F1Score(task=\"multiclass\", num_classes=2, average=\"weighted\").to(device)\n",
    "    for epoch in range(start, config['epochs'] + 1):\n",
    "        # reparse the dataset\n",
    "        new_win_size = np.random.randint(100, 200) * 2\n",
    "        training_dataset = Runtime_parsing_Aversion_SelfTape111_with_word_vec(dataset_location, training_set, training_dataset, window_length=new_win_size)\n",
    "        validation_dataset = Runtime_parsing_Aversion_SelfTape111_with_word_vec(dataset_location, testing_set, validation_dataset)\n",
    "        train_data = torch.utils.data.DataLoader(training_dataset, config['batch_size'], True)\n",
    "        valid_data = torch.utils.data.DataLoader(validation_dataset, config['batch_size'], True)\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "        total_aversion_predicted = 0\n",
    "        total_train_f1 = 0\n",
    "        total_valid_f1 = 0\n",
    "        train_batch_counter = 0\n",
    "        valid_batch_counter = 0\n",
    "        total_prediction_counter = 0\n",
    "        prediction_mean = 0\n",
    "        prediction_std = 0\n",
    "        model.zero_grad()\n",
    "        model.train()\n",
    "        for _, (X, [Y, Y_vel]) in enumerate(train_data):\n",
    "            train_batch_counter += 1\n",
    "            X, Y, Y_vel = X.to(device), Y.to(device), Y_vel.to(device)\n",
    "            # print(X.size(), Y.size(), pred.size())\n",
    "            optimiser.zero_grad()\n",
    "            if \"Transformer\" in config[\"model_type\"]:\n",
    "                all_zero = torch.zeros(Y.shape).to(device)\n",
    "                pred = model(X, all_zero)\n",
    "            else:\n",
    "                pred = model(X, Y[:, 0:1])[:, :, 0]\n",
    "            loss = loss_fn(pred, Y)\n",
    "            # get the softmax\n",
    "            pred_vel = get_velocity(pred)\n",
    "            # add the velocity along dimension 1 and 2 respectively, the velocity are opposite \n",
    "            loss = loss + 0.3 * loss_fn_vel(pred_vel[:, :], Y_vel)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_train_loss += loss.item()\n",
    "            # binary_pred = torch.round(pred)\n",
    "            binary_pred = (pred >= 0.5).int()\n",
    "            prediction_mean = torch.mean(binary_pred.float()).item()\n",
    "            prediction_std = torch.std(binary_pred.float()).item()            \n",
    "            f1_train = f1_score(binary_pred, Y).item()\n",
    "            total_aversion_predicted += torch.sum(binary_pred).item()\n",
    "            total_prediction_counter += binary_pred.size()[0] * binary_pred.size()[1] \n",
    "            total_train_f1 += f1_train\n",
    "            del X, Y, pred\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        total_train_f1 /= train_batch_counter\n",
    "        total_train_loss /= len(train_data)\n",
    "        total_aversion_predicted /= total_prediction_counter\n",
    "\n",
    "        for _, (X, [Y, Y_vel]) in enumerate(valid_data):\n",
    "            with torch.no_grad():\n",
    "                valid_batch_counter += 1\n",
    "                X, Y = X.to(device), Y.to(device)\n",
    "                model.eval()\n",
    "                if \"Transformer\" in config[\"model_type\"]:\n",
    "                    all_zero = torch.zeros(Y.shape).to(device)\n",
    "                    pred = model(X, all_zero)\n",
    "                else:\n",
    "                    pred = model(X, Y[:, 0:1])[:, :, 0]\n",
    "                loss = loss_fn(pred, Y)\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "                # binary_pred = torch.round(pred)\n",
    "                binary_pred = (pred >= 0.5).int()\n",
    "                f1_valid = f1_score(binary_pred, Y).item()\n",
    "                total_valid_f1 += f1_valid\n",
    "                del X, Y, pred\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        total_valid_f1 /= valid_batch_counter\n",
    "        total_valid_loss /= len(valid_data)\n",
    "        \n",
    "        if config['wandb']:\n",
    "            wandb.log({'training loss': total_train_loss,\n",
    "                        'validation_loss': total_valid_loss,\n",
    "                        'training_f1': total_train_f1,\n",
    "                        'validation_f1': total_valid_f1, \n",
    "                        \"percentage_predicted_aversion\": total_aversion_predicted})\n",
    "        training_loss.append(total_train_loss)\n",
    "        valid_loss.append(total_valid_loss)\n",
    "        training_f1.append(total_train_f1)\n",
    "        valid_f1.append(total_valid_f1)\n",
    "        aversion_vs_start.append(total_aversion_predicted)\n",
    "        if count == 0:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1))\n",
    "        else:\n",
    "            print(\"Epoch {}, mean: {}, std: {}\\ntraining L: {}\\nvalidation L:{}, model have not improved for {} iterations\".format(epoch, prediction_mean, prediction_std, total_train_f1, total_valid_f1, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on: cuda:0\n",
      "Nicholas Sparrow - Self Tape -The Rock_0\n",
      "Nicholas Sparrow - Self Tape -The Rock_1\n",
      "Ronen Rubinstein Self Tape_0\n",
      "Ronen Rubinstein Self Tape_1\n",
      "‘SWEATER’ DANIEL SELF-TAPE - ZACK FERNANDEZ_0\n",
      "dacre montgomery audition tape_0\n",
      "A self tape I_m very very proud of_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59056/2659105570.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  h_0_0 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n",
      "/tmp/ipykernel_59056/2659105570.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c_0_0 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n",
      "/tmp/ipykernel_59056/2659105570.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  h_0_1 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n",
      "/tmp/ipykernel_59056/2659105570.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c_0_1 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A self tape I_m very very proud of_1\n",
      "A self tape I_m very very proud of_2\n",
      "Natalia Dyer - Stranger Things ＂Nancy Wheeler＂  Audition Tape_0\n",
      "Harrison Green self tape reel 2021_0\n",
      "Harrison Green self tape reel 2021_1\n",
      "Harrison Green self tape reel 2021_2\n",
      "Harrison Green self tape reel 2021_3\n",
      "Dramatic Audition Self-Tape “Shameless”_0\n",
      "SELF-TAPE THAT GOT ME BOOKED ｜ Indie Short Film Audition_0\n",
      "Dramatic Self Tape Reel_0\n",
      "Dramatic Self Tape Reel_1\n",
      "Dramatic Self Tape Reel_2\n",
      "Dramatic Self Tape Reel_3\n",
      "Self Tape Audition_0\n",
      "The Audition That Got Me ACCEPTED Into Drama School!_0\n",
      "Therapist - Acting - Audition - Self-tape - by Thain Wesley_0\n",
      "Therapist - Acting - Audition - Self-tape - by Thain Wesley_1\n",
      "Fabricio Suarez self-tape for Comedic Series_0\n",
      "Self-Tape Demo Reel_0\n",
      "Self-Tape Demo Reel_1\n",
      "Kelsey Boze Self Tape Reel_0\n",
      "Stefania self tape： Danielle Reading lines with her_0\n",
      "Stephanie Hsu ‘Everything Everywhere All At Once’ Audition_0\n",
      "Stephanie Hsu ‘Everything Everywhere All At Once’ Audition_1\n",
      "Stephanie Hsu ‘Everything Everywhere All At Once’ Audition_2\n",
      "Monologue Self Tape- Savannah, Dear John_0\n",
      "Euphoria AMI self-tape audition_0\n",
      "Self Tape Audition ＂Amos＂ in ＂My Brothers Keeper＂_0\n",
      "Self Tape Audition ＂Amos＂ in ＂My Brothers Keeper＂_1\n",
      "My Pretty Little Liars Audition Tape! ｜ Shay Mitchell_0\n",
      "＂Transfers＂ Audition Self-tape_0\n",
      "＂Transfers＂ Audition Self-tape_1\n",
      "＂Transfers＂ Audition Self-tape_2\n",
      "Self Taping (Audition) Example of 4 candidates_0\n",
      "Self Taping (Audition) Example of 4 candidates_1\n",
      "Self Taping (Audition) Example of 4 candidates_2\n",
      "Self Taping (Audition) Example of 4 candidates_3\n",
      "Self Taping (Audition) Example of 4 candidates_4\n",
      "Allison Ungar Self Tape_0\n",
      "Allison Ungar Self Tape_1\n",
      "Skai Jackson’s audition tape for the role of Zoya Lott in the #GossipGirl revival (Viral)_0\n",
      "Skai Jackson’s audition tape for the role of Zoya Lott in the #GossipGirl revival (Viral)_1\n",
      "Skai Jackson’s audition tape for the role of Zoya Lott in the #GossipGirl revival (Viral)_2\n",
      "Skai Jackson’s audition tape for the role of Zoya Lott in the #GossipGirl revival (Viral)_3\n",
      "Chernobyl - Actor - Self-tape - Thain Wesley_0\n",
      "Mikey Hurn self-tape for the role of Walter Hopkins_0\n",
      "Child Actor Farts During Self tape_0\n",
      "Rachel McAdams Audition Tape_0\n",
      "Rachel McAdams Audition Tape_1\n",
      "Rachel McAdams Audition Tape_2\n",
      "Rachel McAdams Audition Tape_3\n",
      "Dacre Montgomery - Audition Tape_0\n",
      "Dacre Montgomery - Audition Tape_1\n",
      "Self Tape Audition Example (Drama) #teenactor_0\n",
      "Self Tape Audition Example (Drama) #teenactor_1\n",
      "Degrassi High, Alex (self tape acting)_0\n",
      "Dramatic Monologue - Fleabag (self-tape style)_0\n",
      "Dramatic Self Tape_0\n",
      "Self Tapes I BOOKED!- All American on THE CW- season 3 episode 7- scene 2_0\n",
      "Self Tape Showreel 2022_0\n",
      "Self Tape Showreel 2022_1\n",
      "Self Tape Showreel 2022_2\n",
      "Self Tape Showreel 2022_3\n",
      "Self Tape Showreel 2022_4\n",
      "Self Tape Showreel 2022_5\n",
      "DEMO REEL EXAMPLES 2020_0\n",
      "DEMO REEL EXAMPLES 2020_1\n",
      "DEMO REEL EXAMPLES 2020_2\n",
      "DEMO REEL EXAMPLES 2020_3\n",
      "DEMO REEL EXAMPLES 2020_4\n",
      "DEMO REEL EXAMPLES 2020_5\n",
      "DEMO REEL EXAMPLES 2020_6\n",
      "DEMO REEL EXAMPLES 2020_7\n",
      "DEMO REEL EXAMPLES 2020_8\n",
      "DEMO REEL EXAMPLES 2020_9\n",
      "DEMO REEL EXAMPLES 2020_10\n",
      "DEMO REEL EXAMPLES 2020_11\n",
      "DEMO REEL EXAMPLES 2020_12\n",
      "Stranger Things’ Dacre Montgomery’s Insane _Billy_ Audition Tape ｜ GQ_0\n",
      "Stranger Things’ Dacre Montgomery’s Insane _Billy_ Audition Tape ｜ GQ_1\n",
      "Stranger Things’ Dacre Montgomery’s Insane _Billy_ Audition Tape ｜ GQ_2\n",
      "Stranger Things’ Dacre Montgomery’s Insane _Billy_ Audition Tape ｜ GQ_3\n",
      "Amber Pursey, Self- tape_0\n",
      "Neda Ghassemi- Self Tape Demo Reel_0\n",
      "Self-tape_0\n",
      "Libby Lee, Self Tape Reel (2021)_0\n",
      "Libby Lee, Self Tape Reel (2021)_1\n",
      "Libby Lee, Self Tape Reel (2021)_2\n",
      "Libby Lee, Self Tape Reel (2021)_3\n",
      "Libby Lee, Self Tape Reel (2021)_4\n",
      "Libby Lee, Self Tape Reel (2021)_5\n",
      "Libby Lee, Self Tape Reel (2021)_6\n",
      "Libby Lee, Self Tape Reel (2021)_7\n",
      "Libby Lee, Self Tape Reel (2021)_8\n",
      "Libby Lee, Self Tape Reel (2021)_9\n",
      "Libby Lee, Self Tape Reel (2021)_10\n",
      "Self-Tape Audition Sample, ＂Hallmark＂ Christmas Film Scene： Veterinarian_0\n",
      "Self-Tape Audition Sample, ＂Hallmark＂ Christmas Film Scene： Veterinarian_1\n",
      "Self-Tape Audition Sample, ＂Hallmark＂ Christmas Film Scene： Veterinarian_2\n",
      "Self-Tape Audition Sample, ＂Hallmark＂ Christmas Film Scene： Veterinarian_3\n",
      "Self-Tape Example 48 Hour Film Festival ｜ How to do a self tape audition How to book your next movie_0\n",
      "Self-Tape Example 48 Hour Film Festival ｜ How to do a self tape audition How to book your next movie_1\n",
      "3 Minute Audition Monologue From Search Party ｜ Day 24 of 31 Self Tape Challenge_0\n",
      "SHERRY HSU - Self Tape_0\n",
      "SHERRY HSU - Self Tape_1\n",
      "Self Tape ｜ Viola_s ＂I left no ring with her＂ monologue ｜ Twelfth Night_0\n",
      "Self-Tape Audition Reel 2022_0\n",
      "Self-Tape Audition Reel 2022_1\n",
      "Self-Tape Audition Reel 2022_2\n",
      "Self-Tape Audition Reel 2022_3\n",
      "Self-Tape Audition Reel 2022_4\n",
      "Self-Tape Audition Reel 2022_5\n",
      "Self-Tape Audition Reel 2022_6\n",
      "Self-Tape Audition Reel 2022_7\n",
      "Self-Tape Audition Reel 2022_8\n",
      "TV Audition Scene Day 30 of 31 Self-Tape Challenge_0\n",
      "Self Tape - Elaine Morillo - Undisclosed Role (RAYN)_0\n",
      "Nykki McGee - Self Tape Reel 2020_0\n",
      "Nykki McGee - Self Tape Reel 2020_1\n",
      "Self-tape showreel_0\n",
      "Self-tape showreel_1\n",
      "Self-tape showreel_2\n",
      "Self-tape showreel_3\n",
      "Self-tape showreel_4\n",
      "Self-tape showreel_5\n",
      "Self-tape showreel_6\n",
      "Self-tape showreel_7\n",
      "Self-tape showreel_8\n",
      "Self-tape showreel_9\n",
      "Self-tape showreel_10\n",
      "Self-Tape example_0\n",
      "Self-Tape example_1\n",
      "Self-Tape example_2\n",
      "Self-Tape example_3\n",
      "Self-Tape example_4\n",
      "Self-Tape example_5\n",
      "Self-Tape example_6\n",
      "National Youth Theatre Self-Tape Audition_0\n",
      "National Youth Theatre Self-Tape Audition_1\n",
      "SELF TAPE Role： Malcolm Wyatt by Anthony Mark Mancini_0\n",
      "Pull My Strings (Self Tape)_0\n",
      "CMTC 2019 - Child 4-11 Self-Tape Competition Winner_0\n",
      "NicholasZoto  REEL_0\n",
      "NicholasZoto  REEL_1\n",
      "NicholasZoto  REEL_2\n",
      "NicholasZoto  REEL_3\n",
      "NicholasZoto  REEL_4\n",
      "NicholasZoto  REEL_5\n",
      "NicholasZoto  REEL_6\n",
      "NicholasZoto  REEL_7\n",
      "NicholasZoto  REEL_8\n",
      "Commercial Self Tape - People Store_0\n",
      "Self-Tape Audition Example Tasmania ｜ How to Get an Agent in Hollywood and Los Angeles ｜ Slate_0\n",
      "Self-Tape Audition Example Tasmania ｜ How to Get an Agent in Hollywood and Los Angeles ｜ Slate_1\n",
      "Emily Owens - Acting Selftape Reel_0\n",
      "Emily Owens - Acting Selftape Reel_1\n",
      "Emily Owens - Acting Selftape Reel_2\n",
      "Self Tape Reel_0\n",
      "Self Tape Reel_1\n",
      "Self Tape Reel_2\n",
      "Self Tape Reel_3\n",
      "Self Tape Reel_4\n",
      "Self tape 4⧸16_0\n",
      "Acting Self Tape Audition Example - Resentful Daughter_0\n",
      "Self-Tape Audition_0\n",
      "Lilly Niehaus Self Tape Reel_0\n",
      "Lilly Niehaus Self Tape Reel_1\n",
      "Lilly Niehaus Self Tape Reel_2\n",
      "Lilly Niehaus Self Tape Reel_3\n",
      "Lilly Niehaus Self Tape Reel_4\n",
      "My Audition Tape for The National Youth Theatre 2020 - Great News Today!!_0\n",
      "SELF TAPE DEMO REEL- Lillian Johnson_0\n",
      "SELF TAPE DEMO REEL- Lillian Johnson_1\n",
      "SELF TAPE DEMO REEL- Lillian Johnson_2\n",
      "SELF TAPE DEMO REEL- Lillian Johnson_3\n",
      "IP-1110 WHEELER Self-Tape Cleo_0\n",
      "Acting self tape 1_0\n",
      "Madelaine Petsch audition for The Prom_0\n",
      "Madelaine Petsch audition for The Prom_1\n",
      "CLYDE SELF-TAPE - ZACK FERNANDEZ_0\n",
      "Self Tape example_0\n",
      "Self Tape example_1\n",
      "Self Tape example_2\n",
      "George Diss -Self tape montage_0\n",
      "George Diss -Self tape montage_1\n",
      "George Diss -Self tape montage_2\n",
      "Freya Callaghan - Self Tapes, March 2022_0\n",
      "Freya Callaghan - Self Tapes, March 2022_1\n",
      "Freya Callaghan - Self Tapes, March 2022_2\n",
      "Freya Callaghan - Self Tapes, March 2022_3\n",
      "Freya Callaghan - Self Tapes, March 2022_4\n",
      "Freya Callaghan - Self Tapes, March 2022_5\n",
      "Freya Callaghan - Self Tapes, March 2022_6\n",
      "Freya Callaghan - Self Tapes, March 2022_7\n",
      "Freya Callaghan - Self Tapes, March 2022_8\n",
      "Freya Callaghan - Self Tapes, March 2022_9\n",
      "Self tape For All Mankind_Iryna_0\n",
      "Self tape_0\n",
      "Self tape_1\n",
      "Self tape_2\n",
      "Self tape_3\n",
      "Self tape_4\n",
      "Self tape_5\n",
      "Hugh Laurie - House MD, Audition Tape_0\n",
      "Hugh Laurie - House MD, Audition Tape_1\n",
      "Hugh Laurie - House MD, Audition Tape_2\n",
      "Oxford School of Drama Audition Tape (I GOT IN!!)_0\n",
      "Oxford School of Drama Audition Tape (I GOT IN!!)_1\n",
      "Jake Williams： Self-Tape Reel_0\n",
      "Jake Williams： Self-Tape Reel_1\n",
      "Jake Williams： Self-Tape Reel_2\n",
      "Jake Williams： Self-Tape Reel_3\n",
      "Jake Williams： Self-Tape Reel_4\n",
      "Barb the Awkward Disaster - Self-tape Practice_0\n",
      "Self-Tape： Grosse Pointe Blank_0\n",
      "CMTC 2019 - 12+ Winner of Self-Tape Competition_0\n",
      "Mihail Krastev self-tape_0\n",
      "SELF TAPE SHOWREEL_0\n",
      "SELF TAPE SHOWREEL_1\n",
      "SELF TAPE SHOWREEL_2\n",
      "SELF TAPE SHOWREEL_3\n",
      "SELF TAPE SHOWREEL_4\n",
      "SELF TAPE SHOWREEL_5\n",
      "SELF TAPE SHOWREEL_6\n",
      "SELF TAPE SHOWREEL_7\n",
      "SELF TAPE SHOWREEL_8\n",
      "SELF TAPE SHOWREEL_9\n",
      "SELF TAPE SHOWREEL_10\n",
      "SELF TAPE SHOWREEL_11\n",
      "SELF TAPE SHOWREEL_12\n",
      "SELF TAPE SHOWREEL_13\n",
      "SELF TAPE SHOWREEL_14\n",
      "SELF TAPE SHOWREEL_15\n",
      "SELF TAPE SHOWREEL_16\n",
      "SELF TAPE SHOWREEL_17\n",
      "SELF TAPE SHOWREEL_18\n",
      "SELF TAPE SHOWREEL_19\n",
      "SELF TAPE SHOWREEL_20\n",
      "SELF TAPE SHOWREEL_21\n",
      "SELF TAPE SHOWREEL_22\n",
      "SELF TAPE SHOWREEL_23\n",
      "SELF TAPE SHOWREEL_24\n",
      "SELF TAPE SHOWREEL_25\n",
      "SELF TAPE SHOWREEL_26\n",
      "SELF TAPE SHOWREEL_27\n",
      "SELF TAPE SHOWREEL_28\n",
      "SELF TAPE SHOWREEL_29\n",
      "SELF TAPE SHOWREEL_30\n",
      "SELF TAPE SHOWREEL_31\n",
      "SELF TAPE SHOWREEL_32\n",
      "SELF TAPE SHOWREEL_33\n",
      "SELF TAPE SHOWREEL_34\n",
      "SELF TAPE SHOWREEL_35\n",
      "SELF TAPE SHOWREEL_36\n",
      "Self tape ＂London＂ from Suite life of Zack and Cody_0\n",
      "_Emma_ Self Tape (2)_0\n",
      "My audition tape break up scene show reel_0\n",
      "Self-Tape Audition by Louise Chiasson for Wallflower Drama Series_0\n",
      "Self-Tape Audition by Louise Chiasson for Wallflower Drama Series_1\n",
      "Self-Tape Audition by Louise Chiasson for Wallflower Drama Series_2\n",
      "Self-Tape Challenge - Deborah Lettner  - Final Destination_0\n",
      "Self-Tape Challenge - Deborah Lettner  - Final Destination_1\n",
      "Self-Tape Challenge - Deborah Lettner  - Final Destination_2\n",
      "Self-Tape Challenge - Deborah Lettner  - Final Destination_3\n",
      "Self-Tape Challenge - Deborah Lettner  - Final Destination_4\n",
      "Self Tape Demo Reel_0\n",
      "Self Tape Demo Reel_1\n",
      "Self Tape Demo Reel_2\n",
      "Self Tape Demo Reel_3\n",
      "Self Tape Demo Reel_4\n",
      "Victoria Summer - Self Tape Audition - Mary Poppins_0\n",
      "Victoria Summer - Self Tape Audition - Mary Poppins_1\n",
      "ACTING SELF TAPE ｜｜ January 2023 ｜｜ Dream Girl Georgina Allerton Monologue_0\n",
      "Sheriff Jessica Cooper Self-Tape Audition_0\n",
      "Sheriff Jessica Cooper Self-Tape Audition_1\n",
      "Self-tape Dramatic Acting Reel - Rose Eshay_0\n",
      "Self-tape Dramatic Acting Reel - Rose Eshay_1\n",
      "Self-tape Dramatic Acting Reel - Rose Eshay_2\n",
      "Self-tape Dramatic Acting Reel - Rose Eshay_3\n",
      "IP-1110 ⧸ WHEELER ⧸ Self-tape Audition ⧸ Captain Townes Tk1_0\n",
      "IP-1110 ⧸ WHEELER ⧸ Self-tape Audition ⧸ Captain Townes Tk1_1\n",
      "CMTC 2019- 1st Runner Up for Child 4-11 Self- Tape Competition_0\n",
      "Rebecca Schmautz Self Tape_0\n",
      "Rebecca Schmautz Self Tape_1\n",
      "Rebecca Schmautz Self Tape_2\n",
      "Rebecca Schmautz Self Tape_3\n",
      "Memory ｜ Theatrical Audition ｜ Acting in Hollywood ｜ Crying on Camera Example Self Tape_0\n",
      "Memory ｜ Theatrical Audition ｜ Acting in Hollywood ｜ Crying on Camera Example Self Tape_1\n",
      "Julia Senise Self Tape_0\n",
      "Julia Senise Self Tape_1\n",
      "Aussie self tape reel_0\n",
      "Aussie self tape reel_1\n",
      "Aussie self tape reel_2\n",
      "Aussie self tape reel_3\n",
      "Scottish Detective monologue ｜ Self tape ｜ _Solving murder mysteries aye__0\n",
      "Delilah Self Tape Audition ＂The After Party＂_0\n",
      "Delilah Self Tape Audition ＂The After Party＂_1\n",
      "Audition to Booking   OZARK_0\n",
      "Audition to Booking   OZARK_1\n",
      "Audition to Booking   OZARK_2\n",
      "Audition to Booking   OZARK_3\n",
      "Audition to Booking   OZARK_4\n",
      "Audition to Booking   OZARK_5\n",
      "Audition to Booking   OZARK_6\n",
      "Audition to Booking   OZARK_7\n",
      "Audition to Booking   OZARK_8\n",
      "Audition to Booking   OZARK_9\n",
      "Audition to Booking   OZARK_10\n",
      "Audition to Booking   OZARK_11\n",
      "Audition to Booking   OZARK_12\n",
      "Phillip Lanos ｜ Self tape ｜ Audition_0\n",
      "Phillip Lanos ｜ Self tape ｜ Audition_1\n",
      "Hannah Crowe Self tape-short horror film_0\n",
      "Ryan Johns - Self Tape Reel_0\n",
      "Ryan Johns - Self Tape Reel_1\n",
      "Ryan Johns - Self Tape Reel_2\n",
      "Ryan Johns - Self Tape Reel_3\n"
     ]
    }
   ],
   "source": [
    "# inputs\n",
    "gpu = 0\n",
    "dataset_location = \"/scratch/ondemand27/evanpan/data/deep_learning_processed_dataset_real_time_aug\"\n",
    "model_save_location = \"/scratch/ondemand27/evanpan/data/Gaze_aversion_models\"\n",
    "config = json.load(open(\"/scratch/ondemand27/evanpan/Gaze_project/training/shuffling_window_config.json\", \"r\"))\n",
    "# do the training test split here:\n",
    "dataset_metadata = \"/scratch/ondemand27/evanpan/data/deep_learning_processed_dataset_real_time_aug/video_to_window_metadata.json\"\n",
    "dataset_metadata = json.load(open(dataset_metadata, \"r\"))\n",
    "all_videos = list(dataset_metadata.keys())\n",
    "training_set = []\n",
    "testing_set = []\n",
    "# get the name of the videos (this ensures no contamination because the same shot is split)\n",
    "for i in range(0, len(all_videos)):\n",
    "    if i / len(all_videos) < 0.9:\n",
    "        training_set.append(all_videos[i])\n",
    "    else:\n",
    "        testing_set.append(all_videos[i])\n",
    "device = torch.device('cuda:{}'.format(gpu) if torch.cuda.is_available() else 'cpu')\n",
    "run_obj = None\n",
    "config[\"load_model\"] = False\n",
    "config[\"wandb\"] = False\n",
    "config[\"learning_rate\"] = 0.001\n",
    "config[\"batch_size\"] = 32\n",
    "config[\"device\"] = 'cuda:{}'.format(gpu) if torch.cuda.is_available() else 'cpu'\n",
    "initial_training_dataset = Runtime_parsing_Aversion_SelfTape111_with_word_vec(dataset_location, training_set)\n",
    "initial_validation_dataset = Runtime_parsing_Aversion_SelfTape111_with_word_vec(dataset_location, testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is on: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59056/4202945842.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  h_0_0 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n",
      "/tmp/ipykernel_59056/4202945842.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c_0_0 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n",
      "/tmp/ipykernel_59056/4202945842.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  h_0_1 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n",
      "/tmp/ipykernel_59056/4202945842.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c_0_1 = torch.tensor(torch.randn(self.num_lstm_layer, self.lstm_hidden_dims), requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 104, got 84",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m Explicit_context_GazePredictionModel_mel_only_with_word_vec(config)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model_with_vel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreparse_dataset_explicit_first_frame_with_freq_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_training_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_training_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_validation_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_validation_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 57\u001b[0m, in \u001b[0;36mtrain_model_with_vel\u001b[0;34m(model, config, dataset_location, training_set, testing_set, wandb, model_name, start, initial_training_dataset, initial_validation_dataset)\u001b[0m\n\u001b[1;32m     55\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(X, all_zero)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[:, :, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, Y)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# get the softmax\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[29], line 99\u001b[0m, in \u001b[0;36mExplicit_context_GazePredictionModel_mel_only_with_word_vec.forward\u001b[0;34m(self, input_feature, initial_state)\u001b[0m\n\u001b[1;32m     97\u001b[0m h_0 \u001b[38;5;241m=\u001b[39m initial_state \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_0_1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m initial_state) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh_0_0, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     98\u001b[0m c_0 \u001b[38;5;241m=\u001b[39m initial_state \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_0_1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m initial_state) \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_0_0, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m out, hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# bn\u001b[39;00m\n\u001b[1;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(out)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/nn/modules/rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/nn/modules/rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    726\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[1;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[1;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[1;32m    729\u001b[0m                        ):\n\u001b[0;32m--> 730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    732\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[1;32m    734\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/scratch/ondemand27/evanpan/conda_env/.conda/envs/jaligaze/lib/python3.10/site-packages/torch/nn/modules/rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    216\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    220\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 104, got 84"
     ]
    }
   ],
   "source": [
    "model = Explicit_context_GazePredictionModel_mel_only_with_word_vec(config)\n",
    "train_model_with_vel(model, config, dataset_location, training_set, testing_set, run_obj, \"reparse_dataset_explicit_first_frame_with_freq_mask\", start=1, initial_training_dataset=initial_training_dataset, initial_validation_dataset=initial_validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = Runtime_parsing_Aversion_SelfTape111(dataset_location, training_set, training_dataset, window_length=new_win_size)\n",
    "validation_dataset = Runtime_parsing_Aversion_SelfTape111(dataset_location, testing_set, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 1000):\n",
    "    new_win_size = np.random.randint(100, 200) * 2\n",
    "    training_dataset = Runtime_parsing_Aversion_SelfTape111(dataset_location, training_set, training_dataset, window_length=new_win_size)\n",
    "    validation_dataset = Runtime_parsing_Aversion_SelfTape111(dataset_location, testing_set, validation_dataset)\n",
    "    for l in range(len(training_dataset)):\n",
    "        training_dataset[l][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2927"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaligaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
