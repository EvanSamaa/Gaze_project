{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2 as cv\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from scipy import signal\n",
    "from scipy import stats, spatial, ndimage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_folder = \"/Users/evanpan/Documents/Datasets/Ribhav_processed_dataset/\"\n",
    "input_folder = \"F:/MASC/Ribhav_processed_dataset/\"\n",
    "input_file = \"Madelaine Petsch audition for The Prom\"\n",
    "shot_id = 1 \n",
    "speaker_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_file_no_space = \"_\".join(input_file.split(\" \"))\n",
    "shot_file_name = os.path.join(os.path.join(os.path.join(input_folder, \"shots\"), input_file), \"shot_cuts.json\")\n",
    "video_path = os.path.join(*[input_folder, \"video\", input_file+\".mp4\"])\n",
    "audio_1_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.wav\".format(shot_id, 1))\n",
    "audio_2_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.wav\".format(shot_id, 2))\n",
    "script_1_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.txt\".format(shot_id, 1))\n",
    "script_2_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}.txt\".format(shot_id, 2))\n",
    "praatoutput_1_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}_PraatOutput.txt\".format(shot_id, 1))\n",
    "praatoutput_2_path = os.path.join(os.path.join(os.path.join(input_folder, \"JaliTranscript\"), input_file_no_space), \"shot_{}_{}_PraatOutput.txt\".format(shot_id, 2))\n",
    "gaze_direction_json_path = os.path.join(*[input_folder, \"L2CSNet\", input_file+\".json\"])\n",
    "head_direction_json_path = os.path.join(*[input_folder, \"pose\", input_file+\".pkl\"])\n",
    "# out_path = os.path.join(*[\"/Users/evanpan/Documents/GitHub/Gaze_project/data/look_at_points\", \"video_annotation.json\"])\n",
    "out_path = \".out.json\" \n",
    "\n",
    "vid = cv.VideoCapture(video_path)\n",
    "fps = vid.get(cv.CAP_PROP_FPS)\n",
    "# load the input shots range\n",
    "shots = json.load(open(shot_file_name))[\"shots\"]\n",
    "valid_shots = []\n",
    "t0 = datetime.strptime(\"00:00:00.0\", \"%H:%M:%S.%f\").replace(tzinfo=timezone.utc).timestamp()\n",
    "for i in range(len(shots)):\n",
    "    start = datetime.strptime(shots[i][0], '%H:%M:%S.%f').replace(tzinfo=timezone.utc).timestamp()\n",
    "    end = datetime.strptime(shots[i][1], '%H:%M:%S.%f').replace(tzinfo=timezone.utc).timestamp()\n",
    "    if (end-start) >= 5:\n",
    "        valid_shots.append([start-t0, end-t0])\n",
    "shot_used_range = valid_shots[shot_id]\n",
    "range_low = int(np.round(shot_used_range[0] * fps))\n",
    "range_high = int(np.round(shot_used_range[1] * fps))\n",
    "ts = np.arange(0, range_high-range_low)/fps+valid_shots[shot_id][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### helper functions (scipy's rotation package sucks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_angles_frome_positions(arr):\n",
    "    \"\"\"\n",
    "    converts an array of positions to an array of rotation angles (azimuth, elevation)\n",
    "    centered at the origin, where:\n",
    "        azimuth: +right,-left\n",
    "        elevation: +up,-down\n",
    "    here we assume that the input vectors are in world coordinates\n",
    "    :param arr: array with shape (N, 3)\n",
    "    :return: array with shape (N, 2)\n",
    "    \"\"\"\n",
    "    # F: arr (N, 3) -> arr (N, 2) or arr (3, ) -> (2, )\n",
    "    # in the output is in the convention of (azimuth, elevation)\n",
    "    if len(arr.shape) == 2:\n",
    "        mag = np.sqrt(np.sum(arr * arr, axis=1, keepdims=True))\n",
    "        out = arr / mag\n",
    "        out[:, 0] = np.arcsin(out[:, 0])\n",
    "        out[:, 1] = np.arcsin(out[:, 1])\n",
    "        return out[:, 0:2] * 180 / np.pi\n",
    "    else:\n",
    "        mag = np.sqrt(np.sum(arr * arr))\n",
    "        out = arr / mag\n",
    "        out[0] = np.arcsin(out[0])\n",
    "        out[1] = np.arcsin(out[1])\n",
    "        return out[0:2] * 180 / np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head data\n",
    "all_head_data = pkl.load(open(head_direction_json_path, \"rb\"))\n",
    "head_angle_data = all_head_data[\"HEAD\"]\n",
    "head_rotmat_per_frame = head_angle_data[\"ROTMAT\"]\n",
    "head_euler_per_frame = head_angle_data[\"EULER\"]\n",
    "head_angle_per_frame = []\n",
    "neutral_position = np.array([0, 0, 100])\n",
    "for i in range(0, head_rotmat_per_frame.shape[0]):\n",
    "    pos = head_rotmat_per_frame[i] @ neutral_position\n",
    "    head_angle_per_frame.append(rotation_angles_frome_positions(pos[:]))\n",
    "head_angle_per_frame = np.array(head_angle_per_frame)[range_low:range_high]\n",
    "# gaze data\n",
    "f = open(gaze_direction_json_path, \"rb\")\n",
    "all_gaze_data = pkl.load(f)\n",
    "gaze_angle_data = all_gaze_data[\"RAW_GAZE\"]\n",
    "gaze_angle_per_frame = gaze_angle_data[\"EULER\"][range_low:range_high]\n",
    "gaze_rotmat_per_frame = gaze_angle_data[\"ROTMAT\"][range_low:range_high]\n",
    "blinks = all_head_data[\"BLINKS\"][range_low:range_high]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesture_only_head_motion = []\n",
    "for i in range(0, 2):\n",
    "    head = head_angle_per_frame[:, i]\n",
    "    head_smooth = ndimage.gaussian_filter1d(head, sigma=10)\n",
    "    delta_head = head-head_smooth\n",
    "    gesture_only_head_motion.append(delta_head)\n",
    "# export them to a json file so I can see what they look like in maya\n",
    "out_json = {\"ts\":ts.tolist(), \n",
    "            \"no_gaze_head\":[ndimage.gaussian_filter1d(gesture_only_head_motion[0], sigma=3).tolist(), \n",
    "                            ndimage.gaussian_filter1d(gesture_only_head_motion[1], sigma=3).tolist()],\n",
    "            \"all_head\":[ndimage.gaussian_filter1d(head_angle_per_frame[:, 0], sigma=3).tolist(), \n",
    "                            ndimage.gaussian_filter1d(head_angle_per_frame[:, 1], sigma=3).tolist()], }\n",
    "\n",
    "json.dump(out_json, open(out_path, \"w\"))\n",
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Visemenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
