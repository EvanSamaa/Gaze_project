{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2 as cv\n",
    "import pickle as pkl\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os, sys\n",
    "import librosa\n",
    "import shutil \n",
    "\n",
    "from scipy import stats, spatial, ndimage\n",
    "sys.path.insert(0, '/Users/evanpan/Documents/GitHub/EvansToolBox/Utils')\n",
    "sys.path.insert(0, '/Users/evanpan/Documents/GitHub/Gaze_project')\n",
    "sys.path.insert(0, \"C:/Users/evansamaa/Documents/GitHub/EvansToolBox/Utils\")\n",
    "sys.path.insert(0, \"C:/Users/evansamaa/Documents/GitHub/Gaze_project\")\n",
    "sys.path.insert(0, \"C:/Users/evan1/Documents/GitHub/EvansToolBox/Utils\")\n",
    "sys.path.insert(0, \"C:/Users/evan1/Documents/GitHub/Gaze_project\")\n",
    "from Signal_processing_utils import intensity_from_signal, pitch_from_signal, sparse_key_smoothing, laplacian_smoothing\n",
    "from Speech_Data_util import Sentence_word_phone_parser\n",
    "from prototypes.InputDataStructures import Dietic_Conversation_Gaze_Scene_Info\n",
    "from prototypes.MVP.MVP_static_saliency_list import ObjectBasedFixSaliency\n",
    "from prototypes.MVP.MVP_Aversion_saliency_list import AversionSignalDrivenSaliency, CTSAversionSignalDrivenSaliency\n",
    "from prototypes.MVP.MVP_look_at_point_planner import HabituationBasedPlanner, RandomPlanner, PartnerHabituationPlanner\n",
    "from prototypes.MVP.MVP_eye_head_driver import HeuristicGazeMotionGenerator\n",
    "from prototypes.MVP.MVP_Aversion_saliency_list import Base_Static_Saliency_List\n",
    "from prototypes.EyeCatch.Saccade_model_with_internal_model import *\n",
    "from prototypes.Gaze_aversion_prior.Heuristic_model import *\n",
    "from prototypes.Boccignone2020.Gaze_target_planner import Scavenger_based_planner\n",
    "from prototypes.Boccignone2020.Improved_gaze_target_planner import Scavenger_planner_with_nest, Scavenger_planner_simple \n",
    "from prototypes.JaliNeck.JaliNeck import NeckCurve\n",
    "from prototypes.Gaze_aversion_prior.Ribhav_model import predict_aversion\n",
    "from prototypes.Gaze_aversion_prior.Evan_model import Aversion111Prior\n",
    "from prototypes.InputDataStructures import AgentInfo, TurnTakingData\n",
    "from prototypes.MVP.MVP_gaze_path_planner import Responsive_planner_simple, Responsive_planner_no_heuristics, Responsive_planner_no_Gaze_deploy, Responsive_planner_React_to_gaze_no_Gaze_deploy\n",
    "import pickle\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "import scipy.stats as ss\n",
    "from Signal_processing_utils import interpolate1D, runEuro\n",
    "from scipy.interpolate import interp1d\n",
    "from Geometry_Util import directions_from_rotation_angles\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%aimport prototypes.InputDataStructures\n",
    "%aimport Speech_Data_util\n",
    "%aimport Signal_processing_utils\n",
    "%aimport prototypes.MVP.MVP_static_saliency_list\n",
    "%aimport prototypes.EyeCatch.Saccade_model_with_internal_model\n",
    "%aimport prototypes.InputDataStructures\n",
    "%aimport prototypes.Jin2019.EyeHeadDecomposition\n",
    "%aimport prototypes.Optimization_based_head_eye_seperator.Baseline_optimization\n",
    "%aimport prototypes.Boccignone2020.Improved_gaze_target_planner\n",
    "%aimport prototypes.MVP.MVP_gaze_path_planner\n",
    "%aimport prototypes.JaliNeck.JaliNeck\n",
    "%aimport prototypes.Gaze_aversion_prior.Evan_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You need to make the temp folder yourself before running these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs (for desk top at home)\n",
    "# input_folder = \"/Volumes/EVAN_DISK/MASC/Ribhav_processed_dataset/\"\n",
    "input_folder = \"F:/MASC/shot_processed_dataset/\"\n",
    "input_file = \"heat_source_video\"\n",
    "outside_dataset = True\n",
    "model_location = \"C:/Users/evansamaa/Documents/GitHub/Gaze_project/prototypes/Gaze_aversion_prior/sentence_word_audio_velocity_model\"\n",
    "whisper_location = \"C:/Users/evansamaa/Documents/GitHub/Gaze_project/models\"\n",
    "temp_folder = \"F:/MASC/JALI_gaze/Animations/heat\" \n",
    "speaker_id = 1\n",
    "turn_taking_threshold = 2\n",
    "fps = 25\n",
    "np.random.seed(speaker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs (for jali laptop)\n",
    "# input_folder = \"/Volumes/EVAN_DISK/MASC/Ribhav_processed_dataset/\"\n",
    "input_folder = \"D:/MASC/shot_processed_dataset/\"\n",
    "input_file = \"heat_source_video\"\n",
    "outside_dataset = True\n",
    "model_location = \"C:/Users/evan1/Documents/GitHub/Gaze_project/prototypes/Gaze_aversion_prior/sentence_word_audio_velocity_model\"\n",
    "whisper_location = \"C:/Users/evan1/Documents/GitHub/Gaze_project/models\"\n",
    "temp_folder = \"D:/MASC/JALI_gaze/Animations/heat\" \n",
    "turn_taking_threshold = 2\n",
    "fps = 25\n",
    "np.random.seed(speaker_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all the file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder already exist\n",
      "folder already exist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_file_no_space = input_file.replace(\" \", \"_\")\n",
    "gaze_animation_output_location = os.path.join(*[temp_folder, \"outputs\"])\n",
    "try:\n",
    "    os.mkdir(os.path.join(*[temp_folder, \"outputs\"]))   \n",
    "except:\n",
    "    print(\"folder already exist\")   \n",
    "try:\n",
    "    os.mkdir(os.path.join(*[temp_folder, \"annotated_scene\"]))   \n",
    "except:\n",
    "    print(\"folder already exist\")    \n",
    "# input_file_no_space = \"'\" + input_file_no_space + \"'\"\n",
    "# for both speakers\n",
    "raw_audio_path = os.path.join(*[temp_folder, input_file_no_space+\".wav\"]).replace(os.sep, \"/\")\n",
    "basic_scene_data_path = \"./data/look_at_points/simplest_scene2_less_items.json\"\n",
    "# for the first speaker\n",
    "speaker_id = 0\n",
    "praatoutput_path_0 = os.path.join(temp_folder, input_file_no_space+\"_{}_PraatOutput.txt\".format(speaker_id)).replace(os.sep, \"/\")\n",
    "audio_path_0 = os.path.join(*[temp_folder, input_file_no_space+\"_{}.wav\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "text_file_path_0 = os.path.join(*[temp_folder, input_file_no_space+\"_{}.txt\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "transcript_file_path_0 = os.path.join(*[temp_folder, input_file+\"_transcript.json\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "tagged_text_file_path_0 = os.path.join(*[temp_folder, input_file_no_space+\"_{}_tagged.txt\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "if outside_dataset:\n",
    "    audio_path_0 = os.path.join(*[temp_folder, input_file_no_space+\"_{}.wav\".format(speaker_id)]).replace(os.sep, \"/\")    \n",
    "# output paths\n",
    "output_neural_location_0 = os.path.join(*[temp_folder, \"outputs\", input_file+\"_neural_{}.pkl\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "# other important input data (such as scene )\n",
    "annotation_data_path_0= os.path.join(*[temp_folder, \"annotated_scene\", input_file+\"_points_{}.json\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "scene_data_path_0 = os.path.join(*[temp_folder, \"annotated_scene\", input_file+\"_scene_{}.json\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "\n",
    "# for the first speaker\n",
    "speaker_id = 1\n",
    "praatoutput_path_1 = os.path.join(temp_folder, input_file_no_space+\"_{}_PraatOutput.txt\".format(speaker_id)).replace(os.sep, \"/\")\n",
    "audio_path_1 = os.path.join(*[temp_folder, input_file_no_space+\"_{}.wav\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "text_file_path_1 = os.path.join(*[temp_folder, input_file_no_space+\"_{}.txt\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "transcript_file_path_1 = os.path.join(*[temp_folder, input_file+\"_transcript.json\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "tagged_text_file_path_1 = os.path.join(*[temp_folder, input_file_no_space+\"_{}_tagged.txt\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "if outside_dataset:\n",
    "    audio_path_1 = os.path.join(*[temp_folder, input_file_no_space+\"_{}.wav\".format(speaker_id)]).replace(os.sep, \"/\")    \n",
    "# output paths\n",
    "output_neural_location_1 = os.path.join(*[temp_folder, \"outputs\", input_file+\"_neural_{}.pkl\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "# other important input data (such as scene )\n",
    "annotation_data_path_1 = os.path.join(*[temp_folder, \"annotated_scene\", input_file+\"_points_{}.json\".format(speaker_id)]).replace(os.sep, \"/\")\n",
    "scene_data_path_1 = os.path.join(*[temp_folder, \"annotated_scene\", input_file+\"_scene_{}.json\".format(speaker_id)]).replace(os.sep, \"/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Neck Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n",
      "73\n",
      "73\n"
     ]
    }
   ],
   "source": [
    "conversational_neck_0 = NeckCurve(audio_path_0)\n",
    "jali_neck_output_0 = conversational_neck_0.compute_curve()\n",
    "\n",
    "conversational_neck_1 = NeckCurve(audio_path_1)\n",
    "jali_neck_output_1 = conversational_neck_1.compute_curve()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following script generate two scene files. Edit and save them in MAYA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene file 0 already exist\n",
      "scene file 1 already exist\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(scene_data_path_0):\n",
    "    print(\"scene file 0 already exist\")\n",
    "else:\n",
    "    shutil.copy(basic_scene_data_path, scene_data_path_0)\n",
    "if os.path.isfile(scene_data_path_1):\n",
    "    print(\"scene file 1 already exist\")\n",
    "else:\n",
    "    shutil.copy(basic_scene_data_path, scene_data_path_1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use deep learning to generate saliency map for the characters when they are speaking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salinecy Map Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_input_Saliency(Base_Static_Saliency_List):\n",
    "    def __init__(self, scene_info: AgentInfo, ts, dt=1/24):\n",
    "        self.scene_info: AgentInfo = scene_info\n",
    "        self._number_of_objects = scene_info.get_all_positions().shape[0]\n",
    "        self._dt = dt # 100 hz\n",
    "        self._numb_of_frames = ts.shape[0] # total number of frames\n",
    "        self.evaluated = False\n",
    "        self.map = np.zeros((int(self._numb_of_frames), self._number_of_objects))\n",
    "        self.map_interp = None\n",
    "    def get_object_positions(self):\n",
    "        return self.scene_info.get_all_positions()\n",
    "    \n",
    "    def evaluate_all(self):\n",
    "        if self.evaluated:\n",
    "            return self.map\n",
    "        else:\n",
    "            self.compute_salience()\n",
    "            x = np.arange(0, self._numb_of_frames) * self._dt\n",
    "            self.map_interp = interp1d(x, self.map, axis=0, fill_value=\"extrapolate\")\n",
    "            self.evaluated = True\n",
    "            return self.map\n",
    "    def evaluate(self, t):\n",
    "        if self.evaluated:\n",
    "            return self.map_interp(t)\n",
    "        else:\n",
    "            self.compute_salience()\n",
    "            x = np.arange(0, self._numb_of_frames) * self._dt\n",
    "            self.map_interp = interp1d(x, self.map, axis=0, fill_value=\"extrapolate\")\n",
    "            self.evaluated = True\n",
    "            return self.map_interp(t)\n",
    "    def compute_salience(self, aversion_prob_time, aversion_prob_val, interval=True):\n",
    "        # continue setting salience for all objects\n",
    "        inteppp = interp1d(aversion_prob_time, aversion_prob_val, bounds_error=False)\n",
    "        for j in range(0, self._numb_of_frames):\n",
    "            for i in range(0, self._number_of_objects):\n",
    "                if i < self.scene_info.get_object_positions(coordinate_space=\"global\").shape[0]:\n",
    "                    self.map[j, i] = self.scene_info.object_interest[i]\n",
    "                elif i == self.scene_info.get_object_positions(coordinate_space=\"global\").shape[0]:\n",
    "                    self.map[j, i] = 1 - inteppp(float(j) * self._dt)\n",
    "                else:\n",
    "                    if inteppp(float(j) * self._dt) < 0.3:\n",
    "                        self.map[j, i] = 0\n",
    "                    else:\n",
    "                        self.map[j, i] = 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Aversion Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Aversion111Prior(model_location, whisper_location)\n",
    "# for speaker 0 \n",
    "aversion_probabilities_0 = model.predict(temp_folder, input_folder, input_file, 0, in_dataset=False)\n",
    "ts = np.arange(0, aversion_probabilities_0.shape[0]) / fps\n",
    "# 1 is direct gaze, 0 is aversion, here I want aversion probability\n",
    "aversion_probability_0 = runEuro(ts, aversion_probabilities_0)[:, 0]\n",
    "# since the model doesn't give prediction until later in the audio. \n",
    "# I will omit the prediction until the mode is fairly certain\n",
    "for i in range(0, aversion_probability_0.shape[0]):\n",
    "    if np.abs(aversion_probability_0[i] - 0.5) <= 0.3:\n",
    "        aversion_probability_0[i] = 0\n",
    "    else:\n",
    "        break\n",
    "aversion_probability_0 = np.where(aversion_probability_0 > 0.5, 1, 0)\n",
    "\n",
    "aversion_probabilities_1 = model.predict(temp_folder, input_folder, input_file, 1, in_dataset=False)\n",
    "ts = np.arange(0, aversion_probabilities_0.shape[0]) / fps\n",
    "# 1 is direct gaze, 0 is aversion, here I want aversion probability\n",
    "aversion_probability_1 = runEuro(ts, aversion_probabilities_1)[:, 0]\n",
    "# since the model doesn't give prediction until later in the audio. \n",
    "# I will omit the prediction until the mode is fairly certain\n",
    "for i in range(0, aversion_probability_1.shape[0]):\n",
    "    if np.abs(aversion_probability_1[i] - 0.5) <= 0.3:\n",
    "        aversion_probability_1[i] = 0\n",
    "    else:\n",
    "        break\n",
    "aversion_probability_1 = np.where(aversion_probability_1 > 0.5, 1, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get activity interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_0, sr = librosa.load(audio_path_0)\n",
    "audio_1, sr = librosa.load(audio_path_1)\n",
    "intensity_0 = intensity_from_signal(audio_0, int(sr/25))\n",
    "intensity_1 = intensity_from_signal(audio_1, int(sr/25))\n",
    "acitivity = np.where(intensity_0 > intensity_1, -1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain Tag timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transcript list from the JSON file\n",
    "def get_beats(audio, sr):\n",
    "    fps = 50\n",
    "    audio_energy = intensity_from_signal(audio, int(sr/fps))\n",
    "    beat_ts = np.arange(0, audio_energy.shape[0]) / fps\n",
    "    daudio_dt = dx_dt(audio_energy)\n",
    "    Dm = 0.2\n",
    "    DM = 0.7\n",
    "    DM_frame = math.floor(DM / (beat_ts[1] - beat_ts[0]))\n",
    "    energy_interp = interp1d(beat_ts, audio_energy, bounds_error=False)\n",
    "    # iterative find audio onset between 0.2 and 0.6 seconds to identify beats\n",
    "    beats = [[0, False]] # start with a pseudo beat\n",
    "    for i in range(0, audio_energy.shape[0]):\n",
    "        if daudio_dt[i] > 5:\n",
    "            current_beat_t = beat_ts[i]\n",
    "            if current_beat_t - beat_ts[beats[-1][0]] <= Dm:\n",
    "                continue\n",
    "            if current_beat_t - beat_ts[beats[-1][0]] >= DM:\n",
    "                # these are stored as integer indexes\n",
    "                start = beats[-1][0]\n",
    "                end = i\n",
    "                counter = start + DM_frame\n",
    "                while counter < end:\n",
    "                    beats.append([counter, False])\n",
    "                    counter = counter + DM_frame\n",
    "            beats.append([i, True])\n",
    "    beats_arr = []\n",
    "    for i in range(0, len(beats)):\n",
    "        if beats[i][1]:\n",
    "            beats_arr.append([beat_ts[beats[i][0]], audio_energy[beats[i][0]]])\n",
    "    beats_arr = np.array(beats_arr)\n",
    "    return beats_arr\n",
    "def get_tags(transcript_file_path, tagged_text_file_path, speaker_id):\n",
    "    with open(transcript_file_path, 'r') as f:\n",
    "        if speaker_id == 0:\n",
    "            transcript_list = json.load(f)[\"self\"]\n",
    "        else:\n",
    "            transcript_list = json.load(f)[\"other\"]\n",
    "\n",
    "    # Load the transcript text file\n",
    "    with open(tagged_text_file_path, 'r') as f:\n",
    "        transcript = f.read()\n",
    "    transcript_list.append({\"text\":\"EOH\", \"start\":transcript_list[-1][\"end\"]})\n",
    "    word_list = [word[\"text\"] for word in transcript_list]\n",
    "    translator = str.maketrans('', '', string.punctuation.replace('<', '').replace('>', '').replace('-', '').replace('_', '').replace(\"'\", '').replace(\"/\", ''))\n",
    "    # Remove all punctuation except < and > from the input string using the translation table\n",
    "    word_list = [word.translate(translator) for word in word_list]\n",
    "    transcript = transcript.translate(translator)\n",
    "    transcript = transcript.split(\" \")\n",
    "    transcript = [s for s in transcript if s != \"\"]\n",
    "    transcript.append(\"EOT\")\n",
    "    i = 0\n",
    "    j = 0\n",
    "    tag_durations = {}\n",
    "    while i < len(transcript):\n",
    "        if transcript[i] == word_list[j]:\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            # if this is a starting tag\n",
    "            if transcript[i][-2:] != \"/>\":\n",
    "                active_tag = transcript[i][1:-1]\n",
    "                active_tag_interval = [j]\n",
    "                # iterate through the list to find the matching tag\n",
    "                ii = i + 1\n",
    "                jj = j\n",
    "                while ii < len(transcript):\n",
    "                    if jj < len(word_list) and transcript[ii] == word_list[jj]:\n",
    "                        ii += 1\n",
    "                        jj += 1\n",
    "                    else:\n",
    "                        # if we have foudn the end tag\n",
    "                        if transcript[ii][-2:] == \"/>\" and transcript[ii][1:-2] == active_tag:\n",
    "                            active_tag_interval.append(jj)\n",
    "                            active_tag_interval[0] = transcript_list[active_tag_interval[0]][\"start\"]\n",
    "                            active_tag_interval[1] = transcript_list[active_tag_interval[1]][\"start\"]\n",
    "                            try:\n",
    "                                tag_durations[active_tag].append(active_tag_interval.copy())\n",
    "                            except:\n",
    "                                tag_durations[active_tag] = [active_tag_interval.copy()]\n",
    "                            \n",
    "                            break\n",
    "                        else:\n",
    "                            ii+=2\n",
    "                            jj+=1\n",
    "            i += 2\n",
    "            j += 1\n",
    "    return tag_durations\n",
    "def apply_stare_tags(tag_durations, ts, aversion_probability):\n",
    "    try:\n",
    "        stare_intervals = tag_durations[\"stare\"]\n",
    "        stare_intervals_index_sets = []\n",
    "        for i in range(0, len(stare_intervals)):\n",
    "            index_set = []\n",
    "            for t in range(ts.shape[0]):\n",
    "                if ts[t] >= stare_intervals[i][0] and len(index_set) == 0:\n",
    "                    index_set.append(t)\n",
    "                if ts[t] >= stare_intervals[i][1]:\n",
    "                    index_set.append(t)\n",
    "                    break\n",
    "            stare_intervals_index_sets.append(index_set)\n",
    "\n",
    "        for i in range(len(stare_intervals_index_sets)):\n",
    "            aversion_probability[stare_intervals_index_sets[i][0]:stare_intervals_index_sets[i][1]] = 0\n",
    "    except:\n",
    "        pass\n",
    "    return aversion_probability\n",
    "def apply_directional_tag(tag_durations, aversion_saliency):\n",
    "    tag_directions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "    for dire in tag_directions:\n",
    "        look_up_intervals = []\n",
    "        try:\n",
    "            look_up_intervals = tag_durations[dire]\n",
    "        except:\n",
    "            continue\n",
    "        look_up_intervals_index_sets = []\n",
    "        for i in range(0, len(look_up_intervals)):\n",
    "            index_set = []\n",
    "            for t in range(ts.shape[0]):\n",
    "                if ts[t] >= look_up_intervals[i][0] and len(index_set) == 0:\n",
    "                    index_set.append(t)\n",
    "                if ts[t] >= look_up_intervals[i][1]:\n",
    "                    index_set.append(t)\n",
    "                    break\n",
    "            look_up_intervals_index_sets.append(index_set)\n",
    "        objects_positions = aversion_saliency.get_object_positions()\n",
    "        if dire == \"up\":\n",
    "            above = np.where(objects_positions[:, 1] > objects_positions[aversion_saliency.scene_info.get_object_positions().shape[0], 1], 1, 0)\n",
    "            above[aversion_saliency.scene_info.get_object_positions(coordinate_space=\"global\").shape[0]] = 1\n",
    "        elif dire == \"down\":\n",
    "            above = np.where(objects_positions[:, 1] < objects_positions[aversion_saliency.scene_info.get_object_positions().shape[0], 1], 1, 0)\n",
    "            above[aversion_saliency.scene_info.get_object_positions(coordinate_space=\"global\").shape[0]] = 1\n",
    "        elif dire == \"left\":\n",
    "            above = np.where(objects_positions[:, 2] < objects_positions[aversion_saliency.scene_info.get_object_positions().shape[0], 2], 1, 0)\n",
    "            above[aversion_saliency.scene_info.get_object_positions(coordinate_space=\"global\").shape[0]] = 1\n",
    "        elif dire == \"right\":\n",
    "            above = np.where(objects_positions[:, 2] > objects_positions[aversion_saliency.scene_info.get_object_positions().shape[0], 2], 1, 0)\n",
    "            above[aversion_saliency.scene_info.get_object_positions(coordinate_space=\"global\").shape[0]] = 1\n",
    "        for i in range(len(look_up_intervals_index_sets)):\n",
    "            mask = np.tile(np.expand_dims(above, axis=0), [look_up_intervals_index_sets[i][1] - look_up_intervals_index_sets[i][0], 1])\n",
    "            aversion_saliency.map[look_up_intervals_index_sets[i][0]:look_up_intervals_index_sets[i][1]] *= mask\n",
    "    return aversion_saliency\n",
    "\n",
    "tag_durations_0 = get_tags(transcript_file_path_0, tagged_text_file_path_0, 0)\n",
    "tag_durations_1 = get_tags(transcript_file_path_1, tagged_text_file_path_1, 1)\n",
    "aversion_probability_0 = apply_stare_tags(tag_durations_0, ts, aversion_probability_0)\n",
    "aversion_probability_1 = apply_stare_tags(tag_durations_1, ts, aversion_probability_1)\n",
    "\n",
    "sementic_script_0 = Sentence_word_phone_parser(praatoutput_path_0, praatoutput_path_0)\n",
    "sementic_script_0.get_turns(turn_taking_threshold)\n",
    "sementic_script_1 = Sentence_word_phone_parser(praatoutput_path_1, praatoutput_path_1)\n",
    "sementic_script_1.get_turns(turn_taking_threshold)\n",
    "# agentScene1 = AgentInfo(scene_data_path, wonder=False)\n",
    "agentScene0 = AgentInfo(scene_data_path_0, wonder=True)\n",
    "agentScene1 = AgentInfo(scene_data_path_1, wonder=True)\n",
    "aversion_saliency_0 = Neural_input_Saliency(agentScene0, ts, 1.0/fps)\n",
    "aversion_saliency_0.compute_salience(ts, aversion_probability_0)\n",
    "aversion_saliency_0 = apply_directional_tag(tag_durations_0, aversion_saliency_0)\n",
    "aversion_saliency_1 = Neural_input_Saliency(agentScene1, ts, 1.0/fps)\n",
    "aversion_saliency_1.compute_salience(ts, aversion_probability_1)\n",
    "aversion_saliency_1 = apply_directional_tag(tag_durations_1, aversion_saliency_1)\n",
    "\n",
    "beats_0 = get_beats(audio_0, sr)[:, 0]\n",
    "beats_1 = get_beats(audio_1, sr)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down': [[3.77, 357.11]], 'stare': [[296.93, 350.63]]}\n"
     ]
    }
   ],
   "source": [
    "print(tag_durations_0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Gaze Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner0 = Responsive_planner_React_to_gaze_no_Gaze_deploy([aversion_saliency_0], agentScene0, aversion_probability_0, aversion_probability_1, acitivity, beats_0, min_saccade_time_consecutive=2)\n",
    "planner1 = Responsive_planner_React_to_gaze_no_Gaze_deploy([aversion_saliency_1], agentScene1, aversion_probability_1, aversion_probability_0, acitivity, -beats_1, min_saccade_time_consecutive=2)\n",
    "\n",
    "output_times_0, output_targets_0 = planner0.compute()\n",
    "output_times_1, output_targets_1 = planner1.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Gaze motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/MASC/JALI_gaze/Animations/heat/outputs/heat_source_video_neural_0.pkl\n",
      "D:/MASC/JALI_gaze/Animations/heat/outputs/heat_source_video_neural_1.pkl\n"
     ]
    }
   ],
   "source": [
    "#get the output_targets_positions from the scene\n",
    "def output_files(agentScene, output_targets, output_times, jali_neck_output, output_neural_location):\n",
    "    output_target_positions = []\n",
    "    for i in output_targets:\n",
    "        output_target_positions.append(agentScene.get_all_positions(coordinate_space=\"local\", index=i))\n",
    "    internal_model = InternalModelCenterBias(agentScene)\n",
    "    # the good model\n",
    "    generator = SacccadeGenerator(output_times, output_target_positions, output_targets, internal_model, dt=1/fps)\n",
    "    ek, hk, micro_saccade = generator.compute()\n",
    "    blend_weight = []\n",
    "    for i in range(1, len(hk[0])-1):\n",
    "        velocity = math.sqrt((hk[0][i][1]-hk[0][i-1][1])**2 + (hk[0][i-1][2]-hk[0][i][2])**2)\n",
    "        blend_weight.append([hk[0][i][0], 1 - min(1, velocity/0.75)])\n",
    "    out = {\"eye_frames\": ek,\n",
    "            \"head_frames\": hk,\n",
    "            \"micro_saccade\": micro_saccade,\n",
    "            \"other_neck\": jali_neck_output,\n",
    "            \"envelope\":[]}\n",
    "            # \"output_times\": output_times, \n",
    "            # \"output_targets\": output_targets\n",
    "    with open(output_neural_location, 'wb') as f:\n",
    "        pickle.dump(out, f, protocol=2)\n",
    "    print(output_neural_location)\n",
    "output_files(agentScene0, output_targets_0, output_times_0, jali_neck_output_0, output_neural_location_0)\n",
    "output_files(agentScene1, output_targets_1, output_times_1, jali_neck_output_1, output_neural_location_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaligaze",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
